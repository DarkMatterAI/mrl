{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "! [ -e /content ] && pip install -Uqq mrl-pypi  # upgrade mrl on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp g_models.generative_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Base\n",
    "\n",
    "> Base class for generative models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from mrl.imports import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *\n",
    "from mrl.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Models\n",
    "\n",
    "`GenerativeModel` is a base class that defines the functions a model needs to integrate with the MRL library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class GenerativeModel(nn.Module):\n",
    "    '''\n",
    "    GenerativeModel - base generative model class\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def x_to_latent(self, x):\n",
    "        '''\n",
    "        x_to_latent - convert `x` to a latent vector\n",
    "        \n",
    "        Inputs:\n",
    "        \n",
    "        - `x`: `x` comes from a Dataloader. The specific \n",
    "        form of `x` depends on the dataloader used\n",
    "            \n",
    "        Returns:\n",
    "        \n",
    "        If the model in question makes use of latent vectors \n",
    "        for sampling or reconstruction, the function should \n",
    "        return a batch of latent vectors. If latent vectors \n",
    "        are not compatible, the function should return None\n",
    "            \n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def sample(self, **sample_kwargs):\n",
    "        '''\n",
    "        sample - sample items from tthe model\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def sample_no_grad(self, **sample_kwargs):\n",
    "        'no grad wrapper for sample'\n",
    "        with torch.no_grad():\n",
    "            return self.sample(**sample_kwargs)\n",
    "        \n",
    "    def get_rl_tensors(self):\n",
    "        '''\n",
    "        get_rl_tensors - generate tensors needed in the training loop\n",
    "        '''\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def beam_search(model, seed_ints, k, beam_size, sl, temperature, pad_idx=None):\n",
    "    '''\n",
    "    beam_search - perform beam search using `model`\n",
    "    \n",
    "    Inputs:\n",
    "        \n",
    "    - `model nn.Module`: model\n",
    "\n",
    "    - `seed_ints torch.Longtensor`: seed sequence\n",
    "\n",
    "    - `k int`: top k beam sampling\n",
    "\n",
    "    - `beam_size int`: maximum number of beams to retain\n",
    "\n",
    "    - `sl int`: max sequence length\n",
    "\n",
    "    - `temperature float`: sample temperature\n",
    "\n",
    "    - `pad_idx Optional[int]`: pad index if applicable\n",
    "    '''\n",
    "    \n",
    "    # currently only works for LSTM_LM. TODO: work for all generative models\n",
    "    \n",
    "    current_device = next(model.parameters()).device\n",
    "    \n",
    "    if seed_ints.ndim==1:\n",
    "        seed_ints = seed_ints.unsqueeze(0)\n",
    "        \n",
    "    preds = seed_ints.repeat(k,1)\n",
    "    preds = to_device(preds, current_device)\n",
    "    idxs = preds[:,-1].unsqueeze(-1)\n",
    "    lps = idxs.new_zeros((k, 1)).float()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(sl):\n",
    "            x, hiddens, encoded = model._forward(idxs, hiddens)\n",
    "            x.div_(temperature)\n",
    "            log_probs = F.log_softmax(x, -1)\n",
    "            values, indices = log_probs.topk(k, dim=-1)\n",
    "        \n",
    "            lps = torch.cat([lps.unsqueeze(-1).repeat(1,1,values.shape[-1]), -values], 1)\n",
    "            current_sl = lps.shape[1]\n",
    "            lps = lps.permute(0,2,1).reshape(-1,current_sl)\n",
    "\n",
    "            preds = torch.cat([preds[:,None].expand(preds.size(0), k , preds.size(1)),\n",
    "                    indices.squeeze(1)[:,:,None].expand(preds.size(0), k, 1),], dim=2)\n",
    "\n",
    "            preds = preds.view(-1, preds.size(2))\n",
    "        \n",
    "            scores = lps.sum(-1)\n",
    "            indices_idx = torch.arange(0,preds.size(0))[:,None].expand(preds.size(0), k).contiguous().view(-1)\n",
    "            sort_idx = scores.argsort()[:beam_size]\n",
    "\n",
    "            preds = preds[sort_idx]\n",
    "            lps = lps[sort_idx]\n",
    "\n",
    "            idxs = preds[:,-1].unsqueeze(-1)\n",
    "            hiddens = [(i[0][:, indices_idx[sort_idx], :], \n",
    "                        i[1][:, indices_idx[sort_idx], :]) for i in hiddens]\n",
    "        \n",
    "            if pad_idx is not None:\n",
    "                if (preds[:,-1]==pad_idx).all():\n",
    "                    break\n",
    "                \n",
    "                \n",
    "    return preds, -lps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
