{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward\n",
    "\n",
    "> Reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from mrl.imports import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *\n",
    "from mrl.chem import *\n",
    "from mrl.templates import *\n",
    "from mrl.agent import *\n",
    "from mrl.environment import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Reward(Callback):\n",
    "    def __init__(self, name, order=10, weight=1., track=True):\n",
    "        self.name = name\n",
    "        self.order = order\n",
    "        self.track = track\n",
    "        self.weight = weight\n",
    "        \n",
    "    def setup(self):\n",
    "        log = self.environment.log\n",
    "        log.add_log(self.name)\n",
    "        if self.track:\n",
    "            log.add_metric(self.name)\n",
    "            \n",
    "    def _compute_reward(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def compute_reward(self):\n",
    "        rewards = self._compute_reward()\n",
    "        rewards = rewards.squeeze()\n",
    "        self.batch_state.rewards += self.weight*rewards\n",
    "        self.batch_state[self.name] = rewards\n",
    "        \n",
    "        if self.track:\n",
    "            self.environment.log.update_metric(self.name, rewards.mean().detach().cpu().numpy())\n",
    "            \n",
    "            \n",
    "class FunctionReward(Reward):\n",
    "    def __init__(self, reward_function, name, order=10, weight=1., track=True):\n",
    "        super().__init__(name, order, weight, track)\n",
    "        self.reward_function = reward_function\n",
    "        \n",
    "    def _compute_reward(self):\n",
    "        return self.reward_function(self.batch_state)\n",
    "        \n",
    "        \n",
    "class SampleReward(Reward):\n",
    "    def __init__(self, reward_function, template_filter, lookup, \n",
    "                 name, order=10, weight=1., track=True):\n",
    "        super().__init__(name, order, weight, track)\n",
    "        self.reward_function = reward_function\n",
    "        self.lookup = lookup\n",
    "        self.template_filter = template_filter\n",
    "        self.lookup_table = {}\n",
    "    \n",
    "    def _compute_reward(self):\n",
    "        \n",
    "        samples = self.batch_state.samples\n",
    "        hps = self.batch_state.template_passes\n",
    "        outputs = to_device(torch.tensor([0. for i in samples]))\n",
    "        \n",
    "        to_score = []\n",
    "        to_score_idxs = []\n",
    "        \n",
    "        for i, sample in enumerate(samples):\n",
    "            if self.lookup and sample in self.lookup_table.keys():\n",
    "                outputs[i] = self.lookup_table[sample]\n",
    "                \n",
    "            else:\n",
    "                if (self.template_filter and hps[i]) or (not self.template_filter):\n",
    "                    to_score.append(sample)\n",
    "                    to_score_idxs.append(i)\n",
    "                    \n",
    "        if to_score:\n",
    "            scores = self.reward_function(samples)\n",
    "            \n",
    "            for i in range(len(to_score)):\n",
    "                outputs[to_score_idxs[i]] = scores[i]\n",
    "            \n",
    "                if self.lookup:\n",
    "                    item_score = scores[i]\n",
    "                    if type(item_score) == torch.Tensor:\n",
    "                        item_score = item_score.detach().cpu()\n",
    "                    self.lookup_table[to_score[i]] = item_score\n",
    "        \n",
    "        \n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class NoveltyBonus(Reward):\n",
    "    def __init__(self, weight, name='novel', order=100, track=True):\n",
    "        super().__init__(name, order, weight, track)\n",
    "        \n",
    "    def _compute_reward(self):\n",
    "        log = self.environment.log\n",
    "        state = self.batch_state\n",
    "        old = log.unique_samples\n",
    "        \n",
    "        new = [not i in old for i in state.samples]\n",
    "        reward = to_device(torch.tensor(new)).float()\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parallel reward\n",
    "    parallel process calculation on one sequence at a time\n",
    "    \n",
    "batch reward\n",
    "    parallel featurize\n",
    "    batch\n",
    "    compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mrl)",
   "language": "python",
   "name": "mrl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
