{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward\n",
    "\n",
    "> Reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from mrl.imports import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *\n",
    "from mrl.chem import *\n",
    "from mrl.templates import *\n",
    "from mrl.agent import *\n",
    "from mrl.environment import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Reward(Callback):\n",
    "    def __init__(self, name, order=10, weight=1., track=True):\n",
    "        self.name = name\n",
    "        self.order = order\n",
    "        self.track = track\n",
    "        self.weight = weight\n",
    "        \n",
    "    def setup(self):\n",
    "        log = self.environment.log\n",
    "        log.add_log(self.name)\n",
    "        if self.track:\n",
    "            log.add_metric(self.name)\n",
    "            \n",
    "    def _compute_reward(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def compute_reward(self):\n",
    "        rewards = self._compute_reward()\n",
    "        rewards = rewards.squeeze()\n",
    "        self.batch_state.rewards += self.weight*rewards\n",
    "        self.batch_state[self.name] = rewards\n",
    "        \n",
    "        if self.track:\n",
    "            self.environment.log.update_metric(self.name, rewards.mean().detach().cpu().numpy())\n",
    "            \n",
    "            \n",
    "class FunctionReward(Reward):\n",
    "    def __init__(self, reward_function, name, order=10, weight=1., track=True):\n",
    "        super().__init__(name, order, weight, track)\n",
    "        self.reward_function = reward_function\n",
    "        \n",
    "    def _compute_reward(self):\n",
    "        return self.reward_function(self.batch_state)\n",
    "        \n",
    "        \n",
    "class SampleReward(Reward):\n",
    "    def __init__(self, reward_function, template_filter, lookup, \n",
    "                 name, order=10, weight=1., track=True):\n",
    "        super().__init__(name, order, weight, track)\n",
    "        self.reward_function = reward_function\n",
    "        self.lookup = lookup\n",
    "        self.template_filter = template_filter\n",
    "        self.lookup_table = {}\n",
    "    \n",
    "    def _compute_reward(self):\n",
    "        \n",
    "        samples = self.batch_state.samples\n",
    "        hps = self.batch_state.template_passes\n",
    "        outputs = to_device(torch.tensor([0. for i in samples]))\n",
    "        \n",
    "        to_score = []\n",
    "        to_score_idxs = []\n",
    "        \n",
    "        for i, sample in enumerate(samples):\n",
    "            if self.lookup and sample in self.lookup_table.keys():\n",
    "                outputs[i] = self.lookup_table[sample]\n",
    "                \n",
    "            else:\n",
    "                if (self.template_filter and hps[i]) or (not self.template_filter):\n",
    "                    to_score.append(sample)\n",
    "                    to_score_idxs.append(i)\n",
    "                    \n",
    "        if to_score:\n",
    "            scores = self.reward_function(samples)\n",
    "            \n",
    "            for i in range(len(to_score)):\n",
    "                outputs[to_score_idxs[i]] = scores[i]\n",
    "            \n",
    "                if self.lookup:\n",
    "                    self.lookup_table[to_score[i]] = scores[i]\n",
    "        \n",
    "        \n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class NoveltyBonus(Reward):\n",
    "    def __init__(self, weight, name='novel', order=100, track=True):\n",
    "        super().__init__(name, order, weight, track)\n",
    "        \n",
    "    def _compute_reward(self):\n",
    "        log = self.environment.log\n",
    "        state = self.batch_state\n",
    "        old = log.unique_samples\n",
    "        \n",
    "        new = [not i in old for i in state.samples]\n",
    "        reward = to_device(torch.tensor(new)).float()\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredReward(Callback):\n",
    "    def __init__(self, name, agent, weight=1.):\n",
    "        super().__init__(order=1)\n",
    "        self.name = name\n",
    "        self.weight = weight\n",
    "        self.agent = agent\n",
    "        \n",
    "    def setup(self):\n",
    "        log = self.environment.log\n",
    "        log.add_metric(self.name)\n",
    "        log.add_log(self.name)\n",
    "        \n",
    "    def compute_reward(self):\n",
    "        env = self.environment\n",
    "        samples = self.batch_state.samples\n",
    "        with torch.no_grad():\n",
    "            preds = self.agent.predict_data(samples).squeeze()\n",
    "        reward = -preds * self.weight\n",
    "        \n",
    "        env.log.update_metric(self.name, reward.mean().detach().cpu().numpy())\n",
    "        self.batch_state.rewards += reward\n",
    "        self.batch_state[self.name] = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback():\n",
    "    def __init__(self, name='callback', order=10):\n",
    "        self.order=order\n",
    "        self.name = name\n",
    "    \n",
    "    def __call__(self, event_name):\n",
    "        \n",
    "        event = getattr(self, event_name, None)\n",
    "        if event is not None:\n",
    "            output = event()\n",
    "        else:\n",
    "            output = None\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Event():\n",
    "    def __init__(self):\n",
    "        self.setup = 'setup'\n",
    "        self.before_train = 'before_train'\n",
    "        self.build_buffer = 'build_buffer'\n",
    "        self.after_build_buffer = 'after_build_buffer'\n",
    "        self.before_batch = 'before_batch'\n",
    "        self.sample_batch = 'sample_batch'\n",
    "        self.after_sample = 'after_sample'\n",
    "        self.get_model_outputs = 'get_model_outputs'\n",
    "        self.compute_reward = 'compute_reward'\n",
    "        self.after_compute_reward = 'after_compute_reward'\n",
    "        self.compute_loss = 'compute_loss'\n",
    "        self.zero_grad = 'zero_grad'\n",
    "        self.before_step = 'before_step'\n",
    "        self.step = 'step'\n",
    "        self.after_batch = 'after_batch'\n",
    "        self.after_train = 'after_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Reward():\n",
    "    def __init__(self, template=None, reward_modules=[], trajectory_modules=[]):\n",
    "        \n",
    "        if template == None:\n",
    "            template = Template([])\n",
    "            \n",
    "        self.template = template\n",
    "        self.reward_modules = reward_modules\n",
    "        self.trajectory_modules = trajectory_modules\n",
    "        self.mean_reward = None\n",
    "        \n",
    "    def __call__(self, model_output):\n",
    "        \n",
    "        template_passes = np.array(np.array(self.template(model_output['sequences'])))\n",
    "        template_rewards = np.array(self.template.eval_mols(model_output['sequences']))\n",
    "        \n",
    "        rewards = self.compute_rewards(model_output, template_passes)\n",
    "        trajectory_rewards = self.compute_trajectory_reward(model_output, template_passes)\n",
    "        \n",
    "        rewards = template_rewards + rewards\n",
    "        \n",
    "        if self.mean_reward is None:\n",
    "            self.mean_reward = rewards.mean()\n",
    "        else:\n",
    "            self.mean_reward = (1-reward_decay)*rewards.mean() + reward_decay*self.mean_reward\n",
    "            \n",
    "        rewards_scaled = rewards - self.mean_rewards\n",
    "        \n",
    "        model_output['rewards'] = rewards\n",
    "        model_output['rewards_scaled'] = rewards_scaled\n",
    "        model_output['trajectory_rewards'] = trajectory_rewards\n",
    "        \n",
    "        return model_output\n",
    "    \n",
    "    def compute_trajectory_reward(self, model_output, template_passes):\n",
    "        \n",
    "        all_rewards = []\n",
    "        \n",
    "        for rm in self.trajectory_modules:\n",
    "            all_rewards.append(rm(model_output, template_passes))\n",
    "            \n",
    "        all_rewards = np.stack(all_rewards, -1)\n",
    "        all_rewards = all_rewards.sum(-1)\n",
    "        return all_rewards\n",
    "    \n",
    "    def compute_rewards(self, model_output, template_passes):\n",
    "        \n",
    "        all_rewards = []\n",
    "        \n",
    "        for rm in self.reward_modules:\n",
    "            all_rewards.append(rm(model_output, template_passes))\n",
    "            \n",
    "        all_rewards = np.stack(all_rewards, -1)\n",
    "        all_rewards = all_rewards.sum(-1)\n",
    "        return all_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trajectory_wrapper(inputs, function):\n",
    "    return np.array([function(i) for i in inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RewardModule():\n",
    "    \n",
    "    def __call__(self, model_output, template_passes=None):\n",
    "        \n",
    "        reward_inputs = self.prepare_reward_inputs(model_output, template_passes)\n",
    "        reward_outputs = self.reward_function(reward_inputs)\n",
    "        final_reward = self.aggregate_reward(reward_outputs, model_output, template_passes)\n",
    "        return final_reward\n",
    "        \n",
    "    def aggregate_reward(self, reward_outputs, model_output, template_passes=None):\n",
    "        pass\n",
    "        \n",
    "    def prepare_reward_inputs(self, model_output, template_passes=None):\n",
    "        pass\n",
    "    \n",
    "    def reward_function(self, inputs):\n",
    "        pass\n",
    "    \n",
    "class MolReward(RewardModule):\n",
    "    def __init__(self, mol_function, trajectory=False):\n",
    "        self.mol_function = mol_function\n",
    "        self.trajectory = trajectory\n",
    "        \n",
    "    def aggregate_reward(self, reward_outputs, model_output, template_passes=None):\n",
    "        \n",
    "        if template_passes is not None:\n",
    "            passed_idxs = np.array([i for i in range(len(template_passes)) if template_passes[i]])\n",
    "            bs = len(template_passes)\n",
    "        else:\n",
    "            passed_idxs = np.arange(len(reward_outputs))\n",
    "            bs = len(reward_outputs)\n",
    "        \n",
    "        if self.trajectory:\n",
    "            outputs = np.zeros((bs, model_output['sl']))\n",
    "            \n",
    "            for i, idx in enumerate(passed_idxs):\n",
    "                traj = reward_outputs[i]\n",
    "                traj_len = len(traj)\n",
    "                outputs[idx, :traj_len] = traj\n",
    "                \n",
    "        else:\n",
    "            outputs = np.zeros((bs))\n",
    "            outputs[passed_idxs] = reward_outputs\n",
    "            \n",
    "        return outputs\n",
    "        \n",
    "    def prepare_reward_inputs(self, model_output, template_passes=None):\n",
    "        \n",
    "        if self.trajectory:\n",
    "            inputs = model_output['sequence_trajectories']\n",
    "        else:\n",
    "            inputs = model_output['sequences']\n",
    "            \n",
    "        output = np.zeros((len(inputs)))\n",
    "        \n",
    "        if template_passes is not None:\n",
    "            inputs = [inputs[i] for i in range(len(inputs)) if template_passes[i]]\n",
    "            \n",
    "        return inputs\n",
    "    \n",
    "    def reward_function(self, inputs):\n",
    "        if self.trajectory:\n",
    "            func = partial(trajectory_wrapper, function=self.mol_function)\n",
    "        else:\n",
    "            func = self.mol_function\n",
    "            \n",
    "        return maybe_parallel(func, inputs)\n",
    "    \n",
    "class MLReward():\n",
    "    def __init__(self, model, trajectory=False):\n",
    "        self.model = model\n",
    "        self.trajectory = trajectory\n",
    "        \n",
    "    def reward_function(self, inputs):\n",
    "        if not type(inputs)==list:\n",
    "            inputs = [inputs]\n",
    "        return np.array(self.model(*inputs).detach().cpu())\n",
    "    \n",
    "    def prepare_reward_inputs(self, model_output, template_passes=None):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def aggregate_reward(self, reward_outputs, model_output, template_passes=None):\n",
    "        if template_passes is not None:\n",
    "            passed_idxs = np.array([i for i in range(len(template_passes)) if template_passes[i]])\n",
    "            bs = len(template_passes)\n",
    "        else:\n",
    "            passed_idxs = np.arange(len(reward_outputs))\n",
    "            bs = len(reward_outputs)\n",
    "        \n",
    "        if reward_outputs.ndim==2:\n",
    "            output = np.zeros((bs, reward_outputs.shape[-1]))\n",
    "            output[passed_idxs] = reward_output\n",
    "            \n",
    "        else:\n",
    "            output = np.zeros((bs,))\n",
    "            output[passed_idxs] = reward_outputs\n",
    "            \n",
    "        return output\n",
    "    \n",
    "class FPModelReward(MLReward):\n",
    "    def __init__(self, model, fp_func, trajectory=False):\n",
    "        super().__init__(model, trajectory)\n",
    "        self.fp_func = fp_func\n",
    "        \n",
    "    def prepare_reward_inputs(self, model_output, template_passes=None):\n",
    "        \n",
    "        smiles = model_output['sequences']\n",
    "        fps = np.stack(maybe_parallel(self.fp_func, smiles))\n",
    "        fps = to_device(torch.from_numpy(fps).float())\n",
    "        return fps\n",
    "    \n",
    "class SequenceModelReward(MLReward):\n",
    "\n",
    "    def prepare_reward_inputs(self, model_output, template_passes=None):\n",
    "        \n",
    "        return model_output['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mf(smile):\n",
    "    mol = to_mol(smile)\n",
    "    if mol is None:\n",
    "        output = 0.\n",
    "    else:\n",
    "        output = qed(mol)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = MolReward(mf, trajectory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo = ModelOutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo['sequences'] = ['C', 'CCC', 'CCCC']\n",
    "mo['sequence_trajectories'] = [['C'], ['C', 'CC', 'CCC'], ['C', 'CC', 'CCC', 'CCCC']]\n",
    "mo['sl'] = 4\n",
    "template_passes = np.array([True, False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.35978494, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.35978494, 0.37278556, 0.38547066, 0.43102436]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r(mo, template_passes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['C'], ['C', 'CC', 'CCC'], ['C', 'CC', 'CCC', 'CCCC']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.prepare_reward_inputs(mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r.reward_function(r.prepare_reward_inputs(mo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[5]]).ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([['C'], ['C', 'CC', 'CCC'], ['C', 'CC', 'CCC', 'CCCC']], dtype=object).ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parallel reward\n",
    "    parallel process calculation on one sequence at a time\n",
    "    \n",
    "batch reward\n",
    "    parallel featurize\n",
    "    batch\n",
    "    compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mrl)",
   "language": "python",
   "name": "mrl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
