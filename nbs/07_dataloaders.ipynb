{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders\n",
    "\n",
    "> Pytorch datasets, dataloaders, collate functions and vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from mrl.imports import *\n",
    "from mrl.torch_imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expot\n",
    "\n",
    "SMILES_CHAR_VOCAB = ['#', '(', ')', '+', '-', '/', '0',\n",
    "                 '1', '2', '3', '4', '5', '6', '7',\n",
    "                 '8', '=', '@', 'B', 'C', 'F', 'H',\n",
    "                 'I', 'N', 'O', 'P', 'S', '[', '\\\\',\n",
    "                 ']', 'c', 'i', 'l', 'n', 'o', 'r', 's',\n",
    "                 '*', ':']\n",
    "\n",
    "\n",
    "SPECIAL_TOKENS = ['bos', 'eos', 'pad', 'unk']\n",
    "\n",
    "MAPPING_TOKENS = ['[1*:1]', '[2*:1]', '[1*:2]', '[2*:2]', '[1*:3]',\n",
    "                  '[2*:3]', '[1*:4]', '[2*:4]', '[1*:5]', '[2*:5]']\n",
    "\n",
    "HALOGEN_REPLACE = {'Br':'R',\n",
    "                   'Cl':'L'}\n",
    "\n",
    "MAPPING_REPLACE = {'[1*:1]':'A',\n",
    "                   '[2*:1]':'D',\n",
    "                   '[1*:2]':'E',\n",
    "                   '[2*:2]':'G',\n",
    "                   '[1*:3]':'J',\n",
    "                   '[2*:3]':'M',\n",
    "                   '[1*:4]':'Q',\n",
    "                   '[2*:4]':'T',\n",
    "                   '[1*:5]':'U', \n",
    "                   '[2*:5]':'V'}\n",
    "\n",
    "SMILE_REGEX = \"\"\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|H|\\(|\\)|\\.|=|\n",
    "                 #|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\"\"\n",
    "\n",
    "MAPPING_REGEX = \"\"\"(\\[.\\*:.]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|H|\\[|\\]|\\(|\\)|\\.|=|\n",
    "                    #|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def tokenize_by_character(smile):\n",
    "    return [i for i in smile]\n",
    "\n",
    "def tokenize_with_replacements(smile, replacement_dict):\n",
    "    for k,v in replacement_dict.items():\n",
    "        smile = smile.replace(k,v)\n",
    "    return [i for i in smile]\n",
    "\n",
    "def regex_tokenize(smile, regex):\n",
    "    tokens = [token for token in regex.findall(smile)]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Vocab():\n",
    "    def __init__(self, itos):\n",
    "        self.special_tokens = ['bos', 'eos', 'pad', 'unk']\n",
    "        \n",
    "        self.itos = self.special_tokens + [i for i in itos if not i in self.special_tokens]\n",
    "        self.stoi = {self.itos[i]:i for i in range(len(self.itos))}\n",
    "        self.unks = []\n",
    "        \n",
    "    def tokenize(self, input):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def numericalize(self, input):\n",
    "        output = []\n",
    "        for tok in input:\n",
    "            if tok in self.stoi.keys():\n",
    "                output.append(self.stoi[tok])\n",
    "            else:\n",
    "                output.append(self.stoi['unk'])\n",
    "                self.unks.append(tok)\n",
    "        return output\n",
    "    \n",
    "    def reconstruct(self, input):\n",
    "        \n",
    "        output = []\n",
    "        for item in input:\n",
    "            item = self.itos[item]\n",
    "            if item=='eos':\n",
    "                break\n",
    "                \n",
    "            if not item=='bos':\n",
    "                output.append(item)\n",
    "        \n",
    "        return ''.join(output)\n",
    "                \n",
    "    def update_vocab(self):\n",
    "        unks = list(set(self.unks))\n",
    "        self.itos += unks\n",
    "        self.stoi = {self.itos[i]:i for i in range(len(self.itos))}\n",
    "        self.unks = []\n",
    "        \n",
    "    def update_vocab_from_data(self, inputs):\n",
    "        _ = [self.numericalize(self.tokenize(i)) for i in inputs]\n",
    "        self.update_vocab()\n",
    "        \n",
    "        \n",
    "class CharacterVocab(Vocab):\n",
    "    def tokenize(self, smile):\n",
    "        toks = tokenize_by_character(smile)\n",
    "        toks = ['bos'] + toks + ['eos']\n",
    "        return toks\n",
    "    \n",
    "    \n",
    "class CharacterReplaceVocab(Vocab):\n",
    "    def __init__(self, itos, replace_dict):\n",
    "        self.replace_dict = replace_dict\n",
    "        self.reverse_dict = {v:k for k,v in replace_dict.items()}\n",
    "        for rep in self.reverse_dict.keys():\n",
    "            if not rep in itos:\n",
    "                itos.append(rep)\n",
    "        super().__init__(itos)\n",
    "        \n",
    "    def tokenize(self, smile):\n",
    "        toks = tokenize_with_replacements(smile, self.replace_dict)\n",
    "        toks = ['bos'] + toks + ['eos']\n",
    "        return toks\n",
    "    \n",
    "    def reconstruct(self, input):\n",
    "        output = []\n",
    "        for item in input:\n",
    "            item = self.itos[item]\n",
    "            if item=='eos':\n",
    "                break\n",
    "            \n",
    "            if not item=='bos':\n",
    "                if item in self.reverse_dict.keys():\n",
    "                    item = self.reverse_dict[item]\n",
    "\n",
    "                output.append(item)\n",
    "        \n",
    "        return ''.join(output)\n",
    "    \n",
    "    \n",
    "class RegexVocab(Vocab):\n",
    "    def __init__(self, itos, pattern):\n",
    "        super().__init__(itos)\n",
    "        \n",
    "        self.pattern = pattern\n",
    "        self.regex = re.compile(self.pattern)\n",
    "        \n",
    "    def tokenize(self, smile):\n",
    "        toks = regex_tokenize(smile, self.regex)\n",
    "        toks = ['bos'] + toks + ['eos']\n",
    "        return toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def test_reconstruction(vocab, inputs):\n",
    "    fails = []\n",
    "    for item in inputs:\n",
    "        recon = vocab.reconstruct(vocab.numericalize(vocab.tokenize(item)))\n",
    "        if not item==recon:\n",
    "            fails.append((item, recon))\n",
    "            \n",
    "    return fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class SmilesDataset(Dataset):\n",
    "    def __init__(self, smiles, vocab):\n",
    "        self.smiles = smiles\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        smile = self.smiles[idx]\n",
    "        tokens = self.vocab.tokenize(smile)\n",
    "        ints = self.vocab.numericalize(tokens)\n",
    "        ints = torch.LongTensor(ints)\n",
    "        return ints\n",
    "    \n",
    "        \n",
    "class SmilesPredictionDataset(SmilesDataset):\n",
    "    def __init__(self, smiles, y_vals, vocab):\n",
    "        super().__init__(smiles, vocab)\n",
    "        self.y_vals = y_vals\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        ints = super().__getitem__(idx)\n",
    "        y_val = torch.FloatTensor(self.y_vals[i])\n",
    "        return (ints, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class FPDataset(Dataset):\n",
    "    def __init__(self, smiles, y_vals, vocab, fp_function):\n",
    "        self.smiles = smiles\n",
    "        self.vocab = vocab\n",
    "        self.y_vals = y_vals\n",
    "        self.fp_function = fp_function\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        smile = self.smiles[idx]\n",
    "        fp = fp_function(smile)\n",
    "        fp = torch.FloatTensor(fp)\n",
    "        y_val = torch.FloatTensor(self.y_vals[idx])\n",
    "        return (fp, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def batch_sequences(sequences, pad_idx):\n",
    "    \n",
    "    max_len = max([len(i) for i in sequences])+1\n",
    "    bs = len(sequences)\n",
    "    \n",
    "    batch_tensor = torch.zeros((bs, max_len)).long() + pad_idx\n",
    "    \n",
    "    for i,item in enumerate(sequences):\n",
    "        batch_tensor[i,:item.shape[0]] = item\n",
    "        \n",
    "    return batch_tensor\n",
    "    \n",
    "    \n",
    "def lm_collate(batch, pad_idx, batch_first=True):\n",
    "    \n",
    "    batch_tensor = batch_sequences(batch, pad_idx)\n",
    "        \n",
    "    if batch_first:\n",
    "        output = (batch_tensor[:,:-1], batch_tensor[:,1:])\n",
    "    else:\n",
    "        batch_tensor = batch_tensor.T\n",
    "        output = (batch_tensor[:-1,:], batch_tensor[1:,:])\n",
    "        \n",
    "    return output\n",
    "\n",
    "def sequence_prediction_collate(batch, pad_idx, batch_first=True):\n",
    "    \n",
    "    batch_tensor = batch_sequences([i[0] for i in batch])\n",
    "    y_vals = torch.stack([i[1] for i in batch])\n",
    "\n",
    "    if not batch_first:\n",
    "        batch_tensor = batch_tensor.T\n",
    "        \n",
    "    return (batch_tensor, y_vals)\n",
    "\n",
    "def fp_collate(batch):\n",
    "    fps = torch.stack([i[0] for i in batch])\n",
    "    y_vals = torch.stack([i[1] for i in batch])\n",
    "    return (fps, y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
