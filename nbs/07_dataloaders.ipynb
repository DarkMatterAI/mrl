{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders\n",
    "\n",
    "> Pytorch datasets, dataloaders, collate functions and vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from mrl.imports import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenzation defines how we break text strings (ie SMILES strings) down into subunits that are fed to the model. The standard process goes as follows:\n",
    "1. A tokenization process breaks a string down into tokens\n",
    "2. Tokens are mapped to integers\n",
    "3. The token integers are sent to the model\n",
    "\n",
    "This brings up the problem of how best to tokenize smiles. The following methods are implemented out of the box:\n",
    "\n",
    "### Character Tokenization\n",
    "\n",
    "Character Tokenization is when we break down SMILES by character. This is implemented with the `tokenize_by_character` function.\n",
    "\n",
    "```\n",
    "tokenize_by_character('CC[NH]CC')\n",
    ">> ['C', 'C', '[', 'N', 'H', ']', 'C', 'C']\n",
    "```\n",
    "\n",
    "This form of tokenization is quick and simple. One drawback of this approach is some characters might be overloaded. For example, `Br` is tokenized to `['B', 'r']`, leading to the `B` token meaning both boron (in the standard context) and Bromine (in the `Br` context). In practice, this isn't much of an issue. Language models are particularly adept at learning co-location of tokens.\n",
    "\n",
    "### Character Tokenization with Replacement\n",
    "\n",
    "Character tokenization with replacement is the same as character tokenization except we add a dictionary of multi-character tokens to be replaced with singel-character tokens. This dictinary has the form `{multi_character_token : single_character_token}`. Before tokenizing by character, all instances of `multi_character_token` are replaced with `single_character_token`. Character Tokenization with Replacement is implemented with the `tokenize_with_replacements` function.\n",
    "\n",
    "```\n",
    "replacement_dict = {'Br' : 'R', 'Cl' : 'L'}\n",
    "tokenize_with_replacements('[Cl]CC[Br]', replacement_dict)\n",
    ">> ['[', 'L', ']', 'C', 'C', '[', 'R', ']']\n",
    "```\n",
    "\n",
    "### Regex Tokenization\n",
    "\n",
    "Regex tokenization uses a regex string to decompose SMILES. This is mainly used to keep bracketed terms (ie `[O-]`) as single tokens. This method avoids character overloading by keeping all bracketed terms as individual tokens, but has issues with generating a large number of low frequency tokens. Regex tokenization is implemented with the `regex_tokenize` function\n",
    "\n",
    "```\n",
    "SMILE_REGEX = \"\"\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|H|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\"\"\n",
    "regex_tokenize('CCC[Br]', re.compile(SMILE_REGEX))\n",
    ">>['C', 'C', 'C', '[Br]']\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "SMILES_CHAR_VOCAB = ['#', '(', ')', '+', '-', '/', '0',\n",
    "                 '1', '2', '3', '4', '5', '6', '7',\n",
    "                 '8', '=', '@', 'B', 'C', 'F', 'H',\n",
    "                 'I', 'N', 'O', 'P', 'S', '[', '\\\\',\n",
    "                 ']', 'c', 'i', 'l', 'n', 'o', 'r', 's',\n",
    "                 '*', ':']\n",
    "\n",
    "\n",
    "SPECIAL_TOKENS = ['bos', 'eos', 'pad', 'unk']\n",
    "\n",
    "MAPPING_TOKENS = ['[1*:1]', '[2*:1]', '[1*:2]', '[2*:2]', '[1*:3]',\n",
    "                  '[2*:3]', '[1*:4]', '[2*:4]', '[1*:5]', '[2*:5]']\n",
    "\n",
    "HALOGEN_REPLACE = {'Br':'R',\n",
    "                   'Cl':'L'}\n",
    "\n",
    "MAPPING_REPLACE = {'[1*:1]':'A',\n",
    "                   '[2*:1]':'D',\n",
    "                   '[1*:2]':'E',\n",
    "                   '[2*:2]':'G',\n",
    "                   '[1*:3]':'J',\n",
    "                   '[2*:3]':'M',\n",
    "                   '[1*:4]':'Q',\n",
    "                   '[2*:4]':'T',\n",
    "                   '[1*:5]':'U', \n",
    "                   '[2*:5]':'V'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are regex patterns to decompose smiles into tokens\n",
    "\n",
    "`SMILE_REGEX` is based off [this work](https://github.com/pschwllr/MolecularTransformer/blob/master/README.md). The pattern decomposes SMILES into individual characters, but keeps `Cl`, `Br`, and any term in brackets (ie `[O-]`) intact. \n",
    "\n",
    "`MAPPING_REGEX` is a derivative of `SMILE_REGEX` designed to work with the mapping framework used with the `Block` class. `MAPPING_REGEX` keeps `Cl`, `Br`, and any string of the form `[{isotope}*:{map_num}]` intact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "SMILE_REGEX = \"\"\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|H|\\(|\\)|\\.|=|\n",
    "                 #|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|#|\\*|\\$|\\%[0-9]{2}|[0-9])\"\"\"\n",
    "\n",
    "MAPPING_REGEX = \"\"\"(\\[.\\*:.]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|H|\\[|\\]|\\(|\\)|\\.|=|\n",
    "                    #|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|#|\\*|\\$|\\%[0-9]{2}|[0-9])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def tokenize_by_character(input):\n",
    "    \"Splits `input` into inividual characters\"\n",
    "    return [i for i in input]\n",
    "\n",
    "def tokenize_with_replacements(input, replacement_dict):\n",
    "    \"Replaces substrings in `input` using `replacement_dict`, then tokenizes by character\"\n",
    "    for k,v in replacement_dict.items():\n",
    "        input = input.replace(k,v)\n",
    "    return [i for i in input]\n",
    "\n",
    "def regex_tokenize(input, regex):\n",
    "    'Uses `regex` to tokenize `input`'\n",
    "    tokens = [token for token in regex.findall(input)]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenize_by_character('CCC[Br]') == ['C', 'C', 'C', '[', 'B', 'r', ']']\n",
    "assert tokenize_with_replacements('CCC[Br]', HALOGEN_REPLACE) == ['C', 'C', 'C', '[', 'R', ']']\n",
    "assert regex_tokenize('CCC[Br]', re.compile(SMILE_REGEX)) == ['C', 'C', 'C', '[Br]']\n",
    "assert regex_tokenize('[1*:1]CCC[Br]', re.compile(MAPPING_REGEX)) == ['[1*:1]', 'C', 'C', 'C', '[', 'Br', ']']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "The `Vocab` class handles tokenization. `Vocab.tokenize` breaks strings down into tokens. `Vocab.numericalize` maps tokens to integers. `Vocab.reconstruct` converts integers back into strings.\n",
    "\n",
    "`Vocab` holds `itos`, a list of tokens, and `stoi`, a dictionary mapping tokens to integers. `Vocab` automatically adds four special tokens `['bos', 'eos', 'pad', 'unk']` indicating beginning of sentence, end of sentence, padding and unknown.\n",
    "\n",
    "### Custom Vocbulary\n",
    "\n",
    "To implement custom tokenization, subclass `Vocab` and update the `tokenize`, `numericalize` and `reconstruct` methods. Use the `test_reconstruction` function to verify your custom vocab can successfully reconstruct sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Vocab():\n",
    "    '''\n",
    "    Vocab - base vocabulary class\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "        `itos` - list, list of tokens in vocabulary\n",
    "    '''\n",
    "    def __init__(self, itos):\n",
    "        self.special_tokens = ['bos', 'eos', 'pad', 'unk']\n",
    "        \n",
    "        self.itos = self.special_tokens + [i for i in itos if not i in self.special_tokens]\n",
    "        self.stoi = {self.itos[i]:i for i in range(len(self.itos))}\n",
    "        self.unks = []\n",
    "        \n",
    "    def tokenize(self, input):\n",
    "        'Tokenize `input`'\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def numericalize(self, input):\n",
    "        'Numericalize `input` into integers'\n",
    "        output = []\n",
    "        for tok in input:\n",
    "            if tok in self.stoi.keys():\n",
    "                output.append(self.stoi[tok])\n",
    "            else:\n",
    "                output.append(self.stoi['unk'])\n",
    "                self.unks.append(tok)\n",
    "        return output\n",
    "    \n",
    "    def reconstruct(self, input):\n",
    "        'Reconstruct `input` into a string'\n",
    "        output = []\n",
    "        for item in input:\n",
    "            item = self.itos[item]\n",
    "            if item=='eos':\n",
    "                break\n",
    "                \n",
    "            if not item=='bos':\n",
    "                output.append(item)\n",
    "        \n",
    "        return ''.join(output)\n",
    "                \n",
    "    def update_vocab(self):\n",
    "        'Adds tokens in `self.unks` to vocabulary'\n",
    "        unks = list(set(self.unks))\n",
    "        self.itos += unks\n",
    "        self.stoi = {self.itos[i]:i for i in range(len(self.itos))}\n",
    "        self.unks = []\n",
    "        \n",
    "    def update_vocab_from_data(self, inputs):\n",
    "        'Tokenizes `inputs` and updates the vocabulary with any unknown tokens'\n",
    "        _ = [self.numericalize(self.tokenize(i)) for i in inputs]\n",
    "        self.update_vocab()\n",
    "        \n",
    "        \n",
    "class CharacterVocab(Vocab):\n",
    "    '''\n",
    "    CharacterVocab - tokenize by character\n",
    "    '''\n",
    "    def tokenize(self, input):\n",
    "        toks = tokenize_by_character(input)\n",
    "        toks = ['bos'] + toks + ['eos']\n",
    "        return toks\n",
    "    \n",
    "    \n",
    "class CharacterReplaceVocab(Vocab):\n",
    "    '''\n",
    "    CharacterReplaceVocab - tokenize by character with replacement\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "        `itos` - list, list of tokens\n",
    "        `replace_dict` - dict, replacement dictionary of the form {multi_character_token : single_character_token}. \n",
    "        ie replace_dict={'Br':'R', 'Cl':'L'}\n",
    "    '''\n",
    "    def __init__(self, itos, replace_dict):\n",
    "        itos = list(itos)\n",
    "        self.replace_dict = replace_dict\n",
    "        self.reverse_dict = {v:k for k,v in replace_dict.items()}\n",
    "        for rep in self.reverse_dict.keys():\n",
    "            if not rep in itos:\n",
    "                itos.append(rep)\n",
    "        super().__init__(itos)\n",
    "        \n",
    "    def tokenize(self, smile):\n",
    "        toks = tokenize_with_replacements(smile, self.replace_dict)\n",
    "        toks = ['bos'] + toks + ['eos']\n",
    "        return toks\n",
    "    \n",
    "    def reconstruct(self, input):\n",
    "        output = []\n",
    "        for item in input:\n",
    "            item = self.itos[item]\n",
    "            if item=='eos':\n",
    "                break\n",
    "            \n",
    "            if not item=='bos':\n",
    "                if item in self.reverse_dict.keys():\n",
    "                    item = self.reverse_dict[item]\n",
    "\n",
    "                output.append(item)\n",
    "        \n",
    "        return ''.join(output)\n",
    "    \n",
    "    \n",
    "class RegexVocab(Vocab):\n",
    "    '''\n",
    "    RegexVocab - tokenize using `pattern`\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "        `itos` - list, list of tokens\n",
    "        `pattern` - str, regex string\n",
    "    '''\n",
    "    def __init__(self, itos, pattern):\n",
    "        super().__init__(itos)\n",
    "        \n",
    "        self.pattern = pattern\n",
    "        self.regex = re.compile(self.pattern)\n",
    "        \n",
    "    def tokenize(self, smile):\n",
    "        toks = regex_tokenize(smile, self.regex)\n",
    "        toks = ['bos'] + toks + ['eos']\n",
    "        return toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def test_reconstruction(vocab, inputs):\n",
    "    \"Returns all items in `inputs` that can't be correctly reconstructed using `vocab`\"\n",
    "    fails = []\n",
    "    for item in inputs:\n",
    "        recon = vocab.reconstruct(vocab.numericalize(vocab.tokenize(item)))\n",
    "        if not item==recon:\n",
    "            fails.append((item, recon))\n",
    "            \n",
    "    return fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('files/smiles.csv')\n",
    "smiles = df.smiles.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = CharacterVocab(SMILES_CHAR_VOCAB)\n",
    "assert test_reconstruction(vocab, smiles)==[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = CharacterReplaceVocab(SMILES_CHAR_VOCAB, HALOGEN_REPLACE)\n",
    "assert vocab.tokenize('CC[Br]') == ['bos', 'C', 'C', '[', 'R', ']', 'eos']\n",
    "assert test_reconstruction(vocab, smiles)==[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = RegexVocab(SMILES_CHAR_VOCAB, SMILE_REGEX)\n",
    "assert vocab.tokenize('CC[Br]') == ['bos', 'C', 'C', '[Br]', 'eos']\n",
    "vocab.update_vocab_from_data(smiles)\n",
    "assert test_reconstruction(vocab, smiles)==[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collate Functions\n",
    "\n",
    "Collate functions are used to batch `Dataset` outputs into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def batch_sequences(sequences, pad_idx):\n",
    "    'Packs `sequences` into a dense tensor, using `pad_idx` for padding'\n",
    "    max_len = max([len(i) for i in sequences])+1\n",
    "    bs = len(sequences)\n",
    "    \n",
    "    batch_tensor = torch.zeros((bs, max_len)).long() + pad_idx\n",
    "    \n",
    "    for i,item in enumerate(sequences):\n",
    "        batch_tensor[i,:item.shape[0]] = item\n",
    "        \n",
    "    return batch_tensor\n",
    "    \n",
    "    \n",
    "def lm_collate(batch, pad_idx, batch_first=True):\n",
    "    '''\n",
    "    Collate function for language models. Returns packed \n",
    "    batch for next-token prediction\n",
    "    '''\n",
    "    batch_tensor = batch_sequences(batch, pad_idx)\n",
    "        \n",
    "    if batch_first:\n",
    "        output = (batch_tensor[:,:-1], batch_tensor[:,1:])\n",
    "    else:\n",
    "        batch_tensor = batch_tensor.T\n",
    "        output = (batch_tensor[:-1,:], batch_tensor[1:,:])\n",
    "        \n",
    "    return to_device(output)\n",
    "\n",
    "def sequence_prediction_collate(batch, pad_idx, batch_first=True):\n",
    "    '''\n",
    "    Collate function for predicting some y value from a sequence\n",
    "    '''\n",
    "    batch_tensor = batch_sequences([i[0] for i in batch], pad_idx)\n",
    "    y_vals = torch.stack([i[1] for i in batch])\n",
    "    y_vals = y_vals.squeeze(-1)\n",
    "\n",
    "    if not batch_first:\n",
    "        batch_tensor = batch_tensor.T\n",
    "        \n",
    "    return to_device((batch_tensor, y_vals))\n",
    "\n",
    "def vector_collate(batch):\n",
    "    '''\n",
    "    Collate function for vectors\n",
    "    '''\n",
    "    fps = torch.stack(batch)\n",
    "    return to_device(fps)\n",
    "\n",
    "def vector_reconstruction_collate(batch, pad_idx, batch_first=True):\n",
    "    '''\n",
    "    Collate function for predicting a sequence from an input vector where \n",
    "    `batch_tensor` is needed for input (ie predict SMILES from properties)\n",
    "    '''\n",
    "    fps = torch.stack([i[0] for i in batch])\n",
    "    batch_tensor = batch_sequences([i[1] for i in batch], pad_idx)\n",
    "    \n",
    "    if batch_first:\n",
    "        output = ((batch_tensor[:,:-1], fps), batch_tensor[:,1:])\n",
    "    else:\n",
    "        batch_tensor = batch_tensor.T\n",
    "        output = ((batch_tensor[:-1,:], fps), batch_tensor[1:,:])\n",
    "        \n",
    "    return to_device(output)\n",
    "\n",
    "def vector_prediction_collate(batch):\n",
    "    '''\n",
    "    Collate function for predicting some y value from a vector\n",
    "    '''\n",
    "    fps = torch.stack([i[0] for i in batch])\n",
    "    y_vals = torch.stack([i[1] for i in batch])\n",
    "    y_vals = y_vals.squeeze(-1)\n",
    "    return to_device((fps, y_vals))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "Datasets subclass the Pytorch `Dataset` class. MRL datasets add a collate function and the `BaseDataset.dataloader` function to easily generate Pytorch dataloaders from the same class\n",
    "\n",
    "Datasets should all contain a `new` method. The purpose of `new` is to create a new dataset from new data using the same input arguments as the current dataset. This is used during generative training to process and batch generated samples to ensure they are processed and batched the same as training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class BaseDataset(Dataset):\n",
    "    '''\n",
    "    BaseDataset - base dataset\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "        `collate_function` - batch collate function for the particular dataset class\n",
    "    '''\n",
    "    def __init__(self, collate_function):\n",
    "        self.collate_function = collate_function\n",
    "        \n",
    "    def dataloader(self, bs, num_workers=-1, **dl_kwargs):\n",
    "        if num_workers==-1:\n",
    "            if 'ncpus' in os.environ.keys():\n",
    "                num_workers = int(os.environ['ncpus'])\n",
    "            else:\n",
    "                num_workers=os.cpu_count()\n",
    "                \n",
    "        return DataLoader(self, batch_size=bs, num_workers=num_workers, \n",
    "                          collate_fn=self.collate_function, **dl_kwargs)\n",
    "    \n",
    "    def new(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def split(self, percent_valid):\n",
    "        \n",
    "        idxs = torch.randperm(self.__len__()).numpy()\n",
    "        train_length = int(len(smiles)*(1-percent_valid))\n",
    "        train_idxs = idxs[:train_length]\n",
    "        valid_idxs = idxs[train_length:]\n",
    "        \n",
    "        return self.split_on_idxs(train_idxs, valid_idxs)\n",
    "        \n",
    "    def split_on_idxs(self, train_idxs, valid_idxs):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Datasets\n",
    "\n",
    "Text datasets deal with tokenizing and numericalizing text data, like SMILES strings. \n",
    "\n",
    "`TextDataset` returns numericalized SMILES for language modeling.\n",
    "\n",
    "`TextPredictionDataset` returns numericaized SMILES along with some `y_val` output value, for tasks like property prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class TextDataset(BaseDataset):\n",
    "    '''\n",
    "    TextDataset - base dataset for language modes\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "        `smiles` - list[str], list of text sequences\n",
    "        \n",
    "        `vocab` - Vocab, vocabuary for tokenization/numericaization\n",
    "        \n",
    "        `collate_function` - batch collate function. If None, defauts to `lm_collate`\n",
    "    '''\n",
    "    def __init__(self, smiles, vocab, collate_function=None):\n",
    "        self.smiles = smiles\n",
    "        self.vocab = vocab\n",
    "        if collate_function is None:\n",
    "            collate_function = partial(lm_collate, pad_idx=self.vocab.stoi['pad'])\n",
    "        \n",
    "        super().__init__(collate_function)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        smile = self.smiles[idx]\n",
    "        tokens = self.vocab.tokenize(smile)\n",
    "        ints = self.vocab.numericalize(tokens)\n",
    "        ints = torch.LongTensor(ints)\n",
    "        return ints\n",
    "    \n",
    "    def new(self, smiles):\n",
    "        return self.__class__(smiles, self.vocab, self.collate_function)\n",
    "    \n",
    "    def split_on_idxs(self, train_idxs, valid_idxs):\n",
    "        \n",
    "        train_ds = self.new([self.smiles[i] for i in train_idxs])\n",
    "        valid_ds = self.new([self.smiles[i] for i in valid_idxs])\n",
    "        return (train_ds, valid_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('files/smiles.csv')\n",
    "vocab = CharacterVocab(SMILES_CHAR_VOCAB)\n",
    "\n",
    "ds = TextDataset(df.smiles.values, vocab)\n",
    "dl = ds.dataloader(16)\n",
    "x,y = next(iter(dl))\n",
    "\n",
    "assert (x[:,1:] == y[:,:-1]).all()\n",
    "\n",
    "assert sum([len(i) for i in ds.split(0.2)]) == len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class TextPredictionDataset(TextDataset):\n",
    "    '''\n",
    "    TextDataset - base dataset for predicting from text strings\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "        `smiles` - list[str], list of text sequences\n",
    "        \n",
    "        `y_vals` - list[int, float], list of paired output values\n",
    "        \n",
    "        `vocab` - Vocab, vocabuary for tokenization/numericaization\n",
    "        \n",
    "        `collate_function` - batch collate function. If None, defauts to `sequence_prediction_collate`\n",
    "    '''\n",
    "    def __init__(self, smiles, y_vals, vocab, collate_function=None):\n",
    "        \n",
    "        if collate_function is None:\n",
    "            collate_function = partial(sequence_prediction_collate, pad_idx=vocab.stoi['pad'])\n",
    "        \n",
    "        super().__init__(smiles, vocab, collate_function)\n",
    "        \n",
    "        self.y_vals = y_vals\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        ints = super().__getitem__(idx)\n",
    "        y_val = torch.Tensor([self.y_vals[idx]]).float()\n",
    "        return (ints, y_val)\n",
    "    \n",
    "    def new(self, smiles, y_vals):\n",
    "        return self.__class__(smiles, y_vals, self.vocab, self.collate_function)\n",
    "    \n",
    "    def split_on_idxs(self, train_idxs, valid_idxs):\n",
    "        \n",
    "        train_ds = self.new([self.smiles[i] for i in train_idxs],\n",
    "                            [self.y_vals[i] for i in train_idxs])\n",
    "        valid_ds = self.new([self.smiles[i] for i in valid_idxs],\n",
    "                            [self.y_vals[i] for i in valid_idxs])\n",
    "        \n",
    "        return (train_ds, valid_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = TextPredictionDataset(df.smiles.values, [0]*len(df.smiles.values), vocab)\n",
    "dl = ds.dataloader(16)\n",
    "x,y = next(iter(dl))\n",
    "assert (y == torch.zeros(y.shape).float()).all()\n",
    "assert sum([len(i) for i in ds.split(0.2)]) == len(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Datasets\n",
    "\n",
    "Another common dataset framework is where we are dealing with vectors derived from a molecule. This could be a vector of properties, fingerprints, or any task where a molecule-derived vector is needed.\n",
    "\n",
    "`Vector_Dataset` is a base dataset that simply returns the molecule derived vector\n",
    "\n",
    "`Vec_Recon_Dataset` returns the molecule derived vector and tokenized SMILES strings. This is used for tasks like generating compounds based on an input vector or fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Vector_Dataset(BaseDataset):\n",
    "    '''\n",
    "    Vector_Dataset - base dataset for molecule-derived vectors\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "        `smiles` - list[str], list of text sequences\n",
    "        \n",
    "        `mol_function` - function to convert smiles to a vector\n",
    "                        \n",
    "        `collate_function` - batch collate function. If None, defauts to `vector_collate`\n",
    "    '''\n",
    "    def __init__(self, smiles, mol_function, collate_function=None):\n",
    "        if collate_function is None:\n",
    "            collate_function = vector_collate\n",
    "        super().__init__(collate_function)\n",
    "        \n",
    "        self.smiles = smiles\n",
    "        self.mol_function = mol_function\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        smile = self.smiles[idx]\n",
    "        vec = self.mol_function(smile)\n",
    "        vec = torch.FloatTensor(vec)\n",
    "        return vec\n",
    "    \n",
    "    def new(self, smiles):\n",
    "        return self.__class__(smiles, self.mol_function, self.collate_function)\n",
    "    \n",
    "    def split_on_idxs(self, train_idxs, valid_idxs):\n",
    "        \n",
    "        train_ds = self.new([self.smiles[i] for i in train_idxs])\n",
    "        valid_ds = self.new([self.smiles[i] for i in valid_idxs])\n",
    "        \n",
    "        return (train_ds, valid_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/envs/mrl/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for boost::shared_ptr<RDKit::FilterCatalogEntry const> already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from mrl.chem import *\n",
    "\n",
    "def smile_to_props(smile):\n",
    "    mol = to_mol(smile)\n",
    "    weight = molwt(mol)\n",
    "    ps = tpsa(mol)\n",
    "    n_rings = rings(mol)\n",
    "    n_atoms = heavy_atoms(mol)\n",
    "    return np.array([weight/500, ps/100, n_rings/5, n_atoms/20])\n",
    "\n",
    "ds = Vector_Dataset(df.smiles.values, smile_to_props)\n",
    "dl = ds.dataloader(16)\n",
    "x = next(iter(dl))\n",
    "assert x.shape==(16,4)\n",
    "assert sum([len(i) for i in ds.split(0.2)]) == len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Vec_Recon_Dataset(Vector_Dataset):\n",
    "    '''\n",
    "    Vec_Recon_Dataset - base dataset for predicting smiles from molecule-derived vectors\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "        `smiles` - list[str], list of text sequences\n",
    "        \n",
    "        `vocab` - Vocab, vocabuary for tokenization/numericaization\n",
    "        \n",
    "        `mol_function` - function to convert smiles to fingerprints\n",
    "                        \n",
    "        `collate_function` - batch collate function. If None, defauts to `vector_reconstruction_collate`\n",
    "    '''\n",
    "    def __init__(self, smiles, vocab, mol_function, collate_function=None):\n",
    "        \n",
    "        if collate_function is None:\n",
    "            collate_function = partial(vector_reconstruction_collate, pad_idx=vocab.stoi['pad'])\n",
    "            \n",
    "        super().__init__(smiles, mol_function, collate_function)\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        smile = self.smiles[idx]\n",
    "        \n",
    "        vec = self.mol_function(smile)\n",
    "        vec = torch.FloatTensor(vec)\n",
    "        \n",
    "        tokens = self.vocab.tokenize(smile)\n",
    "        ints = self.vocab.numericalize(tokens)\n",
    "        ints = torch.LongTensor(ints)\n",
    "        \n",
    "        return (vec, ints)\n",
    "    \n",
    "    def new(self, smiles):\n",
    "        return self.__class__(smiles, self.vocab, self.mol_function, self.collate_function)\n",
    "    \n",
    "    def split_on_idxs(self, train_idxs, valid_idxs):\n",
    "        \n",
    "        train_ds = self.new([self.smiles[i] for i in train_idxs])\n",
    "        valid_ds = self.new([self.smiles[i] for i in valid_idxs])\n",
    "        \n",
    "        return (train_ds, valid_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Vec_Recon_Dataset(df.smiles.values, vocab, ECFP6)\n",
    "dl = ds.dataloader(16)\n",
    "x,y = next(iter(dl))\n",
    "assert len(x)==2\n",
    "assert (x[0][:,1:] == y[:,:-1]).all()\n",
    "assert sum([len(i) for i in ds.split(0.2)]) == len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Vec_Prediction_Dataset(Vector_Dataset):\n",
    "    '''\n",
    "    Vec_Prediction_Dataset - base dataset for predicting y_vals from molecule derived vectors\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "        `smiles` - list[str], list of text sequences\n",
    "        \n",
    "        `y_vals` - list[int, float], list of paired output values\n",
    "                \n",
    "        `mol_function` - function to convert smiles to fingerprints\n",
    "                        \n",
    "        `collate_function` - batch collate function. If None, defauts to `vector_prediction_collate`\n",
    "    '''\n",
    "    def __init__(self, smiles, y_vals, mol_function, collate_function=None):\n",
    "        if collate_function is None:\n",
    "            collate_function = vector_prediction_collate\n",
    "        super().__init__(smiles, mol_function, collate_function)\n",
    "        \n",
    "        self.y_vals = y_vals\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fp = super().__getitem__(idx)\n",
    "        y_val = torch.FloatTensor([self.y_vals[idx]])\n",
    "        return (fp, y_val)\n",
    "    \n",
    "    def new(self, smiles, y_vals):\n",
    "        return self.__class__(smiles, y_vals, self.mol_function, self.collate_function)\n",
    "    \n",
    "    \n",
    "    def split_on_idxs(self, train_idxs, valid_idxs):\n",
    "        \n",
    "        train_ds = self.new([self.smiles[i] for i in train_idxs],\n",
    "                            [self.y_vals[i] for i in train_idxs])\n",
    "        valid_ds = self.new([self.smiles[i] for i in valid_idxs],\n",
    "                            [self.y_vals[i] for i in valid_idxs])\n",
    "        \n",
    "        return (train_ds, valid_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Vec_Prediction_Dataset(df.smiles.values, [0 for i in df.smiles.values], ECFP6)\n",
    "dl = ds.dataloader(16)\n",
    "x,y = next(iter(dl))\n",
    "assert sum([len(i) for i in ds.split(0.2)]) == len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_chem.ipynb.\n",
      "Converted 02_template.filters.ipynb.\n",
      "Converted 03_template.template.ipynb.\n",
      "Converted 04_template.blocks.ipynb.\n",
      "Converted 05_torch_core.ipynb.\n",
      "Converted 06_layers.ipynb.\n",
      "Converted 07_dataloaders.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted template.overview.ipynb.\n",
      "Converted tutorials.ipynb.\n",
      "Converted tutorials.structure_enumeration.ipynb.\n",
      "Converted tutorials.template.advanced.ipynb.\n",
      "Converted tutorials.template.beginner.ipynb.\n",
      "Converted tutorials.template.intermediate.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
