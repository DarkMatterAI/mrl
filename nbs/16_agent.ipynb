{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n",
    "\n",
    "> Model agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from mrl.imports import *\n",
    "from mrl.core import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *\n",
    "from mrl.callbacks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "move latent to sampler. change latent data from `{name, idxs}` to `{name, latent_vectors}`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Agent(Callback):\n",
    "    def __init__(self, model, loss_function, dataset, opt_kwargs={}, clip=1., name='agent'):\n",
    "        super().__init__(name=name, order=2)\n",
    "        \n",
    "        self.model = model\n",
    "        to_device(self.model)\n",
    "        \n",
    "        self.loss_function = loss_function\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        self.opt = self.get_opt(self.model.parameters(), **opt_kwargs)\n",
    "        self.clip = clip\n",
    "        \n",
    "    def get_opt(self, parameters, **optim_kwargs):\n",
    "        return optim.Adam(parameters, **optim_kwargs)\n",
    "    \n",
    "    def setup(self):\n",
    "        pass\n",
    "    \n",
    "    def before_train(self):\n",
    "        pass\n",
    "    \n",
    "    def build_buffer(self):\n",
    "        pass\n",
    "    \n",
    "    def after_build_buffer(self):\n",
    "        pass\n",
    "    \n",
    "    def before_batch(self):\n",
    "        pass\n",
    "    \n",
    "    def sample_batch(self):\n",
    "        pass\n",
    "    \n",
    "    def after_sample(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        sequences = batch_state.samples\n",
    "                \n",
    "        batch_ds = self.dataset.new(sequences)\n",
    "        batch = batch_ds.collate_function([batch_ds[i] for i in range(len(batch_ds))])\n",
    "        batch = to_device(batch)\n",
    "        bs = len(batch_ds)\n",
    "        x,y = batch\n",
    "            \n",
    "        batch_state.x = x\n",
    "        batch_state.y = y\n",
    "        batch_state.bs = bs\n",
    "        batch_state.rewards = to_device(torch.zeros(bs))\n",
    "        batch_state.trajectory_rewards = None\n",
    "    \n",
    "    def get_model_outputs(self):\n",
    "        pass\n",
    "    \n",
    "    def compute_reward(self):\n",
    "        pass\n",
    "    \n",
    "    def after_compute_reward(self):\n",
    "        pass\n",
    "    \n",
    "    def reward_modification(self):\n",
    "        pass\n",
    "    \n",
    "    def compute_loss(self):\n",
    "        pass\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.opt.zero_grad()\n",
    "    \n",
    "    def before_step(self):\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), self.clip)\n",
    "    \n",
    "    def step(self):\n",
    "        self.opt.step()\n",
    "    \n",
    "    def after_batch(self):\n",
    "        pass\n",
    "    \n",
    "    def after_train(self):\n",
    "        pass\n",
    "    \n",
    "    def one_batch(self, batch):\n",
    "        batch = to_device(batch)\n",
    "        x,y = batch\n",
    "        if not isinstance(x, (list, tuple)):\n",
    "            x = [x]\n",
    "        output = self.model(*x)\n",
    "        loss = self.loss_function(output, y)\n",
    "        return loss\n",
    "    \n",
    "    def train_supervised(self, bs, epochs, lr, percent_valid=0.05):\n",
    "        \n",
    "        train_ds, valid_ds = self.dataset.split(percent_valid)\n",
    "        \n",
    "        train_dl = train_ds.dataloader(bs, shuffle=True)\n",
    "        valid_dl = valid_ds.dataloader(bs)\n",
    "        \n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(self.opt, max_lr=lr,\n",
    "                                                 steps_per_epoch=len(train_dl), epochs=10)\n",
    "\n",
    "        mb = master_bar(range(epochs))\n",
    "        mb.write(['Epoch', 'Train Loss', 'Valid  Loss', 'Time'], table=True)\n",
    "        for epoch in mb:\n",
    "            start = time.time()\n",
    "            train_losses = []\n",
    "            \n",
    "            for batch in progress_bar(train_dl, parent=mb):\n",
    "                \n",
    "                loss = self.one_batch(batch)\n",
    "\n",
    "                self.zero_grad()\n",
    "                loss.backward()\n",
    "                self.step()\n",
    "                scheduler.step()\n",
    "                train_losses.append(loss.detach().cpu())\n",
    "                mb.child.comment = f\"{train_losses[-1]:.5f}\"\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                valid_losses = []\n",
    "                for batch in progress_bar(valid_dl, parent=mb):\n",
    "\n",
    "                    loss = self.one_batch(batch)\n",
    "                    valid_losses.append(loss.detach().cpu())\n",
    "                    mb.child.comment = f\"{valid_losses[-1]:.5f}\"\n",
    "                    \n",
    "            train_loss = smooth_batches(train_losses)\n",
    "            valid_loss = smooth_batches(valid_losses)\n",
    "            end = time.time() - start\n",
    "            mb.write([epoch, f'{train_losses[-1]:.5f}', \n",
    "                      f'{valid_losses[-1]:.5f}', f'{format_time(end)}'], table=True)\n",
    "    \n",
    "    def update_dataset(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    def update_dataset_from_inputs(self, *dataset_inputs):\n",
    "        dataset = self.dataset.new(*dataset_inputs)\n",
    "        self.update_dataset(dataset)\n",
    "    \n",
    "    def load_weights(self, filename):\n",
    "        state_dict = torch.load(filename, map_location=get_model_device(self.model))\n",
    "        \n",
    "        self.model.load_state_dict(state_dict)\n",
    "\n",
    "    def save_weights(self, filename):\n",
    "        \n",
    "        state_dict = self.model.state_dict()\n",
    "        torch.save(state_dict, filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "    \n",
    "class PredictiveAgent(Agent):\n",
    "    \n",
    "    def predict_tensor(self, x):\n",
    "        if not isinstance(x, (list, tuple)):\n",
    "            x = [x]\n",
    "        output = self.model(*x)\n",
    "        return output\n",
    "        \n",
    "    def predict_data(self, data):\n",
    "        ds = self.dataset.new(data, [0 for i in data])\n",
    "        batch = ds.collate_function([ds[i] for i in range(len(ds))])\n",
    "        batch = to_device(batch)\n",
    "        x,y = batch\n",
    "        return self.predict_tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class BaselineAgent(Agent):\n",
    "    def __init__(self, model, loss_function, dataset, base_update=0.99,\n",
    "                 base_update_iter=10, base_model=True, opt_kwargs={}, \n",
    "                 clip=1., name='baseline_agent'):\n",
    "        super().__init__(model, loss_function, dataset, opt_kwargs, clip, name)\n",
    "        \n",
    "        self.set_models(base_model)\n",
    "        self.base_update = base_update\n",
    "        self.base_update_iter = base_update_iter\n",
    "        \n",
    "    def after_batch(self):\n",
    "        log = self.environment.log\n",
    "        iterations = log.iterations\n",
    "        if iterations%self.base_update_iter == 0 and iterations>0:\n",
    "            self.update_base_model()\n",
    "        \n",
    "    def set_models(self, base_model):\n",
    "        \n",
    "        if base_model==True:\n",
    "            self.base_model = copy.deepcopy(self.model)\n",
    "        else:\n",
    "            self.base_model = base_model\n",
    "            \n",
    "        try:\n",
    "            to_device(self.base_model)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    def base_to_model(self):\n",
    "        if type(self.base_model)==type(self.model):\n",
    "            self.base_model.load_state_dict(self.model.state_dict())\n",
    "            \n",
    "    def model_to_base(self):\n",
    "        if type(self.base_model)==type(self.model):\n",
    "            self.model.load_state_dict(self.base_model.state_dict())\n",
    "            \n",
    "    def update_base_model(self):\n",
    "        if type(self.base_model)==type(self.model):\n",
    "            if self.base_update < 1:\n",
    "                merge_models(self.base_model, self.model, alpha=self.base_update)\n",
    "                \n",
    "    def save_weights(self, filename):\n",
    "        state_dict = {}\n",
    "        state_dict['model'] = self.model.state_dict()\n",
    "        \n",
    "        if isinstance(self.base_model, nn.Module):\n",
    "            state_dict['base_model'] = self.base_model.state_dict()\n",
    "        else:\n",
    "            state_dict['base_model'] = None\n",
    "            \n",
    "        torch.save(state_dict, filename)\n",
    "        \n",
    "    def load_weights(self, filename):\n",
    "        state_dict = torch.load(filename, map_location=get_model_device(self.model))\n",
    "        \n",
    "        self.model.load_state_dict(state_dict['model'])\n",
    "        \n",
    "        if isinstance(self.base_model, nn.Module):\n",
    "            self.base_model.load_state_dict(state_dict['base_model'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class CriticAgent(BaselineAgent):\n",
    "    \n",
    "    def predict_tensor(self, x, baseline=False):\n",
    "        if not type(x)==list:\n",
    "            x = [x]\n",
    "        \n",
    "        if baseline:\n",
    "            if isinstance(self.base_model, nn.Module):\n",
    "                output = self.base_model(*x)\n",
    "            else:\n",
    "                output = None\n",
    "        else:\n",
    "            output = self.model(*x)\n",
    "            \n",
    "        return output\n",
    "        \n",
    "    def predict_data(self, data):\n",
    "        ds = self.dataset.new(data, [0 for i in data])\n",
    "        batch = ds.collate_function([ds[i] for i in range(len(ds))])\n",
    "        batch = to_device(batch)\n",
    "        x,y = batch\n",
    "        return self.predict_tensor(x)\n",
    "    \n",
    "    def get_model_outputs(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        x = batch_state.x\n",
    "        y = batch_state.y\n",
    "        \n",
    "        preds = self.predict_tensor(x, baseline=False)\n",
    "        batch_state.model_output = preds\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            base_preds = self.predict_tensor(x, baseline=True)\n",
    "            batch_state.base_output = base_preds\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class GenerativeAgent(BaselineAgent):\n",
    "    def __init__(self, model, vocab, loss_function, dataset, \n",
    "                 base_update=0.99, base_update_iter=10, base_model=True, \n",
    "                 opt_kwargs={}, clip=1., name='generative_agent'):\n",
    "        super().__init__(model, loss_function, dataset, \n",
    "                         base_update=base_update, \n",
    "                         base_update_iter=base_update_iter, \n",
    "                         base_model=base_model, \n",
    "                         opt_kwargs=opt_kwargs,\n",
    "                         clip=clip,\n",
    "                         name=name)\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def reconstruct(self, preds):\n",
    "        return maybe_parallel(self.vocab.reconstruct, [i for i in preds.detach().cpu()])\n",
    "        \n",
    "    def after_sample(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        sequences = batch_state.samples\n",
    "                \n",
    "        batch_ds = self.dataset.new(sequences)\n",
    "        batch = batch_ds.collate_function([batch_ds[i] for i in range(len(batch_ds))])\n",
    "        batch = to_device(batch)\n",
    "        bs = len(batch_ds)\n",
    "        x,y = batch\n",
    "            \n",
    "        batch_state.x = x\n",
    "        batch_state.y = y\n",
    "        batch_state.bs = bs\n",
    "        mask = ~(y==self.vocab.stoi['pad'])\n",
    "        batch_state.mask = mask\n",
    "        batch_state.lengths = mask.sum(-1)\n",
    "        batch_state.sl = y.shape[-1]\n",
    "        batch_state.rewards = to_device(torch.zeros(bs))\n",
    "        batch_state.trajectory_rewards = to_device(torch.zeros(y.shape))\n",
    "        \n",
    "    def get_rl_tensors(self, model, x, y, latent_info, sources):\n",
    "        if latent_info:\n",
    "            latent_sources = []\n",
    "            output_tensors = []\n",
    "            for (latent_source, latents) in latent_info.items():\n",
    "                latent_sources.append(latent_source)\n",
    "#                 latent_mask = torch.tensor([i==latent_source for i in sources]).bool()\n",
    "#                 latents = self.agent.latents[latent_idxs]\n",
    "                out = self.agent.model.get_rl_tensors(subset_tensor(x, latent_mask), \n",
    "                                                      subset_tensor(y, latent_mask),\n",
    "                                                      latent=latents)\n",
    "                output_tensors.append(out)\n",
    "                \n",
    "            non_latent_mask = torch.tensor([not i in latent_sources for i in sources]).bool()\n",
    "            if non_latent_mask.sum()>0:\n",
    "                out = model.get_rl_tensors(subset_tensor(x, non_latent_mask), \n",
    "                                           subset_tensor(y, non_latent_mask))\n",
    "                output_tensors.append(out)\n",
    "            \n",
    "            mo = torch.cat([i[0] for i in output_tensors], 0)\n",
    "            mlp = torch.cat([i[1] for i in output_tensors], 0)\n",
    "            mglp = torch.cat([i[2] for i in output_tensors], 0)\n",
    "            me = torch.cat([i[3] for i in output_tensors], 0)\n",
    "            \n",
    "        else:\n",
    "            mo, mlp, mglp, me = model.get_rl_tensors(x,y)\n",
    "            \n",
    "        return mo, mlp, mglp, me \n",
    "    \n",
    "    def get_model_outputs(self):\n",
    "            \n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        \n",
    "        x = batch_state.x\n",
    "        y = batch_state.y\n",
    "        sources = batch_state.sources\n",
    "        latent_info = batch_state.latent_data\n",
    "            \n",
    "        mo, mlp, mglp, me = self.get_rl_tensors(self.model, x, y, latent_info, sources)\n",
    "        mprob = mlp.exp()\n",
    "        \n",
    "        batch_state.model_output = mo\n",
    "        batch_state.model_logprobs = mlp\n",
    "        batch_state.model_gathered_logprobs = mglp\n",
    "        batch_state.model_encoded = me\n",
    "        batch_state.y_gumbel = F.one_hot(y, len(self.vocab.itos)) + mprob - mprob.detach()\n",
    "        batch_state.value_input = me\n",
    "        \n",
    "#         if self.agent.value_head is not None:\n",
    "#             value_predictions = self.agent.value_head(me)\n",
    "#             with torch.no_grad():\n",
    "#                 base_value_predictions = self.agent.base_value_head(me)\n",
    "#         else:\n",
    "#             value_predictions = None\n",
    "#             base_value_predictions = None\n",
    "            \n",
    "#         self.batch_state.state_values = value_predictions\n",
    "#         self.batch_state.ref_state_values = base_value_predictions\n",
    "        \n",
    "        if self.base_model is not None:\n",
    "            with torch.no_grad():\n",
    "                bo, blp, bglp, be = self.get_rl_tensors(self.base_model, x, y, latent_info, sources)    \n",
    "        else:\n",
    "            bo, blp, bglp, be = None, None, None, None\n",
    "            \n",
    "        batch_state.base_output = bo\n",
    "        batch_state.base_logprobs = blp\n",
    "        batch_state.base_gathered_logprobs = bglp\n",
    "        batch_state.base_encoded = be\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class SupevisedCB(Callback):\n",
    "    def __init__(self, agent, frequency, base_update, percentile, \n",
    "                 lr, bs, log_term='rewards'):\n",
    "        super().__init__('supervised', order=1000)\n",
    "        self.agent = agent\n",
    "        self.frequency = frequency\n",
    "        self.base_update = base_update\n",
    "        self.percentile = percentile\n",
    "        self.lr = lr\n",
    "        self.bs = bs\n",
    "        self.log_term = log_term\n",
    "        \n",
    "    def after_batch(self):\n",
    "        env = self.environment\n",
    "        iterations = self.environment.log.iterations\n",
    "        \n",
    "        if iterations>0 and iterations%self.frequency==0:\n",
    "            self.train_model()\n",
    "            \n",
    "            \n",
    "    def train_model(self):\n",
    "        env = self.environment\n",
    "        df = log_to_df(env.log.log, ['samples', self.log_term])\n",
    "        df.drop_duplicates(subset='samples', inplace=True)\n",
    "        df = df[df.rewards>np.percentile(df.rewards.values, self.percentile)]\n",
    "\n",
    "        self.agent.update_dataset_from_inputs(df.samples.values)\n",
    "        self.agent.train_supervised(self.bs, 1, self.lr)\n",
    "\n",
    "        merge_models(self.agent.base_model, self.agent.model, alpha=self.base_update)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "# standard lm\n",
    "\n",
    "from mrl.vocab import *\n",
    "from mrl.dataloaders import *\n",
    "from mrl.g_models import *\n",
    "\n",
    "df = pd.read_csv('files/smiles.csv')\n",
    "vocab = CharacterVocab(SMILES_CHAR_VOCAB)\n",
    "\n",
    "ds = Text_Dataset(df.smiles.values, vocab)\n",
    "\n",
    "d_vocab = len(vocab.itos)\n",
    "d_embedding = 256\n",
    "d_hidden = 1024\n",
    "n_layers = 3\n",
    "input_dropout = 0.3\n",
    "lstm_dropout = 0.3\n",
    "bos_idx = vocab.stoi['bos']\n",
    "bidir = False\n",
    "tie_weights = True\n",
    "\n",
    "model = LSTM_LM(d_vocab, \n",
    "                d_embedding,\n",
    "                d_hidden, \n",
    "                n_layers,\n",
    "                input_dropout,\n",
    "                lstm_dropout,\n",
    "                bos_idx, \n",
    "                bidir, \n",
    "                tie_weights)\n",
    "\n",
    "model.load_state_dict(torch.load('untracked_files/lstm_lm_zinc.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = GenerativeAgent(model, vocab, CrossEntropy(), ds, opt_kwargs={'lr':1e-4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Valid  Loss</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.17034</td>\n",
       "      <td>1.22670</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.train_supervised(64, 1, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = NormalPrior(torch.zeros((512,)), torch.zeros((512,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NormalPrior()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_device(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class AgentCallback(Callback):\n",
    "#     def __init__(self, agent, name, clip=1.):\n",
    "#         super().__init__(order=20)\n",
    "#         self.agent = agent\n",
    "#         self.name = name\n",
    "#         self.clip = clip\n",
    "        \n",
    "#     def zero_grad(self):\n",
    "#         self.agent.zero_grad()\n",
    "    \n",
    "#     def before_step(self):\n",
    "#         nn.utils.clip_grad_norm_(self.agent.model.parameters(), self.clip)\n",
    "        \n",
    "#     def step(self):\n",
    "#         self.agent.step()\n",
    "        \n",
    "#     def after_sample(self):\n",
    "#         env = self.environment\n",
    "#         sequences = self.batch_state.samples\n",
    "#         diversity = len(set(sequences))/len(sequences)\n",
    "                \n",
    "#         bs = len(sequences)\n",
    "#         self.batch_state.rewards = to_device(torch.zeros(bs))\n",
    "        \n",
    "#     def get_model_outputs(self):\n",
    "#         # get relevant model outputs\n",
    "#         pass\n",
    "        \n",
    "# class GenAgentCallback(AgentCallback):\n",
    "#     def __init__(self, agent, name, contrastive=False):\n",
    "#         super().__init__(agent, name)\n",
    "#         self.contrastive = contrastive\n",
    "    \n",
    "#     def after_sample(self):\n",
    "#         env = self.environment\n",
    "#         sequences = self.batch_state.samples\n",
    "                \n",
    "#         batch_ds = self.agent.dataset.new(sequences)\n",
    "#         batch = batch_ds.collate_function([batch_ds[i] for i in range(len(batch_ds))])\n",
    "#         batch = to_device(batch)\n",
    "#         bs = len(batch_ds)\n",
    "#         x,y = batch\n",
    "            \n",
    "#         self.batch_state.x = x\n",
    "#         self.batch_state.y = y\n",
    "#         self.batch_state.bs = bs\n",
    "#         mask = ~(y==self.agent.vocab.stoi['pad'])\n",
    "#         self.batch_state.mask = mask\n",
    "#         self.batch_state.lengths = mask.sum(-1)\n",
    "#         self.batch_state.sl = y.shape[-1]\n",
    "#         self.batch_state.rewards = to_device(torch.zeros(bs))\n",
    "#         self.batch_state.trajectory_rewards = to_device(torch.zeros(y.shape))\n",
    "        \n",
    "#     def subset_tensor(self, x, mask):\n",
    "#         if type(x)==list:\n",
    "#             x = [i[mask] for i in x]\n",
    "#         else:\n",
    "#             x = x[mask]\n",
    "        \n",
    "#         return x\n",
    "    \n",
    "#     def get_rl_tensors(self, model, x, y, latent_info, sources):\n",
    "#         if latent_info:\n",
    "#             latent_sources = []\n",
    "#             output_tensors = []\n",
    "#             for (latent_source, latent_idxs) in latent_info.items():\n",
    "#                 latent_sources.append(latent_source)\n",
    "#                 latent_mask = torch.tensor([i==latent_source for i in sources]).bool()\n",
    "#                 latents = self.agent.latents[latent_idxs]\n",
    "#                 out = self.agent.model.get_rl_tensors(self.subset_tensor(x, latent_mask), \n",
    "#                                                       self.subset_tensor(y, latent_mask),\n",
    "#                                                       latent=latents)\n",
    "#                 output_tensors.append(out)\n",
    "                \n",
    "#             non_latent_mask = torch.tensor([not i in latent_sources for i in sources]).bool()\n",
    "#             if non_latent_mask.sum()>0:\n",
    "#                 out = model.get_rl_tensors(self.subset_tensor(x, non_latent_mask), \n",
    "#                                                       self.subset_tensor(y, non_latent_mask))\n",
    "#                 output_tensors.append(out)\n",
    "            \n",
    "#             mo = torch.cat([i[0] for i in output_tensors], 0)\n",
    "#             mlp = torch.cat([i[1] for i in output_tensors], 0)\n",
    "#             mglp = torch.cat([i[2] for i in output_tensors], 0)\n",
    "#             me = torch.cat([i[3] for i in output_tensors], 0)\n",
    "            \n",
    "#         else:\n",
    "#             mo, mlp, mglp, me = model.get_rl_tensors(x,y)\n",
    "            \n",
    "#         return mo, mlp, mglp, me \n",
    "        \n",
    "#     def get_model_outputs(self):\n",
    "            \n",
    "#         x = self.batch_state.x\n",
    "#         y = self.batch_state.y\n",
    "#         sources = self.batch_state.sources\n",
    "#         latent_info = self.batch_state.latent_data\n",
    "            \n",
    "#         mo, mlp, mglp, me = self.get_rl_tensors(self.agent.model, x, y, latent_info, sources)\n",
    "#         mprob = mlp.exp()\n",
    "        \n",
    "#         self.batch_state.model_output = mo\n",
    "#         self.batch_state.model_logprobs = mlp\n",
    "#         self.batch_state.model_gathered_logprobs = mglp\n",
    "#         self.batch_state.model_encoded = me\n",
    "#         self.batch_state.y_gumbel = F.one_hot(y, len(self.agent.vocab.itos)) + mprob - mprob.detach()\n",
    "        \n",
    "#         if self.agent.value_head is not None:\n",
    "#             value_predictions = self.agent.value_head(me)\n",
    "#             with torch.no_grad():\n",
    "#                 base_value_predictions = self.agent.base_value_head(me)\n",
    "#         else:\n",
    "#             value_predictions = None\n",
    "#             base_value_predictions = None\n",
    "            \n",
    "#         self.batch_state.state_values = value_predictions\n",
    "#         self.batch_state.ref_state_values = base_value_predictions\n",
    "        \n",
    "#         if self.agent.base_model is not None:\n",
    "#             with torch.no_grad():\n",
    "# #                 bo, blp, bglp, be = self.agent.base_model.get_rl_tensors(x,y)\n",
    "#                 bo, blp, bglp, be = self.get_rl_tensors(self.agent.base_model, x, y, latent_info, sources)    \n",
    "#         else:\n",
    "#             bo, blp, bglp, be = None, None, None, None\n",
    "            \n",
    "#         self.batch_state.reference_output = bo\n",
    "#         self.batch_state.reference_logprobs = blp\n",
    "#         self.batch_state.reference_gathered_logprobs = bglp\n",
    "#         self.batch_state.reference_encoded = be\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class BaselineAgent(Agent):\n",
    "#     def __init__(self, model, loss_function, dataset, base_update=0.99, v_update=0.95,\n",
    "#                 base_model=True, value_head=None, opt_kwargs={}, vopt_kwargs={}):\n",
    "#         super().__init__(model, loss_function, dataset, opt_kwargs)\n",
    "        \n",
    "#         self.opts = [self.opt]\n",
    "#         self.set_models(base_model, value_head, vopt_kwargs)\n",
    "#         self.base_update = base_update\n",
    "#         self.v_update = v_update\n",
    "        \n",
    "#     def set_models(self, base_model, value_head, vopt_kwargs):\n",
    "        \n",
    "#         if base_model==True:\n",
    "#             self.base_model = copy.deepcopy(self.model)\n",
    "#         else:\n",
    "#             self.base_model = base_model\n",
    "            \n",
    "#         try:\n",
    "#             to_device(self.base_model)\n",
    "#         except:\n",
    "#             pass\n",
    "        \n",
    "#         self.value_head = value_head\n",
    "#         if self.value_head is not None:\n",
    "#             self.base_value_head = copy.deepcopy(self.value_head)\n",
    "#             to_device(self.value_head)\n",
    "#             to_device(self.base_value_head)\n",
    "            \n",
    "#             self.value_opt = self.get_opt(self.value_head.parameters(), **vopt_kwargs)\n",
    "#             self.opts.append(self.value_opt)\n",
    "            \n",
    "#     def zero_grad(self):\n",
    "#         for opt in self.opts:\n",
    "#             opt.zero_grad()\n",
    "            \n",
    "#     def step(self):\n",
    "#         for opt in self.opts:\n",
    "#             opt.step()\n",
    "            \n",
    "#     def base_to_model(self):\n",
    "#         if type(self.base_model)==type(self.model):\n",
    "#             self.base_model.load_state_dict(self.model.state_dict())\n",
    "            \n",
    "#     def model_to_base(self):\n",
    "#         if type(self.base_model)==type(self.model):\n",
    "#             self.model.load_state_dict(self.base_model.state_dict())\n",
    "            \n",
    "#     def update_base_models(self):\n",
    "#         if type(self.base_model)==type(self.model):\n",
    "#             if self.base_update < 1:\n",
    "#                 merge_models(self.base_model, self.model, alpha=self.base_update)\n",
    "            \n",
    "#         if self.value_head is not None:\n",
    "#             if self.v_update < 1:\n",
    "#                 merge_models(self.base_value_head, self.value_head, alpha=self.v_update)\n",
    "                \n",
    "#     def save_weights(self, filename):\n",
    "#         state_dict = {}\n",
    "#         state_dict['model'] = self.model.state_dict()\n",
    "        \n",
    "#         if isinstance(self.base_model, nn.Module):\n",
    "#             state_dict['base_model'] = self.base_model.state_dict()\n",
    "#         else:\n",
    "#             state_dict['base_model'] = None\n",
    "            \n",
    "#         if isinstance(self.value_head, nn.Module):\n",
    "#             state_dict['value_head'] = self.value_head.state_dict()\n",
    "#             state_dict['base_value_head'] = self.base_value_head.state_dict()\n",
    "#         else:\n",
    "#             state_dict['value_head'] = None\n",
    "#             state_dict['base_value_head'] = None\n",
    "            \n",
    "#         torch.save(state_dict, filename)\n",
    "        \n",
    "#     def load_weights(self, filename):\n",
    "#         state_dict = torch.load(filename, map_location=get_model_device(self.model))\n",
    "        \n",
    "#         self.model.load_state_dict(state_dict['model'])\n",
    "        \n",
    "#         if isinstance(self.base_model, nn.Module):\n",
    "#             self.base_model.load_state_dict(state_dict['base_model'])\n",
    "            \n",
    "#         if isinstance(self.value_head, nn.Module):\n",
    "#             self.value_head.load_state_dict(state_dict['value_head'])\n",
    "#             self.base_value_head.load_state_dict(state_dict['base_value_head'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class GenerativeAgent(BaselineAgent):\n",
    "#     def __init__(self, model, vocab, loss_function, dataset, base_update=0.99,\n",
    "#                  v_update=0.95, base_model=True, value_head=None, latents=None,\n",
    "#                  opt_kwargs={}, vopt_kwargs={}, lopt_kwargs={}):\n",
    "#         super().__init__(model, loss_function, dataset, \n",
    "#                          base_update=base_update, v_update=v_update,\n",
    "#                          base_model=base_model, value_head=value_head, \n",
    "#                          opt_kwargs=opt_kwargs, vopt_kwargs=vopt_kwargs)\n",
    "        \n",
    "#         self.vocab = vocab\n",
    "#         self.set_latent(latents, lopt_kwargs)\n",
    "        \n",
    "#     def set_latent(self, latents, lopt_kwargs):\n",
    "                    \n",
    "#         self.latents = latents\n",
    "#         if self.latents is not None:\n",
    "#             self.latents = to_device(self.latents)\n",
    "#             self.latents = nn.Parameter(self.latents)\n",
    "#             self.latent_opt = self.get_opt([self.latents], **lopt_kwargs)\n",
    "#             self.opts.append(self.latent_opt)\n",
    "            \n",
    "#     def reconstruct(self, preds):\n",
    "#         return maybe_parallel(self.vocab.reconstruct, [i for i in preds.detach().cpu()])\n",
    "    \n",
    "#     def reconstruct_trajectory(self, preds):\n",
    "#         trajectories = maybe_parallel(self.vocab.reconstruct_trajectory, [i for i in preds.detach().cpu()])\n",
    "#         return trajectories\n",
    "    \n",
    "#     def get_batch_params(self, model_output):\n",
    "#         x = model_output['x']\n",
    "#         y = model_output['y']\n",
    "#         mask = ~(y==self.vocab.stoi['pad'])\n",
    "#         lengths = mask.sum(-1)\n",
    "#         sl = y.shape[-1]\n",
    "#         trajectories = self.reconstruct_trajectory(y)\n",
    "#         smiles = [i[-1] if i else '' for i in trajectories]\n",
    "        \n",
    "#         model_output['mask'] = mask\n",
    "#         model_output['lengths'] = lengths\n",
    "#         model_output['sl'] = sl\n",
    "#         model_output['sequences'] = smiles\n",
    "#         model_output['sequence_trajectories'] = trajectories\n",
    "        \n",
    "#         return model_output\n",
    "        \n",
    "    \n",
    "#     def get_model_outputs(self, model_output):\n",
    "#         x = model_output['x']\n",
    "#         y = model_output['y']\n",
    "#         latent = model_output['latent']\n",
    "#         mo, mlp, mglp, me = self.model.get_rl_tensors(x,y,latent=latent)\n",
    "#         mprob = mlp.exp()\n",
    "    \n",
    "#         model_output['model_output'] = mo\n",
    "#         model_output['model_logprobs'] = mlp\n",
    "#         model_output['model_gathered_logprobs'] = mglp\n",
    "#         model_output['model_encoded'] = me\n",
    "#         model_output['y_gumbel'] = F.one_hot(y, len(self.vocab.itos)) + mprob - mprob.detach()\n",
    "        \n",
    "#         if self.value_head is not None:\n",
    "#             value_predictions = self.value_head(me)\n",
    "#             with torch.no_grad():\n",
    "#                 base_value_predictions = self.base_value_head(me)\n",
    "#         else:\n",
    "#             value_predictions = None\n",
    "#             base_value_predictions = None\n",
    "            \n",
    "#         model_output['state_values'] = value_predictions\n",
    "#         model_output['old_state_values'] = base_value_predictions\n",
    "        \n",
    "#         if self.base_model is not None:\n",
    "#             with torch.no_grad():\n",
    "#                 bo, blp, bglp, be = self.base_model.get_rl_tensors(x,y)\n",
    "#         else:\n",
    "#             bo, blp, bglp, be = None, None, None, None\n",
    "\n",
    "#         model_output['reference_output'] = bo\n",
    "#         model_output['reference_logprobs'] = blp\n",
    "#         model_output['reference_gathered_logprobs'] = bglp\n",
    "#         model_output['reference_encoded'] = be\n",
    "    \n",
    "#         return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class CriticAgent(BaselineAgent):\n",
    "#     def __init__(self, model, loss_function, dataset, base_update=0.99,\n",
    "#                 base_model=True, opt_kwargs={}):\n",
    "#         super().__init__(model, loss_function, dataset, \n",
    "#                          base_update=base_update,\n",
    "#                          base_model=base_model, value_head=None, \n",
    "#                          opt_kwargs=opt_kwargs, vopt_kwargs={})\n",
    "    \n",
    "#     def predict_tensor(self, x, baseline=False):\n",
    "#         if not type(x)==list:\n",
    "#             x = [x]\n",
    "        \n",
    "#         if baseline:\n",
    "#             output = self.base_model(*x)\n",
    "#         else:\n",
    "#             output = self.model(*x)\n",
    "        \n",
    "#     def predict_data(self, data):\n",
    "#         ds = self.dataset.new(data, [0 for i in data])\n",
    "#         batch = ds.collate_function([ds[i] for i in range(len(ds))])\n",
    "#         batch = to_device(batch)\n",
    "#         x,y = batch\n",
    "#         return self.predict_tensor(x)\n",
    "    \n",
    "#     def get_model_outputs(self, model_output):\n",
    "#         x = model_output['x']\n",
    "#         y = model_output['y']\n",
    "        \n",
    "#         model_output['model_output'] = self.predict_tensor(x, baseline=False)\n",
    "#         model_output['reference_output'] = self.predict_tensor(x, baseline=True)\n",
    "    \n",
    "#         return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mrl)",
   "language": "python",
   "name": "mrl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
