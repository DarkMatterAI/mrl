{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n",
    "\n",
    "> Model agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from mrl.imports import *\n",
    "from mrl.core import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *\n",
    "from mrl.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Agent(Callback):\n",
    "    def __init__(self, model, loss_function, dataset, opt_kwargs={}, clip=1., name='agent'):\n",
    "        super().__init__(name=name, order=2)\n",
    "        \n",
    "        self.model = model\n",
    "        to_device(self.model)\n",
    "        \n",
    "        self.loss_function = loss_function\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        self.opt = self.get_opt(self.model.parameters(), **opt_kwargs)\n",
    "        self.clip = clip\n",
    "        \n",
    "    def get_opt(self, parameters, **optim_kwargs):\n",
    "        return optim.Adam(parameters, **optim_kwargs)\n",
    "    \n",
    "    def before_compute_reward(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        sequences = batch_state.samples\n",
    "                \n",
    "        batch_ds = self.dataset.new(sequences)\n",
    "        batch = batch_ds.collate_function([batch_ds[i] for i in range(len(batch_ds))])\n",
    "        batch = to_device(batch)\n",
    "        bs = len(batch_ds)\n",
    "        x,y = batch\n",
    "            \n",
    "        batch_state.x = x\n",
    "        batch_state.y = y\n",
    "        batch_state.bs = bs\n",
    "        batch_state.rewards = to_device(torch.zeros(bs))\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.opt.zero_grad()\n",
    "    \n",
    "    def before_step(self):\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), self.clip)\n",
    "    \n",
    "    def step(self):\n",
    "        self.opt.step()\n",
    "    \n",
    "    def one_batch(self, batch):\n",
    "        batch = to_device(batch)\n",
    "        x,y = batch\n",
    "        if not isinstance(x, (list, tuple)):\n",
    "            x = [x]\n",
    "        output = self.model(*x)\n",
    "        loss = self.loss_function(output, y)\n",
    "        return loss\n",
    "    \n",
    "    def train_supervised(self, bs, epochs, lr, percent_valid=0.05):\n",
    "        \n",
    "        train_ds, valid_ds = self.dataset.split(percent_valid)\n",
    "        \n",
    "        train_dl = train_ds.dataloader(bs, shuffle=True)\n",
    "        valid_dl = valid_ds.dataloader(bs)\n",
    "        \n",
    "        opt = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(opt, max_lr=lr,\n",
    "                                        steps_per_epoch=len(train_dl), epochs=epochs)\n",
    "\n",
    "        mb = master_bar(range(epochs))\n",
    "        mb.write(['Epoch', 'Train Loss', 'Valid  Loss', 'Time'], table=True)\n",
    "        for epoch in mb:\n",
    "            start = time.time()\n",
    "            train_losses = []\n",
    "            \n",
    "            for batch in progress_bar(train_dl, parent=mb):\n",
    "                \n",
    "                loss = self.one_batch(batch)\n",
    "\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                scheduler.step()\n",
    "                train_losses.append(loss.detach().cpu())\n",
    "                mb.child.comment = f\"{train_losses[-1]:.5f}\"\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                self.model.eval()\n",
    "                valid_losses = []\n",
    "                for batch in progress_bar(valid_dl, parent=mb):\n",
    "\n",
    "                    loss = self.one_batch(batch)\n",
    "                    valid_losses.append(loss.detach().cpu())\n",
    "                    mb.child.comment = f\"{valid_losses[-1]:.5f}\"\n",
    "                self.model.train()\n",
    "                    \n",
    "            train_loss = smooth_batches(train_losses)\n",
    "            valid_loss = smooth_batches(valid_losses)\n",
    "            end = time.time() - start\n",
    "            mb.write([epoch, f'{train_losses[-1]:.5f}', \n",
    "                      f'{valid_losses[-1]:.5f}', f'{format_time(end)}'], table=True)\n",
    "    \n",
    "    def update_dataset(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    def update_dataset_from_inputs(self, *dataset_inputs):\n",
    "        dataset = self.dataset.new(*dataset_inputs)\n",
    "        self.update_dataset(dataset)\n",
    "    \n",
    "    def load_weights(self, filename):\n",
    "        state_dict = torch.load(filename, map_location=get_model_device(self.model))\n",
    "        \n",
    "        self.model.load_state_dict(state_dict)\n",
    "\n",
    "    def save_weights(self, filename):\n",
    "        \n",
    "        state_dict = self.model.state_dict()\n",
    "        torch.save(state_dict, filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "    \n",
    "class PredictiveAgent(Agent):\n",
    "    \n",
    "    def predict_tensor(self, x):\n",
    "        if not isinstance(x, (list, tuple)):\n",
    "            x = [x]\n",
    "        output = self.model(*x)\n",
    "        return output\n",
    "        \n",
    "    def predict_data(self, data):\n",
    "        ds = self.dataset.new(data, [0 for i in data])\n",
    "        batch = ds.collate_function([ds[i] for i in range(len(ds))])\n",
    "        batch = to_device(batch)\n",
    "        x,y = batch\n",
    "        return self.predict_tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class BaselineAgent(Agent):\n",
    "    def __init__(self, model, loss_function, dataset, base_update=0.99,\n",
    "                 base_update_iter=10, base_model=True, opt_kwargs={}, \n",
    "                 clip=1., name='baseline_agent'):\n",
    "        super().__init__(model, loss_function, dataset, opt_kwargs, clip, name)\n",
    "        \n",
    "        self.set_models(base_model)\n",
    "        self.base_update = base_update\n",
    "        self.base_update_iter = base_update_iter\n",
    "        \n",
    "    def after_batch(self):\n",
    "        log = self.environment.log\n",
    "        iterations = log.iterations\n",
    "        if iterations%self.base_update_iter == 0 and iterations>0:\n",
    "            self.update_base_model()\n",
    "        \n",
    "    def set_models(self, base_model):\n",
    "        \n",
    "        if base_model==True:\n",
    "            self.base_model = copy.deepcopy(self.model)\n",
    "        else:\n",
    "            self.base_model = base_model\n",
    "            \n",
    "        try:\n",
    "            to_device(self.base_model)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    def base_to_model(self):\n",
    "        if type(self.base_model)==type(self.model):\n",
    "            self.base_model.load_state_dict(self.model.state_dict())\n",
    "            \n",
    "    def model_to_base(self):\n",
    "        if type(self.base_model)==type(self.model):\n",
    "            self.model.load_state_dict(self.base_model.state_dict())\n",
    "            \n",
    "    def update_base_model(self):\n",
    "        if type(self.base_model)==type(self.model):\n",
    "            if self.base_update < 1:\n",
    "                merge_models(self.base_model, self.model, alpha=self.base_update)\n",
    "                \n",
    "    def save_weights(self, filename):\n",
    "        state_dict = {}\n",
    "        state_dict['model'] = self.model.state_dict()\n",
    "        \n",
    "        if isinstance(self.base_model, nn.Module):\n",
    "            state_dict['base_model'] = self.base_model.state_dict()\n",
    "        else:\n",
    "            state_dict['base_model'] = None\n",
    "            \n",
    "        torch.save(state_dict, filename)\n",
    "        \n",
    "    def load_weights(self, filename):\n",
    "        state_dict = torch.load(filename, map_location=get_model_device(self.model))\n",
    "        \n",
    "        self.model.load_state_dict(state_dict['model'])\n",
    "        \n",
    "        if isinstance(self.base_model, nn.Module):\n",
    "            self.base_model.load_state_dict(state_dict['base_model'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class CriticAgent(BaselineAgent):\n",
    "    \n",
    "    def predict_tensor(self, x, baseline=False):\n",
    "        if not type(x)==list:\n",
    "            x = [x]\n",
    "        \n",
    "        if baseline:\n",
    "            if isinstance(self.base_model, nn.Module):\n",
    "                output = self.base_model(*x)\n",
    "            else:\n",
    "                output = None\n",
    "        else:\n",
    "            output = self.model(*x)\n",
    "            \n",
    "        return output\n",
    "        \n",
    "    def predict_data(self, data):\n",
    "        ds = self.dataset.new(data, [0 for i in data])\n",
    "        batch = ds.collate_function([ds[i] for i in range(len(ds))])\n",
    "        batch = to_device(batch)\n",
    "        x,y = batch\n",
    "        return self.predict_tensor(x)\n",
    "    \n",
    "    def get_model_outputs(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        x = batch_state.x\n",
    "        y = batch_state.y\n",
    "        \n",
    "        preds = self.predict_tensor(x, baseline=False)\n",
    "        batch_state.model_output = preds\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            base_preds = self.predict_tensor(x, baseline=True)\n",
    "            batch_state.base_output = base_preds\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class GenerativeAgent(BaselineAgent):\n",
    "    def __init__(self, model, vocab, loss_function, dataset, \n",
    "                 base_update=0.99, base_update_iter=10, base_model=True, \n",
    "                 opt_kwargs={}, clip=1., name='generative_agent'):\n",
    "        super().__init__(model, loss_function, dataset, \n",
    "                         base_update=base_update, \n",
    "                         base_update_iter=base_update_iter, \n",
    "                         base_model=base_model, \n",
    "                         opt_kwargs=opt_kwargs,\n",
    "                         clip=clip,\n",
    "                         name=name)\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def reconstruct(self, preds):\n",
    "        return maybe_parallel(self.vocab.reconstruct, [i for i in preds.detach().cpu()])\n",
    "        \n",
    "    def before_compute_reward(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        sequences = batch_state.samples\n",
    "                \n",
    "        batch_ds = self.dataset.new(sequences)\n",
    "        batch = batch_ds.collate_function([batch_ds[i] for i in range(len(batch_ds))])\n",
    "        batch = to_device(batch)\n",
    "        bs = len(batch_ds)\n",
    "        x,y = batch\n",
    "            \n",
    "        batch_state.x = x\n",
    "        batch_state.y = y\n",
    "        batch_state.bs = bs\n",
    "        mask = ~(y==self.vocab.stoi['pad'])\n",
    "        batch_state.mask = mask\n",
    "        batch_state.lengths = mask.sum(-1)\n",
    "        batch_state.sl = y.shape[-1]\n",
    "        batch_state.rewards = to_device(torch.zeros(bs))\n",
    "        batch_state.trajectory_rewards = to_device(torch.zeros(y.shape))\n",
    "        \n",
    "    def get_rl_tensors(self, model, x, y, latent_info, sources):\n",
    "        if latent_info:\n",
    "            latent_sources = []\n",
    "            output_tensors = []\n",
    "            \n",
    "            \n",
    "            for (latent_source, latents) in latent_info.items():\n",
    "                latent_sources.append(latent_source)\n",
    "                latent_mask = torch.tensor([i==latent_source for i in sources]).bool()\n",
    "                out = self.model.get_rl_tensors(subset_tensor(x, latent_mask), \n",
    "                                                      subset_tensor(y, latent_mask),\n",
    "                                                      latent=latents)\n",
    "                out = list(out)\n",
    "                out.append(latents)\n",
    "                output_tensors.append(out)\n",
    "                \n",
    "            non_latent_mask = torch.tensor([not i in latent_sources for i in sources]).bool()\n",
    "            \n",
    "            if non_latent_mask.sum()>0:\n",
    "                latents = model.x_to_latent(subset_tensor(x, non_latent_mask))\n",
    "                out = model.get_rl_tensors(subset_tensor(x, non_latent_mask), \n",
    "                                           subset_tensor(y, non_latent_mask),\n",
    "                                           latent=latents)\n",
    "                out = list(out)\n",
    "                out.append(latents)\n",
    "                output_tensors.append(out)\n",
    "            \n",
    "            mo = torch.cat([i[0] for i in output_tensors], 0)\n",
    "            mlp = torch.cat([i[1] for i in output_tensors], 0)\n",
    "            mglp = torch.cat([i[2] for i in output_tensors], 0)\n",
    "            me = torch.cat([i[3] for i in output_tensors], 0)\n",
    "            \n",
    "            if not any([i[4] is None for i in output_tensors]):\n",
    "                latents = torch.cat([i[4] for i in output_tensors], 0)\n",
    "            else:\n",
    "                latents = None\n",
    "            \n",
    "        else:\n",
    "            latents = model.x_to_latent(x)\n",
    "            mo, mlp, mglp, me = model.get_rl_tensors(x,y, latent=latents)\n",
    "            \n",
    "        return mo, mlp, mglp, me, latents\n",
    "    \n",
    "    def get_model_outputs(self):\n",
    "            \n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        \n",
    "        x = batch_state.x\n",
    "        y = batch_state.y\n",
    "        sources = batch_state.sources\n",
    "        latent_info = batch_state.latent_data\n",
    "            \n",
    "        mo, mlp, mglp, me, ml = self.get_rl_tensors(self.model, x, y, latent_info, sources)\n",
    "        mprob = mlp.exp()\n",
    "        \n",
    "        batch_state.model_output = mo\n",
    "        batch_state.model_logprobs = mlp\n",
    "        batch_state.model_gathered_logprobs = mglp\n",
    "        batch_state.model_encoded = me\n",
    "        batch_state.model_latent = ml\n",
    "        batch_state.y_gumbel = F.one_hot(y, len(self.vocab.itos)) + mprob - mprob.detach()\n",
    "        batch_state.value_input = me\n",
    "        \n",
    "        if self.base_model is not None:\n",
    "            with torch.no_grad():\n",
    "                bo, blp, bglp, be, bl = self.get_rl_tensors(self.base_model, x, y, latent_info, sources)    \n",
    "        else:\n",
    "            bo, blp, bglp, be, bl = None, None, None, None, None\n",
    "            \n",
    "        batch_state.base_output = bo\n",
    "        batch_state.base_logprobs = blp\n",
    "        batch_state.base_gathered_logprobs = bglp\n",
    "        batch_state.base_encoded = be\n",
    "        batch_state.base_latent = bl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class SupevisedCB(Callback):\n",
    "    def __init__(self, agent, frequency, base_update, percentile, \n",
    "                 lr, bs, log_term='rewards'):\n",
    "        super().__init__('supervised', order=1000)\n",
    "        self.agent = agent\n",
    "        self.frequency = frequency\n",
    "        self.base_update = base_update\n",
    "        self.percentile = percentile\n",
    "        self.lr = lr\n",
    "        self.bs = bs\n",
    "        self.log_term = log_term\n",
    "        \n",
    "    def after_batch(self):\n",
    "        env = self.environment\n",
    "        iterations = self.environment.log.iterations\n",
    "        \n",
    "        if iterations>0 and iterations%self.frequency==0:\n",
    "            self.train_model()\n",
    "            \n",
    "            \n",
    "    def train_model(self):\n",
    "        env = self.environment\n",
    "        df = env.log.df[['samples', self.log_term]]\n",
    "        df = df[df[self.log_term]>np.percentile(df[self.log_term].values, self.percentile)]\n",
    "\n",
    "        self.agent.update_dataset_from_inputs(df.samples.values)\n",
    "        self.agent.train_supervised(self.bs, 1, self.lr)\n",
    "\n",
    "        merge_models(self.agent.base_model, self.agent.model, alpha=self.base_update)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Rollback(Callback):\n",
    "    def __init__(self, agent, metric_name, lookback, target, alpha, name, mode='greater'):\n",
    "        super().__init__(name=name)\n",
    "        self.agent = agent\n",
    "        assert self.agent.base_model is not None\n",
    "        self.metric_name = metric_name\n",
    "        self.lookback = lookback\n",
    "        self.target = target\n",
    "        self.alpha = alpha\n",
    "        self.mode = mode\n",
    "        self.last_rollback = 0\n",
    "        \n",
    "    def after_batch(self):\n",
    "        log = self.environment.log\n",
    "        current_value = np.array(log.metrics[self.metric_name][-self.lookback:]).mean()\n",
    "        \n",
    "        if self.mode == 'greater':\n",
    "            condition = current_val > self.target\n",
    "        else:\n",
    "            condition = current_val < self.target\n",
    "            \n",
    "        if condition and self.last_rollback <= 0:\n",
    "            merge_models(self.agent.model, self.agent.base_model, self.alpha)\n",
    "            self.last_rollback = self.lookback\n",
    "            \n",
    "        self.last_rollback -= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class RetrainRollback(Callback):\n",
    "    def __init__(self, agent, metric_name, lookback, target, \n",
    "                 percentile, lr, bs, base_update, name, mode='greater'):\n",
    "        super().__init__(name=name, order=1000)\n",
    "        \n",
    "        self.agent = agent\n",
    "        self.metric_name = metric_name\n",
    "        self.lookback = lookback\n",
    "        self.target = target\n",
    "        self.percentile = percentile\n",
    "        self.lr = lr\n",
    "        self.bs = bs\n",
    "        self.base_update = base_update\n",
    "        self.mode = mode\n",
    "        self.last_rollback = 0\n",
    "        \n",
    "    def after_batch(self):\n",
    "        log = self.environment.log\n",
    "        current_value = np.array(log.metrics[self.metric_name][-self.lookback:]).mean()\n",
    "        \n",
    "        if self.mode == 'greater':\n",
    "            condition = current_val > self.target\n",
    "        else:\n",
    "            condition = current_val < self.target\n",
    "            \n",
    "        if condition and self.last_rollback <= 0:\n",
    "            self.train_model()\n",
    "            self.last_rollback = self.lookback\n",
    "            \n",
    "        self.last_rollback -= 1\n",
    "    \n",
    "        \n",
    "    def train_model(self):\n",
    "        env = self.environment\n",
    "        df = env.log.df\n",
    "        df.drop_duplicates(subset='samples', inplace=True)\n",
    "        \n",
    "        metric_values = df[self.metric_name]\n",
    "        \n",
    "        df = df[metric_values>np.percentile(metric_values, self.percentile)]\n",
    "\n",
    "        self.agent.update_dataset_from_inputs(df.samples.values)\n",
    "        self.agent.train_supervised(self.bs, 1, self.lr)\n",
    "\n",
    "        merge_models(self.agent.base_model, self.agent.model, alpha=self.base_update)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class ResetAndRetrain(Callback):\n",
    "    def __init__(self, agent, frequency, weight_fp, percentile, \n",
    "                 lr, bs, epochs, log_term='rewards', sample_term='samples'):\n",
    "        super().__init__(name='reset_retrain', order=1000)\n",
    "        self.agent = agent\n",
    "        self.frequency = frequency\n",
    "        self.percentile = percentile\n",
    "        self.lr = lr\n",
    "        self.bs = bs\n",
    "        self.epochs = epochs\n",
    "        self.log_term = log_term\n",
    "        self.sample_term = sample_term\n",
    "        self.weight_fp = weight_fp\n",
    "        \n",
    "    def after_batch(self):\n",
    "        env = self.environment\n",
    "        iterations = self.environment.log.iterations\n",
    "        \n",
    "        if iterations>0 and iterations%self.frequency==0:\n",
    "            self.train_model()\n",
    "            \n",
    "            \n",
    "    def train_model(self):\n",
    "        env = self.environment\n",
    "        df = env.log.df[[self.sample_term, self.log_term]]\n",
    "        df = df[df[self.log_term]>np.percentile(df[self.log_term].values, self.percentile)]\n",
    "        \n",
    "        self.agent.model.load_state_dict(torch.load(self.weight_fp))\n",
    "        \n",
    "        self.agent.update_dataset_from_inputs(df[self.sample_term].values)\n",
    "        self.agent.train_supervised(self.bs, self.epochs, self.lr)\n",
    "\n",
    "        self.agent.base_model.load_state_dict(self.agent.model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class SaveAgentWeights(Callback):\n",
    "    def __init__(self, file_path, filename, n_batches, agent):\n",
    "        super().__init__(name='save_cb')\n",
    "        \n",
    "        self.file_path = file_path        \n",
    "        self.filename = filename\n",
    "        \n",
    "    def after_batch(self):\n",
    "        env = self.environment\n",
    "        iterations = log.iterations\n",
    "        \n",
    "        if iterations>0 and (n_batches%iterations)==0:\n",
    "            filename = self.file_path + self.filename + f'_{iterations}.pt'\n",
    "            agent.save_weights(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "# standard lm\n",
    "\n",
    "from mrl.vocab import *\n",
    "from mrl.dataloaders import *\n",
    "from mrl.g_models import *\n",
    "\n",
    "df = pd.read_csv('files/smiles.csv')\n",
    "vocab = CharacterVocab(SMILES_CHAR_VOCAB)\n",
    "\n",
    "ds = Text_Dataset(df.smiles.values, vocab)\n",
    "\n",
    "d_vocab = len(vocab.itos)\n",
    "d_embedding = 256\n",
    "d_hidden = 1024\n",
    "n_layers = 3\n",
    "input_dropout = 0.3\n",
    "lstm_dropout = 0.3\n",
    "bos_idx = vocab.stoi['bos']\n",
    "bidir = False\n",
    "tie_weights = True\n",
    "\n",
    "model = LSTM_LM(d_vocab, \n",
    "                d_embedding,\n",
    "                d_hidden, \n",
    "                n_layers,\n",
    "                input_dropout,\n",
    "                lstm_dropout,\n",
    "                bos_idx, \n",
    "                bidir, \n",
    "                tie_weights)\n",
    "\n",
    "model.load_state_dict(torch.load('untracked_files/lstm_lm_zinc.pt'))\n",
    "\n",
    "agent = GenerativeAgent(model, vocab, CrossEntropy(), ds, opt_kwargs={'lr':1e-4})\n",
    "\n",
    "agent.train_supervised(64, 1, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mrl)",
   "language": "python",
   "name": "mrl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
