{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "! [ -e /content ] && pip install -Uqq mrl-pypi  # upgrade mrl on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp train.agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n",
    "\n",
    "> Model agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from mrl.imports import *\n",
    "from mrl.core import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *\n",
    "from mrl.train.callback import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "The `Agent` class holds a model, a dataset and a loss function in a single object. The Agent is also a callback and serves several roles in the fit loop.\n",
    "\n",
    "#### Notable Functions\n",
    "\n",
    "- `Agent.train_supervised` - runs a supervised training loop using the items in `Agent.dataset`. Subclass this function for custom supervised training loops\n",
    "\n",
    "- `Agent.update_dataset`/`Agent.update_dataset_from_inputs` - updates `Agent.dataset` with new items\n",
    "\n",
    "- `Agent.before_compute_reward` - used during the fit loop to convert samples into tensors. Items from the current batch are used to create a version of `Agent.dataset` containing the new samples. This dataset is then used to convert samples into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Agent(Callback):\n",
    "    '''\n",
    "    Agent - class for bundling a model, loss function, and dataset\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `model nn.Module`: model\n",
    "    \n",
    "    - `loss_function Callable`: loss function for supervised training. Should \n",
    "    function as `loss = loss_function(model_output, y)`\n",
    "    \n",
    "    - `dataset Base_Dataset`: dataset\n",
    "    \n",
    "    - `opt_kwargs dict`: dictionary of keyword arguments passed to `optim.Adam`\n",
    "    \n",
    "    - `clip float`: gradient clipping\n",
    "    \n",
    "    - `name str`: agent name\n",
    "    '''\n",
    "    def __init__(self, model, loss_function, dataset, opt_kwargs={}, clip=1., name='agent'):\n",
    "        super().__init__(name=name, order=2)\n",
    "        \n",
    "        self.model = model\n",
    "        to_device(self.model)\n",
    "        \n",
    "        self.loss_function = loss_function\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        self.opt = self.get_opt(self.model.parameters(), **opt_kwargs)\n",
    "        self.clip = clip\n",
    "        self.training = True\n",
    "        self.compute_outputs = True\n",
    "        \n",
    "    def get_opt(self, parameters, **optim_kwargs):\n",
    "        return optim.Adam(parameters, **optim_kwargs)\n",
    "    \n",
    "    def before_compute_reward(self):\n",
    "        '''\n",
    "        uses self.dataset to convert samples into tensors\n",
    "        '''\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        sequences = batch_state.samples\n",
    "                \n",
    "        batch_ds = self.dataset.new(sequences)\n",
    "        batch = batch_ds.collate_function([batch_ds[i] for i in range(len(batch_ds))])\n",
    "        batch = to_device(batch)\n",
    "        bs = len(batch_ds)\n",
    "        x,y = batch\n",
    "            \n",
    "        batch_state.x = x\n",
    "        batch_state.y = y\n",
    "        batch_state.bs = bs\n",
    "        batch_state.rewards = to_device(torch.zeros(bs))\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.opt.zero_grad()\n",
    "    \n",
    "    def before_step(self):\n",
    "        if self.training:\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), self.clip)\n",
    "    \n",
    "    def step(self):\n",
    "        if self.training:\n",
    "            self.opt.step()\n",
    "    \n",
    "    def one_batch(self, batch, fp16=False):\n",
    "        batch = to_device(batch)\n",
    "        x,y = batch\n",
    "        if not isinstance(x, (list, tuple)):\n",
    "            x = [x]\n",
    "            \n",
    "        if fp16:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = self.model(*x)\n",
    "                loss = self.loss_function(output, y)\n",
    "        else:\n",
    "            output = self.model(*x)\n",
    "            loss = self.loss_function(output, y)\n",
    "        return loss\n",
    "    \n",
    "    def train_supervised(self, bs, epochs, lr, percent_valid=0.05, \n",
    "                         silent=False, fp16=False, opt_kwargs={}):\n",
    "        '''\n",
    "        train_supervised\n",
    "        \n",
    "        Inputs:\n",
    "        \n",
    "        - `bs int`: batch size\n",
    "        \n",
    "        - `epochs int`: number of epochs\n",
    "        \n",
    "        - `lr float`: learning rate passed to `optim.lr_scheduler.OneCycleLR`\n",
    "        \n",
    "        - `percent_valid float`: validation set percentage\n",
    "        \n",
    "        - `silent bool`: if training losses should be printed\n",
    "        \n",
    "        - `fp16 bool`: if FP16 training should be used\n",
    "        \n",
    "        - `opt_kwargs Optional[dict]`: keyword arguments passed to optimzier\n",
    "        '''\n",
    "        \n",
    "        if fp16:\n",
    "            scaler = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "        train_ds, valid_ds = self.dataset.split(percent_valid)\n",
    "        \n",
    "        if len(train_ds)%bs==1:\n",
    "            train_dl = train_ds.dataloader(bs, shuffle=True, drop_last=True)\n",
    "        else:\n",
    "            train_dl = train_ds.dataloader(bs, shuffle=True)\n",
    "            \n",
    "        valid_dl = valid_ds.dataloader(bs)\n",
    "        \n",
    "        opt = optim.Adam(self.model.parameters(), lr=lr, **opt_kwargs)\n",
    "        \n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(opt, max_lr=lr,\n",
    "                                        steps_per_epoch=len(train_dl), epochs=epochs)\n",
    "\n",
    "        if silent:\n",
    "            mb = range(epochs)\n",
    "        else:\n",
    "            mb = master_bar(range(epochs))\n",
    "            mb.write(['Epoch', 'Train Loss', 'Valid  Loss', 'Time'], table=True)\n",
    "            \n",
    "        for epoch in mb:\n",
    "            start = time.time()\n",
    "            train_losses = []\n",
    "            \n",
    "            if silent:\n",
    "                batch_iter = iter(train_dl)\n",
    "            else:\n",
    "                batch_iter = progress_bar(train_dl, parent=mb)\n",
    "            \n",
    "            for batch in batch_iter:\n",
    "                \n",
    "                loss = self.one_batch(batch, fp16=fp16)\n",
    "                opt.zero_grad()\n",
    "\n",
    "                if fp16:\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(opt)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                    opt.step()\n",
    "                    \n",
    "                scheduler.step()\n",
    "                train_losses.append(loss.detach().cpu())\n",
    "                \n",
    "                if not silent:\n",
    "                    mb.child.comment = f\"{train_losses[-1]:.5f}\"\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                self.model.eval()\n",
    "                valid_losses = []\n",
    "                \n",
    "                if silent:\n",
    "                    batch_iter = iter(valid_dl)\n",
    "                else:\n",
    "                    batch_iter = progress_bar(valid_dl, parent=mb)\n",
    "                \n",
    "                for batch in batch_iter:\n",
    "\n",
    "                    loss = self.one_batch(batch)\n",
    "                    valid_losses.append(loss.detach().cpu())\n",
    "                    \n",
    "                    if not silent:\n",
    "                        mb.child.comment = f\"{valid_losses[-1]:.5f}\"\n",
    "                self.model.train()\n",
    "                    \n",
    "            train_loss = smooth_batches(train_losses)\n",
    "            valid_loss = smooth_batches(valid_losses)\n",
    "            end = time.time() - start\n",
    "            if not silent:\n",
    "                mb.write([epoch, f'{train_losses[-1]:.5f}', \n",
    "                      f'{valid_losses[-1]:.5f}', f'{format_time(end)}'], table=True)\n",
    "    \n",
    "    def update_dataset(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    def update_dataset_from_inputs(self, *dataset_inputs):\n",
    "        dataset = self.dataset.new(*dataset_inputs)\n",
    "        self.update_dataset(dataset)\n",
    "        \n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.model.load_state_dict(state_dict)\n",
    "    \n",
    "    def load_weights(self, filename):\n",
    "        state_dict = torch.load(filename, map_location=get_model_device(self.model))\n",
    "        self.load_state_dict(state_dict)\n",
    "        \n",
    "#         self.model.load_state_dict(state_dict)\n",
    "\n",
    "    def save_weights(self, filename):\n",
    "        \n",
    "        state_dict = self.model.state_dict()\n",
    "        torch.save(state_dict, filename)\n",
    "        \n",
    "    def save(self, filename):\n",
    "        torch.save(self, filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "    \n",
    "class PredictiveAgent(Agent):\n",
    "    '''\n",
    "    PredictiveAgent - Agent class for predictive models\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `model nn.Module`: model\n",
    "    \n",
    "    - `loss_function Callable`: loss function for supervised training. Should \n",
    "    function as `loss = loss_function(model_output, y)`\n",
    "    \n",
    "    - `dataset Base_Dataset`: dataset\n",
    "    \n",
    "    - `opt_kwargs dict`: dictionary of keyword arguments passed to `optim.Adam`\n",
    "    \n",
    "    - `clip float`: gradient clipping\n",
    "    \n",
    "    - `name str`: agent name\n",
    "    '''\n",
    "    \n",
    "    def predict_tensor(self, x):\n",
    "        if not isinstance(x, (list, tuple)):\n",
    "            x = [x]\n",
    "        output = self.model(*x)\n",
    "        return output\n",
    "        \n",
    "    def predict_data(self, data):\n",
    "        ds = self.dataset.new(data, [0 for i in data])\n",
    "        batch = ds.collate_function([ds[i] for i in range(len(ds))])\n",
    "        batch = to_device(batch)\n",
    "        x,y = batch\n",
    "        return self.predict_tensor(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Agent\n",
    "\n",
    "Many RL algorithms make use of two agents. The main agent is trained every batch. The other agent (the baseline agent) is updated every `n` batches. RL algorithms like `PPO` and `TRPO` make use of the ratio between the main agent and the baseline agent. \n",
    "\n",
    "The `BaselineAgent` creates a copy of the model that serves as the baseline. The baseline agent is updated every `base_update_iter` batches.\n",
    "\n",
    "The baseline is updated following `w_baseline_new = alpha*w_baseline_old + (1-alpha)*w_main` where `alpha` is set by the `base_update` parameter. Setting `base_update=0` will cause the weights of the main agent to be simply copied into the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class BaselineAgent(Agent):\n",
    "    '''\n",
    "    BaselineAgent - agent for a model with a baseline model\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `model nn.Module`: model\n",
    "    \n",
    "    - `loss_function Callable`: loss function for supervised training. Should \n",
    "    function as `loss = loss_function(model_output, y)`\n",
    "    \n",
    "    - `dataset Base_Dataset`: dataset\n",
    "    \n",
    "    - `base_update float`: update fraction for the baseline model. Updates \n",
    "    the base model following `base_model = base_update*base_model + (1-base_update)*model`\n",
    "    \n",
    "    - `base_update_iter int`: update frequency for baseline model\n",
    "    \n",
    "    - `base_model bool`: if False, baseline model will not be created\n",
    "    \n",
    "    - `opt_kwargs dict`: dictionary of keyword arguments passed to `optim.Adam`\n",
    "    \n",
    "    - `clip float`: gradient clipping\n",
    "    \n",
    "    - `name str`: agent name\n",
    "    '''\n",
    "    def __init__(self, model, loss_function, dataset, base_update=0.99,\n",
    "                 base_update_iter=10, base_model=True, opt_kwargs={}, \n",
    "                 clip=1., name='baseline_agent'):\n",
    "        super().__init__(model, loss_function, dataset, opt_kwargs, clip, name)\n",
    "        \n",
    "        self.set_models(base_model)\n",
    "        self.base_update = base_update\n",
    "        self.base_update_iter = base_update_iter\n",
    "        \n",
    "    def after_batch(self):\n",
    "        log = self.environment.log\n",
    "        iterations = log.iterations\n",
    "        if iterations%self.base_update_iter == 0 and iterations>0:\n",
    "            self.update_base_model()\n",
    "        \n",
    "    def set_models(self, base_model):\n",
    "        \n",
    "        if base_model==True:\n",
    "            self.base_model = copy.deepcopy(self.model)\n",
    "        else:\n",
    "            self.base_model = base_model\n",
    "            \n",
    "        try:\n",
    "            to_device(self.base_model)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    def base_to_model(self):\n",
    "        '''\n",
    "        copies weights from `model` into `base_model`\n",
    "        '''\n",
    "        if type(self.base_model)==type(self.model):\n",
    "            self.base_model.load_state_dict(self.model.state_dict())\n",
    "            \n",
    "    def model_to_base(self):\n",
    "        '''\n",
    "        copies weights from `base_model` into `model`\n",
    "        '''\n",
    "        if type(self.base_model)==type(self.model):\n",
    "            self.model.load_state_dict(self.base_model.state_dict())\n",
    "            \n",
    "    def update_base_model(self):\n",
    "        '''\n",
    "        updates baseline model weights\n",
    "        '''\n",
    "        if type(self.base_model)==type(self.model):\n",
    "            if self.base_update < 1:\n",
    "                merge_models(self.base_model, self.model, alpha=self.base_update)\n",
    "                \n",
    "    def save_weights(self, filename):\n",
    "        state_dict = {}\n",
    "        state_dict['model'] = self.model.state_dict()\n",
    "        \n",
    "        if isinstance(self.base_model, nn.Module):\n",
    "            state_dict['base_model'] = self.base_model.state_dict()\n",
    "        else:\n",
    "            state_dict['base_model'] = None\n",
    "            \n",
    "        torch.save(state_dict, filename)\n",
    "        \n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.model.load_state_dict(state_dict['model'])\n",
    "        \n",
    "        if isinstance(self.base_model, nn.Module):\n",
    "            self.base_model.load_state_dict(state_dict['base_model'])\n",
    "        \n",
    "    def load_weights(self, filename):\n",
    "        state_dict = torch.load(filename, map_location=get_model_device(self.model))\n",
    "        self.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class CriticAgent(BaselineAgent):\n",
    "    '''\n",
    "    CriticAgent - baseline agent for critic models\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `model nn.Module`: model\n",
    "    \n",
    "    - `loss_function Callable`: loss function for supervised training. Should \n",
    "    function as `loss = loss_function(model_output, y)`\n",
    "    \n",
    "    - `dataset Base_Dataset`: dataset\n",
    "    \n",
    "    - `base_update float`: update fraction for the baseline model. Updates \n",
    "    the base model following `base_model = base_update*base_model + (1-base_update)*model`\n",
    "    \n",
    "    - `base_update_iter int`: update frequency for baseline model\n",
    "    \n",
    "    - `base_model bool`: if False, baseline model will not be created\n",
    "    \n",
    "    - `opt_kwargs dict`: dictionary of keyword arguments passed to `optim.Adam`\n",
    "    \n",
    "    - `clip float`: gradient clipping\n",
    "    \n",
    "    - `name str`: agent name\n",
    "    '''\n",
    "    def predict_tensor(self, x, baseline=False):\n",
    "        if not type(x)==list:\n",
    "            x = [x]\n",
    "        \n",
    "        if baseline:\n",
    "            if isinstance(self.base_model, nn.Module):\n",
    "                output = self.base_model(*x)\n",
    "            else:\n",
    "                output = None\n",
    "        else:\n",
    "            output = self.model(*x)\n",
    "            \n",
    "        return output\n",
    "        \n",
    "    def predict_data(self, data):\n",
    "        ds = self.dataset.new(data, [0 for i in data])\n",
    "        batch = ds.collate_function([ds[i] for i in range(len(ds))])\n",
    "        batch = to_device(batch)\n",
    "        x,y = batch\n",
    "        return self.predict_tensor(x)\n",
    "    \n",
    "    def get_model_outputs(self):\n",
    "        if self.compute_outputs:\n",
    "            env = self.environment\n",
    "            batch_state = env.batch_state\n",
    "            x = batch_state.x\n",
    "            y = batch_state.y\n",
    "\n",
    "            preds = self.predict_tensor(x, baseline=False)\n",
    "            batch_state.model_output = preds\n",
    "\n",
    "            with torch.no_grad():\n",
    "                base_preds = self.predict_tensor(x, baseline=True)\n",
    "                batch_state.base_output = base_preds\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Agent\n",
    "\n",
    "The `GenerativeAgent` class adds in a `vocab` input to reconstruct generated samples. This class also has updated `before_compute_reward` and `get_model_outputs` to create the relevant values for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class GenerativeAgent(BaselineAgent):\n",
    "    '''\n",
    "    GenerativeAgent - baseline agent for generative models\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `model nn.Module`: model\n",
    "    \n",
    "    - `vocab Vocab`: vocabulary\n",
    "    \n",
    "    - `loss_function Callable`: loss function for supervised training. Should \n",
    "    function as `loss = loss_function(model_output, y)`\n",
    "    \n",
    "    - `dataset Base_Dataset`: dataset\n",
    "    \n",
    "    - `base_update float`: update fraction for the baseline model. Updates \n",
    "    the base model following `base_model = base_update*base_model + (1-base_update)*model`\n",
    "    \n",
    "    - `base_update_iter int`: update frequency for baseline model\n",
    "    \n",
    "    - `base_model bool`: if False, baseline model will not be created\n",
    "    \n",
    "    - `opt_kwargs dict`: dictionary of keyword arguments passed to `optim.Adam`\n",
    "    \n",
    "    - `clip float`: gradient clipping\n",
    "    \n",
    "    - `name str`: agent name\n",
    "    '''\n",
    "    def __init__(self, model, vocab, loss_function, dataset, \n",
    "                 base_update=0.99, base_update_iter=10, base_model=True, \n",
    "                 opt_kwargs={}, clip=1., name='generative_agent'):\n",
    "        super().__init__(model, loss_function, dataset, \n",
    "                         base_update=base_update, \n",
    "                         base_update_iter=base_update_iter, \n",
    "                         base_model=base_model, \n",
    "                         opt_kwargs=opt_kwargs,\n",
    "                         clip=clip,\n",
    "                         name=name)\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def reconstruct(self, preds):\n",
    "        return maybe_parallel(self.vocab.reconstruct, [i for i in preds.detach().cpu()])\n",
    "    \n",
    "    def sample_and_reconstruct(self, bs, sl, **sample_kwargs):\n",
    "        preds, _ = self.model.sample_no_grad(bs, sl, **sample_kwargs)\n",
    "        recon = self.reconstruct(preds)\n",
    "        return recon\n",
    "        \n",
    "    def before_compute_reward(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        sequences = batch_state.samples\n",
    "                \n",
    "        batch_ds = self.dataset.new(sequences)\n",
    "        batch = batch_ds.collate_function([batch_ds[i] for i in range(len(batch_ds))])\n",
    "        batch = to_device(batch)\n",
    "        bs = len(batch_ds)\n",
    "        x,y = batch\n",
    "            \n",
    "        batch_state.x = x\n",
    "        batch_state.y = y\n",
    "        batch_state.bs = bs\n",
    "        mask = ~(y==self.vocab.stoi['pad']) # padding mask\n",
    "        batch_state.mask = mask\n",
    "        batch_state.lengths = mask.sum(-1)\n",
    "        batch_state.sl = y.shape[-1]\n",
    "        batch_state.rewards = to_device(torch.zeros(bs))\n",
    "        batch_state.trajectory_rewards = to_device(torch.zeros(y.shape))\n",
    "        \n",
    "    def get_rl_tensors(self, model, x, y, latent_info, sources):\n",
    "        '''\n",
    "        get_rl_tensors - uses latent info to compute output tensors\n",
    "        '''\n",
    "        if latent_info:\n",
    "            latent_sources = []\n",
    "            output_tensors = []\n",
    "            \n",
    "            \n",
    "            for (latent_source, latents) in latent_info.items():\n",
    "                if latents.shape[0]>0:\n",
    "                    latent_sources.append(latent_source)\n",
    "                    latent_mask = torch.tensor([i==latent_source for i in sources]).bool()\n",
    "                    out = self.model.get_rl_tensors(subset_tensor(x, latent_mask), \n",
    "                                                          subset_tensor(y, latent_mask),\n",
    "                                                          latent=latents)\n",
    "                    out = list(out)\n",
    "                    out.append(latents)\n",
    "                    output_tensors.append(out)\n",
    "                \n",
    "            non_latent_mask = torch.tensor([not i in latent_sources for i in sources]).bool()\n",
    "            \n",
    "            if non_latent_mask.sum()>0:\n",
    "                latents = model.x_to_latent(subset_tensor(x, non_latent_mask))\n",
    "                out = model.get_rl_tensors(subset_tensor(x, non_latent_mask), \n",
    "                                           subset_tensor(y, non_latent_mask),\n",
    "                                           latent=latents)\n",
    "                out = list(out)\n",
    "                out.append(latents)\n",
    "                output_tensors.append(out)\n",
    "            \n",
    "            mo = torch.cat([i[0] for i in output_tensors], 0)\n",
    "            mlp = torch.cat([i[1] for i in output_tensors], 0)\n",
    "            mglp = torch.cat([i[2] for i in output_tensors], 0)\n",
    "            me = torch.cat([i[3] for i in output_tensors], 0)\n",
    "            \n",
    "            if not any([i[4] is None for i in output_tensors]):\n",
    "                latents = torch.cat([i[4] for i in output_tensors], 0)\n",
    "            else:\n",
    "                latents = None\n",
    "            \n",
    "        else:\n",
    "            latents = model.x_to_latent(x)\n",
    "            mo, mlp, mglp, me = model.get_rl_tensors(x,y, latent=latents)\n",
    "            \n",
    "        return mo, mlp, mglp, me, latents\n",
    "    \n",
    "    def get_model_outputs(self):\n",
    "            \n",
    "        if self.compute_outputs:\n",
    "            env = self.environment\n",
    "            batch_state = env.batch_state\n",
    "\n",
    "            x = batch_state.x\n",
    "            y = batch_state.y\n",
    "            sources = batch_state.sources\n",
    "            latent_info = batch_state.latent_data\n",
    "\n",
    "            mo, mlp, mglp, me, ml = self.get_rl_tensors(self.model, x, y, latent_info, sources)\n",
    "            mprob = mlp.exp()\n",
    "\n",
    "            batch_state.model_output = mo\n",
    "            batch_state.model_logprobs = mlp\n",
    "            batch_state.model_gathered_logprobs = mglp\n",
    "            batch_state.model_encoded = me\n",
    "            batch_state.model_latent = ml\n",
    "            batch_state.y_gumbel = F.one_hot(y, len(self.vocab.itos)) + mprob - mprob.detach()\n",
    "            batch_state.value_input = me\n",
    "\n",
    "            if self.base_model is not None:\n",
    "                with torch.no_grad():\n",
    "                    bo, blp, bglp, be, bl = self.get_rl_tensors(self.base_model, x, y, latent_info, sources)    \n",
    "            else:\n",
    "                bo, blp, bglp, be, bl = None, None, None, None, None\n",
    "\n",
    "            batch_state.base_output = bo\n",
    "            batch_state.base_logprobs = blp\n",
    "            batch_state.base_gathered_logprobs = bglp\n",
    "            batch_state.base_encoded = be\n",
    "            batch_state.base_latent = bl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Callbacks\n",
    "\n",
    "These callbacks organize using supervised training during the RL fit cycle\n",
    "\n",
    "- `SupervisedCB` - runs supervised training on the top `x` percentile of samples with a set frequency\n",
    "\n",
    "- `Rollback` - if a chosen metric falls (above/below) a certain value, the weights of the main model are reverted to the baseline model\n",
    "\n",
    "- `RetrainRollback` - runs supervised training if a chosen metric falls (above/below) a certain value\n",
    "\n",
    "- `ResetAndRetrain` - with a set frequency, reloads a saved checkpoint and runs supervised training from the sample log\n",
    "\n",
    "- `SaveAgentWeights` - saves weights with a set frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class SupervisedCB(Callback):\n",
    "    '''\n",
    "    SupervisedCB - supervised training callback. When triggered, \n",
    "    this callback grabs the top `percentile` of samples from the \n",
    "    log and runs supervised training with the sampled data\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `agent Agent`: agent\n",
    "    \n",
    "    - `frequency int`: how often to run supervised training\n",
    "    \n",
    "    - `base_update float`: how much to update the baseline model \n",
    "    after supervised training (if applicable)\n",
    "    \n",
    "    - `percentile int`: percentile (int value 1-100) of data \n",
    "    to sample from the log\n",
    "    \n",
    "    - `lr float`: learning rate\n",
    "    \n",
    "    - `bs int`: batch size\n",
    "    \n",
    "    - `log_term str`: what term in the log to take the percentile of\n",
    "    \n",
    "    - `epochs int`: number of training epochs\n",
    "    \n",
    "    - `silent bool`: if training losses should be printed\n",
    "    '''\n",
    "    def __init__(self, agent, frequency, base_update, percentile, \n",
    "                 lr, bs, log_term='rewards', epochs=1, silent=True):\n",
    "        super().__init__('supervised', order=1000)\n",
    "        self.agent = agent\n",
    "        self.frequency = frequency\n",
    "        self.base_update = base_update\n",
    "        self.percentile = percentile\n",
    "        self.lr = lr\n",
    "        self.bs = bs\n",
    "        self.log_term = log_term\n",
    "        self.epochs = epochs\n",
    "        self.silent = silent\n",
    "        \n",
    "    def after_batch(self):\n",
    "        env = self.environment\n",
    "        iterations = self.environment.log.iterations\n",
    "        \n",
    "        if iterations>0 and iterations%self.frequency==0:\n",
    "            self.train_model()\n",
    "            \n",
    "            \n",
    "    def train_model(self):\n",
    "        env = self.environment\n",
    "        df = env.log.df[['samples', self.log_term]]\n",
    "        df = df[df[self.log_term]>np.percentile(df[self.log_term].values, self.percentile)]\n",
    "\n",
    "        self.agent.update_dataset_from_inputs(df.samples.values)\n",
    "        self.agent.train_supervised(self.bs, self.epochs, self.lr, silent=self.silent)\n",
    "\n",
    "        if hasattr(self.agent, 'base_model'):\n",
    "            if isinstance(self.agent.base_model, nn.Module):\n",
    "                merge_models(self.agent.base_model, self.agent.model, alpha=self.base_update)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Rollback(Callback):\n",
    "    '''\n",
    "    Rollback - if `metric_name` falls (above/below) `target`, updates \n",
    "    the main model's weights with the baseline model's weights\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `agent BaselineAgent`: agent\n",
    "    \n",
    "    - `metric_name str`: metric to track\n",
    "    \n",
    "    - `lookback int`: number of batches to look back. Also sets the \n",
    "    maximum rollback frequency\n",
    "    \n",
    "    - `target float`: desired cutoff for `metric_name`\n",
    "    \n",
    "    - `alpha float`: during rollback, the main model weights are \n",
    "    updated following `model = alpha*model + (1-alpha)*base_model`\n",
    "    \n",
    "    - `name str`: callback name\n",
    "    \n",
    "    - `mode str['greater', 'lesser']`: if greater, rollback is triggered by \n",
    "    the metric going over `target`. If lesser, rollback is triggered by the \n",
    "    metric falling below `target`\n",
    "    '''\n",
    "    def __init__(self, agent, metric_name, lookback, target, alpha, name, mode='greater'):\n",
    "        super().__init__(name=name)\n",
    "        self.agent = agent\n",
    "        assert self.agent.base_model is not None\n",
    "        self.metric_name = metric_name\n",
    "        self.lookback = lookback\n",
    "        self.target = target\n",
    "        self.alpha = alpha\n",
    "        self.mode = mode\n",
    "        self.last_rollback = 0\n",
    "        \n",
    "    def after_batch(self):\n",
    "        log = self.environment.log\n",
    "        current_value = np.array(log.metrics[self.metric_name][-self.lookback:]).mean()\n",
    "        \n",
    "        if self.mode == 'greater':\n",
    "            condition = current_val > self.target\n",
    "        else:\n",
    "            condition = current_val < self.target\n",
    "            \n",
    "        if condition and self.last_rollback <= 0:\n",
    "            merge_models(self.agent.model, self.agent.base_model, self.alpha)\n",
    "            self.last_rollback = self.lookback\n",
    "            \n",
    "        self.last_rollback -= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class RetrainRollback(Callback):\n",
    "    '''\n",
    "    RetrainRollback - triggers supervised training if \n",
    "    `metric_name` falls (above/below) `target`\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `agent BaselineAgent`: agent\n",
    "    \n",
    "    - `metric_name str`: metric to track\n",
    "    \n",
    "    - `log_term str`: what term in the log to take the percentile of\n",
    "    \n",
    "    - `lookback int`: number of batches to look back. Also sets the \n",
    "    maximum rollback frequency\n",
    "    \n",
    "    - `target float`: desired cutoff for `metric_name`\n",
    "    \n",
    "    - `percentile int`: percentile (1-100) of data to sample from the log\n",
    "    \n",
    "    - `lr float`: learning rate\n",
    "    \n",
    "    - `bs int`: batch size\n",
    "    \n",
    "    - `base_update float`: after supervised training, the weights \n",
    "    of the baseline model are updated following \n",
    "    `base_model = alpha*base_model + (1-alpha)*model`\n",
    "    \n",
    "    - `name str`: callback name\n",
    "    \n",
    "    - `mode str['greater', 'lesser']`: if greater, rollback is triggered by \n",
    "    the metric going over `target`. If lesser, rollback is triggered by the \n",
    "    metric falling below `target`\n",
    "    \n",
    "    - `silent bool`: if training losses should be printed\n",
    "    '''\n",
    "    def __init__(self, agent, metric_name, log_term, lookback, target, \n",
    "                 percentile, lr, bs, base_update, name, mode='greater',\n",
    "                 silent=False):\n",
    "        super().__init__(name=name, order=1000)\n",
    "        \n",
    "        self.agent = agent\n",
    "        self.metric_name = metric_name\n",
    "        self.log_term = log_term\n",
    "        self.lookback = lookback\n",
    "        self.target = target\n",
    "        self.percentile = percentile\n",
    "        self.lr = lr\n",
    "        self.bs = bs\n",
    "        self.base_update = base_update\n",
    "        self.mode = mode\n",
    "        self.last_rollback = 0\n",
    "        self.silent = silent\n",
    "        \n",
    "    def after_batch(self):\n",
    "        log = self.environment.log\n",
    "        current_value = np.array(log.metrics[self.metric_name][-self.lookback:]).mean()\n",
    "        \n",
    "        if self.mode == 'greater':\n",
    "            condition = current_value > self.target\n",
    "        else:\n",
    "            condition = current_value < self.target\n",
    "            \n",
    "        if condition and self.last_rollback <= 0:\n",
    "            self.train_model()\n",
    "            self.last_rollback = self.lookback\n",
    "            \n",
    "        self.last_rollback -= 1\n",
    "    \n",
    "        \n",
    "    def train_model(self):\n",
    "        env = self.environment\n",
    "        df = env.log.df\n",
    "        df.drop_duplicates(subset='samples', inplace=True)\n",
    "        \n",
    "        metric_values = df[self.log_term]\n",
    "        \n",
    "        df = df[metric_values>np.percentile(metric_values, self.percentile)]\n",
    "\n",
    "        self.agent.update_dataset_from_inputs(df.samples.values)\n",
    "        self.agent.train_supervised(self.bs, 1, self.lr, silent=self.silent)\n",
    "\n",
    "        merge_models(self.agent.base_model, self.agent.model, alpha=self.base_update)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class ResetAndRetrain(Callback):\n",
    "    '''\n",
    "    ResetAndRetrain - with a set frequency, loads a \n",
    "    file of saved weights and runs supervised training \n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `agent BaselineAgent`: agent\n",
    "    \n",
    "    - `frequency int`: how often to run supervised training\n",
    "    \n",
    "    - `weight_fp str`: filepath to weights\n",
    "    \n",
    "    - `percentile int`: percentile (int value 1-100) of data \n",
    "    to sample from the log\n",
    "    \n",
    "    - `lr float`: learning rate\n",
    "    \n",
    "    - `bs int`: batch size\n",
    "    \n",
    "    - `epochs int`: number of epochs to run\n",
    "    \n",
    "    - `log_term str`: what term in the log to take the percentile of\n",
    "    \n",
    "    - `sample_term str`: what log term contains the samples to train on\n",
    "    \n",
    "    - `silent bool`: if training losses should be printed\n",
    "    '''\n",
    "    def __init__(self, agent, frequency, weight_fp, percentile, \n",
    "                 lr, bs, epochs, log_term='rewards', sample_term='samples',\n",
    "                 silent=False):\n",
    "        super().__init__(name='reset_retrain', order=1000)\n",
    "        self.agent = agent\n",
    "        self.frequency = frequency\n",
    "        self.percentile = percentile\n",
    "        self.lr = lr\n",
    "        self.bs = bs\n",
    "        self.epochs = epochs\n",
    "        self.log_term = log_term\n",
    "        self.sample_term = sample_term\n",
    "        self.weight_fp = weight_fp\n",
    "        self.silent = silent\n",
    "        \n",
    "    def after_batch(self):\n",
    "        env = self.environment\n",
    "        iterations = self.environment.log.iterations\n",
    "        \n",
    "        if iterations>0 and iterations%self.frequency==0:\n",
    "            self.train_model()\n",
    "            \n",
    "            \n",
    "    def train_model(self):\n",
    "        env = self.environment\n",
    "        df = env.log.df[[self.sample_term, self.log_term]]\n",
    "        df = df[df[self.log_term]>np.percentile(df[self.log_term].values, self.percentile)]\n",
    "        \n",
    "        self.agent.model.load_state_dict(torch.load(self.weight_fp))\n",
    "        \n",
    "        self.agent.update_dataset_from_inputs(df[self.sample_term].values)\n",
    "        self.agent.train_supervised(self.bs, self.epochs, self.lr, silent=self.silent)\n",
    "\n",
    "        self.agent.base_model.load_state_dict(self.agent.model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class MetricResetAndRetrain(Callback):\n",
    "    '''\n",
    "    MetricResetAndRetrain - loads a file of saved \n",
    "    weights and runs supervised training if \n",
    "    `metric_name` falls (above/below) `target`\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `agent BaselineAgent`: agent\n",
    "    \n",
    "    - `metric_name str`: metric to track\n",
    "    \n",
    "    - `lookback int`: number of batches to look back. Also sets the \n",
    "    maximum rollback frequency\n",
    "    \n",
    "    - `target float`: desired cutoff for `metric_name`\n",
    "    \n",
    "    - `weight_fp str`: filepath to weights\n",
    "    \n",
    "    - `percentile int`: percentile (int value 1-100) of data \n",
    "    to sample from the log\n",
    "    \n",
    "    - `lr float`: learning rate\n",
    "    \n",
    "    - `bs int`: batch size\n",
    "    \n",
    "    - `epochs int`: number of epochs to run\n",
    "    \n",
    "    - `log_term str`: what term in the log to take the percentile of\n",
    "    \n",
    "    - `sample_term str`: what log term contains the samples to train on\n",
    "    \n",
    "    - `mode str['greater', 'lesser']`: if greater, rollback is triggered by \n",
    "    the metric going over `target`. If lesser, rollback is triggered by the \n",
    "    metric falling below `target`\n",
    "    \n",
    "    - `silent bool`: if training losses should be printed\n",
    "    '''\n",
    "    def __init__(self, agent, metric_name, lookback, target, \n",
    "                 weight_fp, percentile, lr, bs, epochs, \n",
    "                 log_term='rewards', sample_term='samples',\n",
    "                 mode='greater', silent=False):\n",
    "        super().__init__(name='metric_retrain', order=1000)\n",
    "        \n",
    "        self.agent = agent\n",
    "        self.metric_name = metric_name\n",
    "        self.lookback = lookback\n",
    "        self.target = target\n",
    "        self.percentile = percentile\n",
    "        self.lr = lr\n",
    "        self.bs = bs\n",
    "        self.epochs = epochs\n",
    "        self.log_term = log_term\n",
    "        self.sample_term = sample_term\n",
    "        self.weight_fp = weight_fp\n",
    "        self.silent = silent\n",
    "        self.mode = mode\n",
    "        self.last_rollback = 0\n",
    "        \n",
    "    def after_batch(self):\n",
    "        log = self.environment.log\n",
    "        current_value = np.array(log.metrics[self.metric_name][-self.lookback:]).mean()\n",
    "        \n",
    "        if self.mode == 'greater':\n",
    "            condition = current_value > self.target\n",
    "        else:\n",
    "            condition = current_value < self.target\n",
    "                        \n",
    "        if condition and self.last_rollback <= 0:\n",
    "            self.train_model()\n",
    "            self.last_rollback = self.lookback\n",
    "            \n",
    "        self.last_rollback -= 1\n",
    "            \n",
    "            \n",
    "    def train_model(self):\n",
    "        env = self.environment\n",
    "        df = env.log.df[[self.sample_term, self.log_term]]\n",
    "        df = df[df[self.log_term]>np.percentile(df[self.log_term].values, self.percentile)]\n",
    "        \n",
    "        self.agent.model.load_state_dict(torch.load(self.weight_fp))\n",
    "        \n",
    "        self.agent.update_dataset_from_inputs(df[self.sample_term].values)\n",
    "        self.agent.train_supervised(self.bs, self.epochs, self.lr, silent=self.silent)\n",
    "\n",
    "        self.agent.base_model.load_state_dict(self.agent.model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class SaveAgentWeights(Callback):\n",
    "    '''\n",
    "    SaveAgentWeights - saves weights every `n_batches`. \n",
    "    Weights are saved to `file_path/filename_iterations.pt`\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `file_path str`: directory to save weights in\n",
    "    \n",
    "    - `filename str`: base filename\n",
    "    \n",
    "    - `n_batches int`: how often to save weights\n",
    "    \n",
    "    - `agent Agent`: agent\n",
    "    '''\n",
    "    def __init__(self, file_path, filename, n_batches, agent):\n",
    "        super().__init__(name='save_cb')\n",
    "        \n",
    "        self.file_path = file_path        \n",
    "        self.filename = filename\n",
    "        \n",
    "    def after_batch(self):\n",
    "        env = self.environment\n",
    "        iterations = log.iterations\n",
    "        \n",
    "        if iterations>0 and (n_batches%iterations)==0:\n",
    "            filename = self.file_path + self.filename + f'_{iterations}.pt'\n",
    "            agent.save_weights(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Valid  Loss</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.42942</td>\n",
       "      <td>0.51512</td>\n",
       "      <td>00:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "from mrl.vocab import *\n",
    "from mrl.dataloaders import *\n",
    "from mrl.g_models.all import *\n",
    "\n",
    "df = pd.read_csv('files/smiles.csv')\n",
    "vocab = CharacterVocab(SMILES_CHAR_VOCAB)\n",
    "\n",
    "ds = Text_Dataset(list(df.smiles.values)*10, vocab)\n",
    "\n",
    "d_vocab = len(vocab.itos)\n",
    "d_embedding = 256\n",
    "d_hidden = 1024\n",
    "n_layers = 3\n",
    "input_dropout = 0.3\n",
    "lstm_dropout = 0.3\n",
    "bos_idx = vocab.stoi['bos']\n",
    "bidir = False\n",
    "tie_weights = True\n",
    "\n",
    "model = LSTM_LM(d_vocab, \n",
    "                d_embedding,\n",
    "                d_hidden, \n",
    "                n_layers,\n",
    "                input_dropout,\n",
    "                lstm_dropout,\n",
    "                bos_idx, \n",
    "                bidir, \n",
    "                tie_weights)\n",
    "\n",
    "model.load_state_dict(torch.load('untracked_files/lstm_lm_zinc.pt'))\n",
    "\n",
    "agent = GenerativeAgent(model, vocab, CrossEntropy(), ds, opt_kwargs={'lr':1e-4})\n",
    "\n",
    "agent.train_supervised(64, 1, 1e-4, silent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Valid  Loss</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.42942</td>\n",
       "      <td>0.51508</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "from mrl.vocab import *\n",
    "from mrl.dataloaders import *\n",
    "from mrl.g_models.all import *\n",
    "\n",
    "df = pd.read_csv('files/smiles.csv')\n",
    "vocab = CharacterVocab(SMILES_CHAR_VOCAB)\n",
    "\n",
    "ds = Text_Dataset(list(df.smiles.values)*10, vocab)\n",
    "\n",
    "d_vocab = len(vocab.itos)\n",
    "d_embedding = 256\n",
    "d_hidden = 1024\n",
    "n_layers = 3\n",
    "input_dropout = 0.3\n",
    "lstm_dropout = 0.3\n",
    "bos_idx = vocab.stoi['bos']\n",
    "bidir = False\n",
    "tie_weights = True\n",
    "\n",
    "model = LSTM_LM(d_vocab, \n",
    "                d_embedding,\n",
    "                d_hidden, \n",
    "                n_layers,\n",
    "                input_dropout,\n",
    "                lstm_dropout,\n",
    "                bos_idx, \n",
    "                bidir, \n",
    "                tie_weights)\n",
    "\n",
    "model.load_state_dict(torch.load('untracked_files/lstm_lm_zinc.pt'))\n",
    "\n",
    "agent = GenerativeAgent(model, vocab, CrossEntropy(), ds, opt_kwargs={'lr':1e-4})\n",
    "\n",
    "agent.train_supervised(64, 1, 1e-4, silent=False, fp16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
