{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# all_tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial - RL Train Cycle Overview\n",
    "\n",
    ">Overview of the RL training cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Train Cycle Overview\n",
    "\n",
    "The goal of this tutorial is to walk through the RL fit cycle to familiarize ourselves with the `Events` cycle and get a better understanding of how `Callback` and `Environment` classes work.\n",
    "\n",
    "## High Level Overview\n",
    "\n",
    "### The Environment\n",
    "\n",
    "At the highest level, we have the `Environment` class. The `Environment` holds together several sub-modules and orchestrates them during the fit loop. The following are contained in the `Environment`:\n",
    "- `agent` - This is the actual model we're training\n",
    "- `template_cb` - this holds a `Template` class that we use to define our chemical space\n",
    "- `samplers` - samplers generate new samples to train on\n",
    "- `buffer` - the buffer collects and distributes samples from all the `samplers`\n",
    "- `rewards` - rewards score samples\n",
    "- `losses` - losses generate values we can backpropagate through \n",
    "- `log` - the log holds a record of all samples in the training process\n",
    "\n",
    "### Callbacks and the Event Cycle\n",
    "\n",
    "Each one of the above items is a `Callback`. A `Callback` is a a general class that can hook into the `Environment` fit cycle at a number of pre-defined `Events`. When the `Environment` calls a specific `Event`, the event name is passed to every callback in the `Environment`. If a given `Callback` has a defined function named after the event, that function is called. This creates a very flexible system for customizing training loops.\n",
    "\n",
    "We'll be looking more at `Events` later. For now, we'll just list them in brief. These are the events called during the RL training cycle in the order they are executed:\n",
    "\n",
    "- `setup` - called when the `Environment` is created, used to set up values\n",
    "- `before_train` - called before training is started\n",
    "- `build_buffer` - draws samples from `samplers` into the `buffer`\n",
    "- `filter_buffer` - filters samples in the buffer\n",
    "- `after_build_buffer` - called after buffer filtering. Used for cleanup, logging, etc\n",
    "- `before_batch` - called before a batch starts, used to set up the `batch state`\n",
    "- `sample_batch` - samples are drawn from `sampers` and `buffer` into the `batch state`\n",
    "- `before_filter_batch` - allows preprocessing of samples before filtering\n",
    "- `filter_batch` - filters samples in `batch state`\n",
    "- `after_sample` - used for calculating sampling metrics\n",
    "- `before_compute_reward` - used to set up any values needed for reward computation \n",
    "- `compute_reward` - used by `rewards` to compute rewards for all samples in the `batch state`\n",
    "- `after_compute_reward` - used for logging reward metrics\n",
    "- `reward_modification` - modify rewards in ways not tracked by the log\n",
    "- `get_model_outputs` - generate necessary tensors from the model\n",
    "- `after_get_model_outputs` - used for any processing required prior to loss calculation \n",
    "- `compute_loss` - compute loss values\n",
    "- `zero_grad` - zero grad\n",
    "- `before_step` - used for computation before optimizer step (ie gradient clipping)\n",
    "- `step` - step optimizer\n",
    "- `after_batch` - compute batch stats\n",
    "- `after_train` - final event after all training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmai/miniconda3/envs/mrl/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for boost::shared_ptr<RDKit::FilterCatalogEntry const> already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from mrl.imports import *\n",
    "from mrl.core import *\n",
    "from mrl.chem import *\n",
    "from mrl.templates.all import *\n",
    "\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *\n",
    "from mrl.layers import *\n",
    "from mrl.dataloaders import *\n",
    "from mrl.g_models.all import *\n",
    "from mrl.vocab import *\n",
    "from mrl.policy_gradient import *\n",
    "from mrl.train.all import *\n",
    "from mrl.model_zoo import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_global_pool(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = Template([ValidityFilter(), \n",
    "                     SingleCompoundFilter(), \n",
    "                     RotBondFilter(None, 8),\n",
    "                     ChargeFilter(None, 0)],\n",
    "                    [QEDFilter(0.5, None, score=1.),\n",
    "                     SAFilter(None, 5, score=1.)], \n",
    "                    fail_score=-10., log=False)\n",
    "\n",
    "template_cb = TemplateCallback(template, prefilter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmai/miniconda3/envs/mrl/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator LinearRegression from version 0.23.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "class FP_Regression_Score():\n",
    "    def __init__(self, fname):\n",
    "        self.model = torch.load(fname)\n",
    "        self.fp_function = partial(failsafe_fp, fp_function=ECFP6)\n",
    "        \n",
    "    def __call__(self, samples):\n",
    "        mols = to_mols(samples)\n",
    "        fps = maybe_parallel(self.fp_function, mols)\n",
    "        fps = [fp_to_array(i) for i in fps]\n",
    "        x_vals = np.stack(fps)\n",
    "        preds = self.model.predict(x_vals)\n",
    "        return preds\n",
    "    \n",
    "reward_function = FP_Regression_Score('files/erbB1_regression.sklearn')\n",
    "\n",
    "reward = Reward(reward_function, weight=10.)\n",
    "\n",
    "aff_reward = RewardCallback(reward, 'aff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg = PPO(0.99,\n",
    "        0.5,\n",
    "        lam=0.95,\n",
    "        v_coef=0.5,\n",
    "        cliprange=0.3,\n",
    "        v_cliprange=0.3,\n",
    "        ent_coef=0.01,\n",
    "        kl_target=0.03,\n",
    "        kl_horizon=3000,\n",
    "        scale_rewards=True)\n",
    "\n",
    "loss = PolicyLoss(pg, 'PPO', \n",
    "                   value_head=ValueHead(256), \n",
    "                   v_update_iter=2, \n",
    "                   vopt_kwargs={'lr':1e-3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = LSTM_LM_Small_ZINC(drop_scale=0.5,opt_kwargs={'lr':5e-5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_bs = 1500\n",
    "df = pd.read_csv('untracked_files/affinity_data_set.csv')\n",
    "df = df[df.value<-1]\n",
    "\n",
    "sampler1 = ModelSampler(agent.vocab, agent.model, 'live', 1000, 0.5, gen_bs)\n",
    "sampler2 = ModelSampler(agent.vocab, agent.base_model, 'base', 1000, 0., gen_bs)\n",
    "sampler3 = LogSampler('samples', 'rewards', 10, 95, 100)\n",
    "sampler4 = DatasetSampler(df.smiles.values, 'erbB1_data', buffer_size=4)\n",
    "\n",
    "samplers = [sampler1, sampler2, sampler3, sampler4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_cb = SupevisedCB(agent, 200, 0.5, 97, 5e-5, 64)\n",
    "live_max = MaxCallback('rewards', 'live')\n",
    "live_p90 = PercentileCallback('rewards', 'live', 90)\n",
    "new_cb = NoveltyReward(weight=0.05)\n",
    "\n",
    "cbs = [new_cb, supervised_cb, live_p90, live_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(agent, template_cb, samplers=samplers, rewards=[aff_reward], losses=[loss],\n",
    "                 cbs=cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>iterations</th>\n",
       "      <th>rewards</th>\n",
       "      <th>rewards_final</th>\n",
       "      <th>new</th>\n",
       "      <th>diversity</th>\n",
       "      <th>bs</th>\n",
       "      <th>template</th>\n",
       "      <th>valid</th>\n",
       "      <th>live_diversity</th>\n",
       "      <th>live_valid</th>\n",
       "      <th>live_rewards</th>\n",
       "      <th>live_new</th>\n",
       "      <th>aff</th>\n",
       "      <th>novel</th>\n",
       "      <th>PPO</th>\n",
       "      <th>rewards_live_p90</th>\n",
       "      <th>rewards_live_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-6.118</td>\n",
       "      <td>-6.118</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>190</td>\n",
       "      <td>1.968</td>\n",
       "      <td>0.950</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.900</td>\n",
       "      <td>-6.763</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-8.087</td>\n",
       "      <td>0.050</td>\n",
       "      <td>3.216</td>\n",
       "      <td>-1.674</td>\n",
       "      <td>3.496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-6.280</td>\n",
       "      <td>-6.280</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>188</td>\n",
       "      <td>1.968</td>\n",
       "      <td>0.940</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.880</td>\n",
       "      <td>-5.995</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-8.249</td>\n",
       "      <td>0.050</td>\n",
       "      <td>2.319</td>\n",
       "      <td>-2.585</td>\n",
       "      <td>6.606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-6.139</td>\n",
       "      <td>-6.139</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>188</td>\n",
       "      <td>1.973</td>\n",
       "      <td>0.940</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.880</td>\n",
       "      <td>-6.146</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-8.113</td>\n",
       "      <td>0.050</td>\n",
       "      <td>2.588</td>\n",
       "      <td>-2.053</td>\n",
       "      <td>1.301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>-6.070</td>\n",
       "      <td>-6.070</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>192</td>\n",
       "      <td>1.984</td>\n",
       "      <td>0.960</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.920</td>\n",
       "      <td>-5.836</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-8.055</td>\n",
       "      <td>0.050</td>\n",
       "      <td>2.991</td>\n",
       "      <td>-0.729</td>\n",
       "      <td>7.122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>-6.530</td>\n",
       "      <td>-6.530</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>189</td>\n",
       "      <td>1.958</td>\n",
       "      <td>0.945</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.890</td>\n",
       "      <td>-5.979</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-8.488</td>\n",
       "      <td>0.050</td>\n",
       "      <td>2.663</td>\n",
       "      <td>-2.031</td>\n",
       "      <td>4.096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.fit(200, 90, 10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrl",
   "language": "python",
   "name": "mrl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
