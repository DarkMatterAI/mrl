{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c92780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp policy_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a774717",
   "metadata": {},
   "source": [
    "# Policy Gradient\n",
    "\n",
    "> Policy gradient modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0482fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d9b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from mrl.imports import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *\n",
    "from mrl.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6961f25e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022784da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class BasePolicy():\n",
    "    def __init__(self, gamma=1.):\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def discount_rewards(self, rewards, mask, traj_rewards):\n",
    "        rewards = scatter_rewards(rewards, mask)\n",
    "\n",
    "        if traj_rewards is not None:\n",
    "            rewards += traj_rewards\n",
    "\n",
    "        discounted = discount_rewards(rewards, self.gamma)\n",
    "\n",
    "        return discounted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d78617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class PolicyGradient(BasePolicy):\n",
    "    def __init__(self, discount=True, gamma=0.97, ratio=False):\n",
    "        super().__init__(gamma)\n",
    "        self.discount = discount\n",
    "        self.ratio = ratio\n",
    "        \n",
    "    def __call__(self, lps, mask, rewards, ref_lps=None, traj_rewards=None):\n",
    "        if self.ratio:\n",
    "            lps = (lps - ref_lps.detach()).exp()\n",
    "            \n",
    "        if not self.discount:\n",
    "            pg_loss = -((lps*mask).sum(-1)*rewards)/mask.sum(-1)\n",
    "            \n",
    "        else:\n",
    "            rewards = self.discount_rewards(rewards, mask, traj_rewards)\n",
    "            rewards = whiten(rewards)\n",
    "            pg_loss = -(lps*rewards*mask).sum(-1)/mask.sum(-1)\n",
    "            \n",
    "        pg_dict = {'loss':loss.detach().cpu(), 'rewards':rewards.detach().cpu()}\n",
    "            \n",
    "        return pg_loss.mean(), pg_dict\n",
    "    \n",
    "    def from_batch_state(self, batch_state):\n",
    "        lps = batch_state.model_gathered_logprobs\n",
    "        ref_lps = batch_state.reference_gathered_logprobs\n",
    "        mask = batch_state.mask\n",
    "        rewards = batch_state.rewards_scaled\n",
    "        traj_rewards = batch_state.trajectory_rewards\n",
    "        \n",
    "        loss, pg_dict = self(lps, mask, rewards, ref_lps, traj_rewards)\n",
    "        batch_state.losses.append(loss)\n",
    "        batch_state.pg_losses = loss\n",
    "        batch_state.pg_dict = pg_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a23520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class TRPO(BasePolicy):\n",
    "    def __init__(self, gamma, kl_target, beta=1., eta=50, lam=0.95, v_coef=0.5):\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.eta = eta\n",
    "        self.lam = lam\n",
    "        self.kl_target = kl_target\n",
    "        self.v_coef = v_coef\n",
    "        \n",
    "    def __call__(self, lps_g, ref_lps_g, lps, ref_lps, mask, \n",
    "                 rewards, values, traj_rewards=None):\n",
    "    \n",
    "        discounted_rewards = self.discount_rewards(rewards, mask, traj_rewards)\n",
    "        advantages = self.compute_advantages(discounted_rewards, values)\n",
    "        advantages = whiten(advantages)\n",
    "        \n",
    "        v_loss = self.value_loss(values, discounted_rewards)\n",
    "        \n",
    "        ratios = (lps_g - ref_lps_g.detach()).exp()\n",
    "        \n",
    "        loss1 = -(ratios*advantages*mask).sum(-1)/mask.sum(-1)\n",
    "        \n",
    "        kl = torch.distributions.kl.kl_divergence(\n",
    "                    Categorical(logits=ref_lps),\n",
    "                    Categorical(logits=lps))\n",
    "        \n",
    "        kl = (kl*mask).sum(-1)/mask.sum(-1)\n",
    "        kl = kl.mean()\n",
    "        loss2 = self.beta*kl\n",
    "        \n",
    "        loss3 = self.eta * torch.maximum(to_device(torch.tensor(0.)), \n",
    "                                         kl - 2.0*self.kl_target)\n",
    "        \n",
    "        loss1 = loss1.mean()\n",
    "        loss3 = loss3.mean()\n",
    "\n",
    "        pg_loss = loss1 + loss2 + loss3 + v_loss\n",
    "        \n",
    "        pg_dict = {'pg_discounted' : discounted_rewards,\n",
    "                    'pg_advantage' : advantages,\n",
    "                    'ratios' : ratios.detach().cpu(),\n",
    "                    'kl' : kl.detach().cpu(),\n",
    "                    'loss1' : loss1.detach().cpu(),\n",
    "                    'loss2' : loss2.detach().cpu(),\n",
    "                    'loss3' : loss3.detach().cpu(),\n",
    "                    'v_loss' : v_loss.detach().cpu()}\n",
    "        \n",
    "        return pg_loss, pg_dict\n",
    "    \n",
    "    def from_batch_state(self, batch_state):\n",
    "        lps_g = batch_state.model_gathered_logprobs\n",
    "        ref_lps_g = batch_state.reference_gathered_logprobs\n",
    "        \n",
    "        lps = batch_state.model_logprobs\n",
    "        ref_lps = batch_state.reference_logprobs\n",
    "        \n",
    "        mask = batch_state.mask\n",
    "        rewards = batch_state.rewards_scaled\n",
    "        traj_rewards = batch_state.trajectory_rewards\n",
    "        \n",
    "        values = batch_state.state_values\n",
    "        \n",
    "        loss, pg_dict = self(lps_g, ref_lps_g, lps, ref_lps, mask, \n",
    "                             rewards, values, traj_rewards)\n",
    "        batch_state.losses.append(loss)\n",
    "        batch_state.pg_losses = loss\n",
    "        batch_state.pg_dict = pg_dict\n",
    "\n",
    "    def compute_advantages(self, rewards, values):\n",
    "\n",
    "        if values is None:\n",
    "            advantages = rewards\n",
    "        else:\n",
    "            advantages = compute_advantages(rewards, values.detach(), self.gamma, self.lam)\n",
    "\n",
    "        return advantages\n",
    "\n",
    "    def value_loss(self, values, rewards):\n",
    "        if values is None:\n",
    "            v_loss = to_device(torch.tensor(0.))\n",
    "        else:\n",
    "            v_loss = self.v_coef*F.mse_loss(values, rewards)\n",
    "\n",
    "        return v_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254d6c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export   \n",
    "\n",
    "class PPO(BasePolicy):\n",
    "    def __init__(self, gamma, kl_coef, lam=0.95, v_coef=0.5, cliprange=0.2, \n",
    "                 v_cliprange=0.2, ent_coef=0.01, kl_target=None, kl_horizon=None):\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.ent_coef = ent_coef\n",
    "        self.kl_coef = kl_coef\n",
    "        self.kl_target = kl_target\n",
    "        self.kl_horizon = kl_horizon\n",
    "        self.v_coef = v_coef\n",
    "        self.cliprange = cliprange\n",
    "        self.v_cliprange = v_cliprange\n",
    "        \n",
    "    def __call__(self, lps, ref_lps, mask, \n",
    "                 rewards, values, ref_values, traj_rewards=None):\n",
    "        \n",
    "        discounted_rewards = self.discount_rewards(rewards, mask, traj_rewards)\n",
    "        kl_reward = self.compute_kl_reward(lps, ref_lps)\n",
    "        \n",
    "        discounted_rewards = discounted_rewards + kl_reward\n",
    "        advantages = self.compute_advantages(discounted_rewards, values)\n",
    "        advantages = whiten(advantages)\n",
    "        \n",
    "        v_loss = self.value_loss(values, ref_values, discounted_rewards)\n",
    "        \n",
    "        ratios = (lps - ref_lps).exp()\n",
    "        ratios_clipped = torch.clamp(ratios, 1.0-self.cliprange, 1.0+self.cliprange)\n",
    "        \n",
    "        loss1 = -(ratios*advantages)\n",
    "        loss2 = -(ratios_clipped*advantages)\n",
    "        \n",
    "        loss = torch.maximum(loss1, loss2)\n",
    "        loss = (loss*mask).sum(-1)/mask.sum(-1)\n",
    "        \n",
    "        entropy = Categorical(lps).entropy().mean()\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        \n",
    "        pg_loss = loss + v_loss - self.ent_coef*entropy\n",
    "        \n",
    "        self.update_kl(lps, ref_lps, mask)\n",
    "         \n",
    "        pg_dict = {'pg_discounted' : discounted_rewards,\n",
    "                    'pg_advantage' : advantages,\n",
    "                    'ratios' : ratios.detach().cpu(),\n",
    "                    'loss' : loss.detach().cpu(),\n",
    "                    'v_loss' : v_loss.detach().cpu(),\n",
    "                    'entropy' : entropy.detach().cpu()}\n",
    "        \n",
    "        return pg_loss, pg_dict\n",
    "    \n",
    "    def from_batch_state(self, batch_state):\n",
    "        lps = batch_state.model_gathered_logprobs\n",
    "        ref_lps = batch_state.reference_gathered_logprobs\n",
    "        \n",
    "        \n",
    "        mask = batch_state.mask\n",
    "        rewards = batch_state.rewards_scaled\n",
    "        traj_rewards = batch_state.trajectory_rewards\n",
    "        \n",
    "        values = batch_state.state_values\n",
    "        ref_values = batch_state.ref_state_values\n",
    "        \n",
    "        loss, pg_dict = self(lps, ref_lps, mask, rewards, \n",
    "                             values, ref_values, traj_rewards)\n",
    "        batch_state.losses.append(loss)\n",
    "        batch_state.pg_losses = loss\n",
    "        batch_state.pg_dict = pg_dict\n",
    "            \n",
    "    def compute_kl_reward(self, lps, ref_lps):\n",
    "        kl = lps - ref_lps\n",
    "        kl_reward = -self.kl_coef * kl.detach()\n",
    "        return kl_reward\n",
    "    \n",
    "    def value_loss(self, values, old_values, rewards):\n",
    "        if values is None:\n",
    "            v_loss = to_device(torch.tensor(0.))\n",
    "        else:\n",
    "            \n",
    "            v_loss = F.mse_loss(values, rewards, reduction='none')\n",
    "            \n",
    "            if old_values is not None:\n",
    "                min_v = old_values - self.v_cliprange\n",
    "                max_v = old_values + self.v_cliprange\n",
    "                \n",
    "                values_clipped = torch.max(torch.min(values, max_v), min_v)\n",
    "                v_loss2 = F.mse_loss(values_clipped, rewards, reduction='none')\n",
    "                \n",
    "                v_loss = torch.max(v_loss, v_loss2)\n",
    "            \n",
    "            v_loss = self.v_coef*v_loss.mean()\n",
    "            \n",
    "        return v_loss\n",
    "    \n",
    "    def compute_advantages(self, rewards, values):\n",
    "        \n",
    "        if values is None:\n",
    "            advantages = rewards\n",
    "        else:\n",
    "            advantages = compute_advantages(rewards, values.detach(), self.gamma, self.lam)\n",
    "            \n",
    "        return advantages\n",
    "    \n",
    "    def update_kl(self, lps, ref_lps, mask):\n",
    "        \n",
    "        if (self.kl_target is not None) and (self.kl_horizon is not None):\n",
    "            kl = (lps - ref_lps).detach()\n",
    "            kl = (kl*mask).sum(-1)/mask.sum(-1)\n",
    "            kl = kl.cpu().mean()\n",
    "            \n",
    "            error = torch.clip(kl/self.kl_target - 1, -0.2, 0.2)\n",
    "            factor = 1 + error * lps.shape[0]/self.kl_horizon\n",
    "            self.kl_coef *= factor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed00261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bca5e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BasePolicy():\n",
    "#     def __init__(self, gamma=1.):\n",
    "#         self.gamma = gamma\n",
    "\n",
    "#     def discount_rewards(self, model_outputs):\n",
    "#         rewards = model_outputs['rewards_scaled']\n",
    "#         mask = model_outputs['mask']\n",
    "#         rewards = scatter_rewards(rewards, mask)\n",
    "\n",
    "#         traj_rewards = model_outputs['trajectory_rewards']\n",
    "#         if traj_rewards is not None:\n",
    "#             rewards += traj_rewards\n",
    "\n",
    "#         discounted = discount_rewards(rewards, self.gamma)\n",
    "\n",
    "#         return discounted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ca7926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PolicyGradient(BasePolicy):\n",
    "#     def __init__(self, discount=True, gamma=0.97, ratio=False):\n",
    "#         super().__init__(gamma)\n",
    "#         self.discount = discount\n",
    "#         self.ratio = ratio\n",
    "\n",
    "#     def __call__(self, model_outputs):\n",
    "\n",
    "#         lps = model_outputs['model_gathered_logprobs']\n",
    "        \n",
    "#         if self.ratio:\n",
    "#             old_lps = model_outputs['reference_gathered_logprobs']\n",
    "#             lps = (lps - old_lps.detach()).exp()\n",
    "        \n",
    "#         mask = model_outputs['mask']\n",
    "#         rewards = model_outputs['rewards_scaled']\n",
    "\n",
    "#         if not self.discount:\n",
    "#             pg_loss = -((lps*mask).sum(-1)*rewards)/mask.sum(-1)\n",
    "#         else:\n",
    "#             rewards = self.discount_rewards(model_outputs)\n",
    "#             rewards = whiten(rewards)\n",
    "#             pg_loss = -(lps*rewards*mask).sum(-1)/mask.sum(-1)\n",
    "\n",
    "#         model_outputs['losses']['pg_loss'] = pg_loss.mean()\n",
    "#         model_outputs['loss_dicts']['pg_dict'] = {'pg_rewards' : rewards}\n",
    "        \n",
    "#         return model_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f26971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TRPO(BasePolicy):\n",
    "#     def __init__(self, gamma, kl_target, beta=1., eta=50, lam=0.95, v_coef=0.5):\n",
    "#         self.gamma = gamma\n",
    "#         self.beta = beta\n",
    "#         self.eta = eta\n",
    "#         self.lam = lam\n",
    "#         self.kl_target = kl_target\n",
    "#         self.v_coef = v_coef\n",
    "\n",
    "#     def __call__(self, model_outputs):\n",
    "#         discounted_rewards = self.discount_rewards(model_outputs)\n",
    "\n",
    "#         values = model_outputs['state_values']\n",
    "#         advantages = self.compute_advantages(discounted_rewards, values)\n",
    "#         advantages = whiten(advantages)\n",
    "\n",
    "#         v_loss = self.value_loss(values, discounted_rewards)\n",
    "\n",
    "#         lps = model_outputs['model_gathered_logprobs']\n",
    "#         ref_lps = model_outputs['reference_gathered_logprobs']\n",
    "#         mask = model_outputs['mask']\n",
    "\n",
    "#         ratios = (lps - ref_lps.detach()).exp()\n",
    "\n",
    "#         loss1 = -(ratios*advantages*mask).sum(-1)/mask.sum(-1)\n",
    "\n",
    "#         kl = torch.distributions.kl.kl_divergence(\n",
    "#                     Categorical(logits=model_outputs['reference_logprobs']),\n",
    "#                     Categorical(logits=model_outputs['model_logprobs'].detach()))\n",
    "\n",
    "#         kl = (kl*mask).sum(-1)/mask.sum(-1)\n",
    "#         kl = kl.mean()\n",
    "\n",
    "#         loss2 = self.beta*kl\n",
    "\n",
    "#         loss3 = self.eta * torch.maximum(to_device(torch.tensor(0.)), \n",
    "#                                          kl - 2.0*self.kl_target)\n",
    "        \n",
    "#         loss1 = loss1.mean()\n",
    "#         loss3 = loss3.mean()\n",
    "\n",
    "#         pg_loss = loss1 + loss2 + loss3 + v_loss\n",
    "\n",
    "#         model_outputs['losses']['pg_loss'] = pg_loss\n",
    "#         model_outputs['loss_dicts']['pg_dict'] = {'pg_discounted' : discounted_rewards,\n",
    "#                                                 'pg_advantage' : advantages,\n",
    "#                                                 'ratios' : ratios.detach().cpu(),\n",
    "#                                                 'kl' : kl.detach().cpu(),\n",
    "#                                                 'loss1' : loss1.detach().cpu(),\n",
    "#                                                 'loss2' : loss2.detach().cpu(),\n",
    "#                                                 'loss3' : loss3.detach().cpu(),\n",
    "#                                                 'v_loss' : v_loss.detach().cpu()}\n",
    "\n",
    "#         return model_outputs\n",
    "\n",
    "#     def compute_advantages(self, rewards, values):\n",
    "\n",
    "#         if values is None:\n",
    "#             advantages = rewards\n",
    "#         else:\n",
    "#             advantages = compute_advantages(rewards, values.detach(), self.gamma, self.lam)\n",
    "\n",
    "#         return advantages\n",
    "\n",
    "#     def value_loss(self, values, rewards):\n",
    "#         if values is None:\n",
    "#             v_loss = to_device(torch.tensor(0.))\n",
    "#         else:\n",
    "#             v_loss = self.v_coef*F.mse_loss(values, rewards)\n",
    "\n",
    "#         return v_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1432a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PPO(BasePolicy):\n",
    "#     def __init__(self, gamma, kl_coef, lam=0.95, v_coef=0.5, cliprange=0.2, \n",
    "#                  v_cliprange=0.2, ent_coef=0.01, kl_target=None, kl_horizon=None):\n",
    "#         self.gamma = gamma\n",
    "#         self.lam = lam\n",
    "#         self.ent_coef = ent_coef\n",
    "#         self.kl_coef = kl_coef\n",
    "#         self.kl_target = kl_target\n",
    "#         self.kl_horizon = kl_horizon\n",
    "#         self.v_coef = v_coef\n",
    "#         self.cliprange = cliprange\n",
    "#         self.v_cliprange = v_cliprange\n",
    "    \n",
    "#     def __call__(self, model_outputs):\n",
    "#         discounted_rewards = self.discount_rewards(model_outputs)\n",
    "        \n",
    "#         kl_reward = self.compute_kl_reward(model_outputs)\n",
    "#         discounted_rewards = discounted_rewards + kl_reward\n",
    "        \n",
    "#         values = model_outputs['state_values']\n",
    "#         old_values = model_outputs['old_state_values']\n",
    "#         advantages = self.compute_advantages(discounted_rewards, values)\n",
    "#         advantages = whiten(advantages)\n",
    "        \n",
    "#         v_loss = self.value_loss(values, old_values, discounted_rewards)\n",
    "        \n",
    "#         lps = model_outputs['model_gathered_logprobs']\n",
    "#         ref_lps = model_outputs['reference_gathered_logprobs']\n",
    "#         mask = model_outputs['mask']\n",
    "\n",
    "#         ratios = (lps - ref_lps).exp()\n",
    "#         ratios_clipped = torch.clamp(ratios, 1.0-self.cliprange, 1.0+self.cliprange)\n",
    "        \n",
    "#         loss1 = -(ratios*advantages)\n",
    "#         loss2 = -(ratios_clipped*advantages)\n",
    "        \n",
    "#         loss = torch.maximum(loss1, loss2)\n",
    "#         loss = (loss*mask).sum(-1)/mask.sum(-1)\n",
    "        \n",
    "#         entropy = Categorical(lps).entropy().mean()\n",
    "        \n",
    "#         loss = loss.mean()\n",
    "        \n",
    "#         pg_loss = loss + v_loss - self.ent_coef*entropy\n",
    "        \n",
    "#         self.update_kl(model_outputs)\n",
    "        \n",
    "#         model_outputs['losses']['pg_loss'] = pg_loss\n",
    "#         model_outputs['loss_dicts']['pg_dict'] = {'pg_discounted' : discounted_rewards,\n",
    "#                                     'pg_advantage' : advantages,\n",
    "#                                     'ratios' : ratios.detach().cpu(),\n",
    "#                                     'loss' : loss.detach().cpu(),\n",
    "#                                     'v_loss' : v_loss.detach().cpu(),\n",
    "#                                     'entropy' : entropy.detach().cpu()}\n",
    "        \n",
    "#         return model_outputs\n",
    "            \n",
    "#     def compute_kl_reward(self, model_outputs):\n",
    "#         lps = model_outputs['model_gathered_logprobs']\n",
    "#         ref_lps = model_outputs['reference_gathered_logprobs']\n",
    "#         kl = lps - ref_lps\n",
    "#         kl_reward = -self.kl_coef * kl.detach()\n",
    "#         return kl_reward\n",
    "    \n",
    "#     def value_loss(self, values, old_values, rewards):\n",
    "#         if values is None:\n",
    "#             v_loss = to_device(torch.tensor(0.))\n",
    "#         else:\n",
    "            \n",
    "#             v_loss = F.mse_loss(values, rewards, reduction='none')\n",
    "            \n",
    "#             if old_values is not None:\n",
    "#                 min_v = old_values - self.v_cliprange\n",
    "#                 max_v = old_values + self.v_cliprange\n",
    "                \n",
    "#                 values_clipped = torch.max(torch.min(values, max_v), min_v)\n",
    "#                 v_loss2 = F.mse_loss(values_clipped, rewards, reduction='none')\n",
    "                \n",
    "#                 v_loss = torch.max(v_loss, v_loss2)\n",
    "            \n",
    "#             v_loss = self.v_coef*v_loss.mean()\n",
    "            \n",
    "#         return v_loss\n",
    "    \n",
    "#     def compute_advantages(self, rewards, values):\n",
    "        \n",
    "#         if values is None:\n",
    "#             advantages = rewards\n",
    "#         else:\n",
    "#             advantages = compute_advantages(rewards, values.detach(), self.gamma, self.lam)\n",
    "            \n",
    "#         return advantages\n",
    "    \n",
    "#     def update_kl(self, model_outputs):\n",
    "        \n",
    "#         if (self.kl_target is not None) and (self.kl_horizon is not None):\n",
    "#             lps = model_outputs['model_gathered_logprobs']\n",
    "#             ref_lps = model_outputs['reference_gathered_logprobs']\n",
    "#             mask = model_outputs['mask']\n",
    "#             kl = (lps - ref_lps).detach()\n",
    "#             kl = (kl*mask).sum(-1)/mask.sum(-1)\n",
    "#             kl = kl.cpu().mean()\n",
    "            \n",
    "#             error = torch.clip(kl/self.kl_target - 1, -0.2, 0.2)\n",
    "#             factor = 1 + error * lps.shape[0]/self.kl_horizon\n",
    "#             self.kl_coef *= factor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238a7fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796dee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c64c604",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
