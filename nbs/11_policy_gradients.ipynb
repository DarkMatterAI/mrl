{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c92780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp policy_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a774717",
   "metadata": {},
   "source": [
    "# Policy Gradient\n",
    "\n",
    "> Policy gradient modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0482fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d9b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from mrl.imports import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *\n",
    "from mrl.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c342e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ddb948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class BasePolicy():\n",
    "    def __init__(self, gamma=1.):\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def discount_rewards(self, model_outputs):\n",
    "        rewards = model_outputs['rewards_scaled']\n",
    "        mask = model_outputs['mask']\n",
    "        rewards = scatter_rewards(rewards, mask)\n",
    "\n",
    "        traj_rewards = model_outputs['trajectory_rewards']\n",
    "        if traj_rewards is not None:\n",
    "            rewards += traj_rewards\n",
    "\n",
    "        discounted = discount_rewards(rewards, self.gamma)\n",
    "\n",
    "        return discounted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c05489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class PolicyGradient(BasePolicy):\n",
    "    def __init__(self, discount=True, gamma=0.97, ratio=False):\n",
    "        super().__init__(gamma)\n",
    "        self.discount = discount\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def __call__(self, model_outputs):\n",
    "\n",
    "        lps = model_outputs['model_gathered_logprobs']\n",
    "        \n",
    "        if self.ratio:\n",
    "            old_lps = model_outputs['reference_gathered_logprobs']\n",
    "            lps = (lps - old_lps.detach()).exp()\n",
    "        \n",
    "        mask = model_outputs['mask']\n",
    "        rewards = model_outputs['rewards_scaled']\n",
    "\n",
    "        if not self.discount:\n",
    "            pg_loss = -((lps*mask).sum(-1)*rewards)/mask.sum(-1)\n",
    "        else:\n",
    "            rewards = self.discount_rewards(model_outputs)\n",
    "            rewards = whiten(rewards)\n",
    "            pg_loss = -(lps*rewards*mask).sum(-1)/mask.sum(-1)\n",
    "\n",
    "        model_outputs['pg_loss'] = pg_loss.mean()\n",
    "        model_outputs['pg_dict'] = {'pg_rewards' : rewards}\n",
    "\n",
    "        return model_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda73363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class TRPO(BasePolicy):\n",
    "    def __init__(self, gamma, kl_target, beta=1., eta=50, lam=0.95, v_coef=0.5):\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.eta = eta\n",
    "        self.lam = lam\n",
    "        self.kl_target = kl_target\n",
    "        self.v_coef = v_coef\n",
    "\n",
    "    def __call__(self, model_outputs):\n",
    "        discounted_rewards = self.discount_rewards(model_outputs)\n",
    "\n",
    "        values = model_outputs['state_values']\n",
    "        advantages = self.compute_advantages(discounted_rewards, values)\n",
    "        advantages = whiten(advantages)\n",
    "\n",
    "        v_loss = self.value_loss(values, discounted_rewards)\n",
    "\n",
    "        lps = model_outputs['model_gathered_logprobs']\n",
    "        ref_lps = model_outputs['reference_gathered_logprobs']\n",
    "        mask = model_outputs['mask']\n",
    "\n",
    "        ratios = (lps - ref_lps.detach()).exp()\n",
    "\n",
    "        loss1 = -(ratios*advantages*mask).sum(-1)/mask.sum(-1)\n",
    "\n",
    "        kl = torch.distributions.kl.kl_divergence(\n",
    "                    Categorical(logits=model_outputs['reference_logprobs']),\n",
    "                    Categorical(logits=model_outputs['model_logprobs'].detach()))\n",
    "\n",
    "        kl = (kl*mask).sum(-1)/mask.sum(-1)\n",
    "        kl = kl.mean()\n",
    "\n",
    "        loss2 = self.beta*kl\n",
    "\n",
    "        loss3 = self.eta * torch.maximum(to_device(torch.tensor(0.)), \n",
    "                                         kl - 2.0*self.kl_target)\n",
    "        \n",
    "        loss1 = loss1.mean()\n",
    "        loss3 = loss3.mean()\n",
    "\n",
    "        pg_loss = loss1 + loss2 + loss3 + v_loss\n",
    "\n",
    "        model_outputs['pg_loss'] = pg_loss\n",
    "        model_outputs['pg_dict'] = {'pg_discounted' : discounted_rewards,\n",
    "                                    'pg_advantage' : advantages,\n",
    "                                    'ratios' : ratios.detach().cpu(),\n",
    "                                    'kl' : kl.detach().cpu(),\n",
    "                                    'loss1' : loss1.detach().cpu(),\n",
    "                                    'loss2' : loss2.detach().cpu(),\n",
    "                                    'loss3' : loss3.detach().cpu(),\n",
    "                                    'v_loss' : v_loss.detach().cpu()}\n",
    "\n",
    "        return model_outputs\n",
    "\n",
    "    def compute_advantages(self, rewards, values):\n",
    "\n",
    "        if values is None:\n",
    "            advantages = rewards\n",
    "        else:\n",
    "            advantages = compute_advantages(rewards, values.detach(), self.gamma, self.lam)\n",
    "\n",
    "        return advantages\n",
    "\n",
    "    def value_loss(self, values, rewards):\n",
    "        if values is None:\n",
    "            v_loss = torch.tensor(0.)\n",
    "        else:\n",
    "            v_loss = self.v_coef*F.mse_loss(values, rewards)\n",
    "\n",
    "        return v_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2b3a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export   \n",
    "\n",
    "class PPO(BasePolicy):\n",
    "    def __init__(self, gamma, kl_coef, lam=0.95, v_coef=0.5, cliprange=0.2, ent_coef=0.01,\n",
    "                 kl_target=None, kl_horizon=None):\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.ent_coef = ent_coef\n",
    "        self.kl_coef = kl_coef\n",
    "        self.kl_target = kl_target\n",
    "        self.kl_horizon = kl_horizon\n",
    "        self.v_coef = v_coef\n",
    "        self.cliprange = cliprange\n",
    "    \n",
    "    def __call__(self, model_outputs):\n",
    "        discounted_rewards = self.discount_rewards(model_outputs)\n",
    "        \n",
    "        kl_reward = self.compute_kl_reward(model_outputs)\n",
    "        discounted_rewards = discounted_rewards + kl_reward\n",
    "        \n",
    "        values = model_outputs['state_values']\n",
    "        advantages = self.compute_advantages(discounted_rewards, values)\n",
    "        advantages = whiten(advantages)\n",
    "        \n",
    "        values = model_outputs['state_values']\n",
    "        v_loss = self.value_loss(values, discounted_rewards)\n",
    "        \n",
    "        lps = model_outputs['model_gathered_logprobs']\n",
    "        ref_lps = model_outputs['reference_gathered_logprobs']\n",
    "        mask = model_outputs['mask']\n",
    "\n",
    "        ratios = (lps - ref_lps).exp()\n",
    "        ratios_clipped = torch.clamp(ratios, 1.0-self.cliprange, 1.0+self.cliprange)\n",
    "        \n",
    "        loss1 = -(ratios*advantages)\n",
    "        loss2 = -(ratios_clipped*advantages)\n",
    "        \n",
    "        loss = torch.maximum(loss1, loss2)\n",
    "        loss = (loss*mask).sum(-1)/mask.sum(-1)\n",
    "        \n",
    "        entropy = Categorical(lps).entropy().mean()\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        \n",
    "        pg_loss = loss + v_loss - self.ent_coef*entropy\n",
    "        \n",
    "        self.update_kl(model_outputs)\n",
    "        \n",
    "        model_outputs['pg_loss'] = pg_loss\n",
    "        model_outputs['pg_dict'] = {'pg_discounted' : discounted_rewards,\n",
    "                                    'pg_advantage' : advantages,\n",
    "                                    'ratios' : ratios.detach().cpu(),\n",
    "                                    'loss' : loss.detach().cpu(),\n",
    "                                    'v_loss' : v_loss.detach().cpu(),\n",
    "                                    'entropy' : entropy.detach().cpu()}\n",
    "        \n",
    "        return model_outputs\n",
    "            \n",
    "    def compute_kl_reward(self, model_outputs):\n",
    "        lps = model_outputs['model_gathered_logprobs']\n",
    "        ref_lps = model_outputs['reference_gathered_logprobs']\n",
    "        kl = lps - ref_lps\n",
    "        kl_reward = -self.kl_coef * kl.detach()\n",
    "        return kl_reward\n",
    "    \n",
    "    def value_loss(self, values, rewards):\n",
    "        if values is None:\n",
    "            v_loss = torch.tensor(0.)\n",
    "        else:\n",
    "            v_loss = self.v_coef*F.mse_loss(values, rewards)\n",
    "            \n",
    "        return v_loss\n",
    "    \n",
    "    def compute_advantages(self, rewards, values):\n",
    "        \n",
    "        if values is None:\n",
    "            advantages = rewards\n",
    "        else:\n",
    "            advantages = compute_advantages(rewards, values.detach(), self.gamma, self.lam)\n",
    "            \n",
    "        return advantages\n",
    "    \n",
    "    def update_kl(self, model_outputs):\n",
    "        \n",
    "        if (self.kl_target is not None) and (self.kl_horizon is not None):\n",
    "            lps = model_outputs['model_gathered_logprobs']\n",
    "            ref_lps = model_outputs['reference_gathered_logprobs']\n",
    "            mask = model_outputs['mask']\n",
    "            kl = (lps - ref_lps).detach()\n",
    "            kl = (kl*mask).sum(-1)/mask.sum(-1)\n",
    "            kl = kl.cpu()\n",
    "            \n",
    "            error = torch.clip(kl/self.kl_target - 1, -0.2, 0.2)\n",
    "            factor = 1 + errror * lps.shape[0]/self.kl_horizon\n",
    "            self.kl_coef *= factor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17be7e97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c891c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955d4ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
