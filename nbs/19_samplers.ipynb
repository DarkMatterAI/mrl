{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp train.sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Samplers\n",
    "\n",
    "> Sampling callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmai/miniconda3/envs/mrl/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for boost::shared_ptr<RDKit::FilterCatalogEntry const> already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "from mrl.imports import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *\n",
    "from mrl.train.callback import *\n",
    "from mrl.chem import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampler Callbacks\n",
    "\n",
    "Samplers serve two main functions in the fit loop. \n",
    "\n",
    "During the `build_buffer` event, samplers add samples to the `Buffer`\n",
    "\n",
    "During the `sample_batch` event, samplers add samples to the current `BatchState`\n",
    "\n",
    "Samplers generally have the ability to toggle which events they add samples to. For example if you wanted to do entirely offline RL, you could disable live sampling during the `sample_batch` event and only train off samples stored in the buffer\n",
    "\n",
    "### Sampler Size\n",
    "\n",
    "Samplers have two main parameters that control sample size.\n",
    "\n",
    "The `buffer_size` parameter is an integer value that control how many samples are generated during the `build_buffer` event.\n",
    "\n",
    "The `p_batch` parameter is a float value between 0 and 1 that determines what percentage of a batch should be drawn from a specific sampler. When using multiple samplers, the sum of all `p_batch` values should be less than or equal to 1. The difference between the sum of `p_batch` values and the desired batch size will be made up by sampling from the buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Sampler(Callback):\n",
    "    '''\n",
    "    Sampler - base sampler callback\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `name str`: sampler name\n",
    "    \n",
    "    - `buffer_size int`: how many samples to add \n",
    "    during `build_buffer`\n",
    "    \n",
    "    - `p_batch float`: what percentage of batch \n",
    "    samples should come from this sampler\n",
    "    \n",
    "    - `track bool`: if metrics from this sampler \n",
    "    should be tracked\n",
    "    '''\n",
    "    def __init__(self, name, buffer_size=0, p_batch=0., track=True):\n",
    "        super().__init__(name=name)\n",
    "        self.name = name\n",
    "        self.buffer_size = buffer_size\n",
    "        self.p_batch = p_batch\n",
    "        self.track = track\n",
    "            \n",
    "    def _build_buffer(self):\n",
    "        return []\n",
    "        \n",
    "    def _sample_batch(self):\n",
    "        return []\n",
    "        \n",
    "    def build_buffer(self):\n",
    "        outputs = self._build_buffer()\n",
    "        if outputs:\n",
    "            self.environment.buffer.add(outputs, self.name)\n",
    "    \n",
    "    def sample_batch(self):\n",
    "        outputs = self._sample_batch()\n",
    "        if outputs:\n",
    "            self.environment.batch_state.samples += outputs\n",
    "            self.environment.batch_state.sources += [self.name]*len(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class DatasetSampler(Sampler):\n",
    "    '''\n",
    "    DatasetSampler - adds items from `data` \n",
    "    to either the buffer or the current batch\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `data list`: list of data points to sample from\n",
    "    \n",
    "    - `name str`: sampler name\n",
    "    \n",
    "    - `buffer_size int`: how many samples to add \n",
    "    during `build_buffer`\n",
    "    \n",
    "    - `p_batch float`: what percentage of batch \n",
    "    samples should come from this sampler\n",
    "    '''\n",
    "    def __init__(self, data, name, buffer_size=0, p_batch=0.):\n",
    "        super().__init__(name, buffer_size, p_batch)\n",
    "        self.data = data\n",
    "        \n",
    "    def sample_data(self, n):\n",
    "        n = min(n, len(self.data))\n",
    "        idxs = np.random.randint(0, len(self.data), n)\n",
    "        samples = [self.data[i] for i in idxs]\n",
    "        return samples\n",
    "        \n",
    "    def _build_buffer(self):\n",
    "        samples = []\n",
    "        if self.buffer_size>0:\n",
    "            samples = self.sample_data(self.buffer_size)\n",
    "        return samples\n",
    "    \n",
    "    def _sample_batch(self):\n",
    "        samples = []\n",
    "        bs = self.environment.bs\n",
    "        bs_sample = int(self.p_batch*bs)\n",
    "        \n",
    "        if bs_sample > 0:\n",
    "            sample = self.sample_data(bs_sample)\n",
    "        \n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Sampler\n",
    "\n",
    "The `ModelSampler` sampler can be used to draw samples from any `GenerativeModel` subclass model. By default, it will track the following sample metrics:\n",
    "\n",
    "- diversity - how many duplicate samples were generated\n",
    "- valid - how many samples are left after filtering\n",
    "- rewards - average rewards from samples generated by the model sampler\n",
    "- new - how many samples are novel to the training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export          \n",
    "                  \n",
    "class ModelSampler(Sampler):\n",
    "    '''\n",
    "    ModelSampler - sampler class to draw samples from a `GenerativeModel`\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `vocab Vocab`: vocabulary for reconstructing samples\n",
    "    \n",
    "    - `model GenerativeModel`: model to sample from\n",
    "    \n",
    "    - `name str`: sampler name\n",
    "    \n",
    "    - `buffer_size int`: number of samples to generate during `build_buffer`\n",
    "    \n",
    "    - `p_batch float`: what percentage of batch \n",
    "    samples should come from this sampler\n",
    "    \n",
    "    - `genbatch int`: generation batch size\n",
    "    \n",
    "    - `track bool`: if metrics from this sampler should be tracked\n",
    "    \n",
    "    - `temperature float`: sampeling temperature\n",
    "    '''\n",
    "    def __init__(self, vocab, model, name, buffer_size, p_batch, \n",
    "                 genbatch, track=True, temperature=1.):\n",
    "        super().__init__(name, buffer_size, p_batch, track)\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.model = model\n",
    "        self.genbatch = genbatch\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def setup(self):\n",
    "        if self.p_batch>0. and self.track:\n",
    "            log = self.environment.log\n",
    "            log.add_metric(f'{self.name}_diversity')\n",
    "            log.add_metric(f'{self.name}_valid')\n",
    "            log.add_metric(f'{self.name}_rewards')\n",
    "            log.add_metric(f'{self.name}_new')\n",
    "            \n",
    "    def build_buffer(self):\n",
    "        env = self.environment\n",
    "        sl = env.sl\n",
    "        outputs = self._build_buffer(sl)\n",
    "        if outputs:\n",
    "            self.environment.buffer.add(outputs, self.name)\n",
    "        \n",
    "    def _build_buffer(self, sl): \n",
    "        buffer_size = self.buffer_size\n",
    "        outputs = []\n",
    "        to_generate = buffer_size\n",
    "        \n",
    "        if buffer_size > 0:\n",
    "            for batch in range(int(np.ceil(buffer_size/self.genbatch))):\n",
    "                current_bs = min(self.genbatch, to_generate)\n",
    "                \n",
    "                preds, _ = self.model.sample_no_grad(current_bs, sl, multinomial=True,\n",
    "                                                     temperature=self.temperature)\n",
    "                sequences = [self.vocab.reconstruct(i) for i in preds]\n",
    "                sequences = list(set(sequences))\n",
    "                outputs += sequences\n",
    "                outputs = list(set(outputs))\n",
    "                to_generate = buffer_size - len(outputs)\n",
    "                \n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "    def sample_batch(self):\n",
    "        env = self.environment\n",
    "        bs = env.bs\n",
    "        sl = env.sl\n",
    "        outputs = self._sample_batch(bs, sl)\n",
    "        env.batch_state[f'{self.name}_raw'] = outputs\n",
    "        if outputs:\n",
    "            env.batch_state.samples += outputs\n",
    "            env.batch_state.sources += [self.name]*len(outputs)\n",
    "            \n",
    "            \n",
    "    def _sample_batch(self, bs, sl):\n",
    "        bs = int(bs * self.p_batch)\n",
    "        sequences = []\n",
    "        \n",
    "        if bs > 0:\n",
    "            \n",
    "            preds, _ = self.model.sample_no_grad(bs, sl, z=None, multinomial=True,\n",
    "                                                temperature=self.temperature)\n",
    "            \n",
    "            sequences = [self.vocab.reconstruct(i) for i in preds]\n",
    "                \n",
    "        return sequences\n",
    "        \n",
    "    def after_compute_reward(self):\n",
    "        if self.p_batch>0. and self.track:\n",
    "            batch_state = self.environment.batch_state\n",
    "            log = self.environment.log\n",
    "            rewards = batch_state.rewards.detach().cpu().numpy()\n",
    "            sources = np.array(batch_state.sources)\n",
    "            \n",
    "            if self.name in sources:\n",
    "                log.update_metric(f'{self.name}_rewards', rewards[sources==self.name].mean())\n",
    "            else:\n",
    "                log.update_metric(f'{self.name}_rewards', 0.)\n",
    "        \n",
    "    def after_sample(self):\n",
    "        if self.p_batch > 0. and self.track:\n",
    "            log = self.environment.log\n",
    "            batch_state = self.environment.batch_state\n",
    "            samples = batch_state.samples\n",
    "            sources = np.array(batch_state.sources)==self.name\n",
    "\n",
    "            samples = [samples[i] for i in range(len(samples)) if sources[i]]\n",
    "            \n",
    "            raw_samples = batch_state[f'{self.name}_raw']\n",
    "            \n",
    "            if len(samples)>0:\n",
    "                diversity = len(set(raw_samples))/len(raw_samples)\n",
    "                used = log.unique_samples\n",
    "                novel = [i for i in samples if not i in used]\n",
    "                percent_novel = len(novel)/len(samples)\n",
    "            else:\n",
    "                diversity = 0\n",
    "                percent_novel = 0.\n",
    "                \n",
    "            valid = len(samples)/len(set(batch_state[f'{self.name}_raw']))\n",
    "\n",
    "            log.update_metric(f'{self.name}_new', percent_novel)\n",
    "            log.update_metric(f\"{self.name}_diversity\", diversity)\n",
    "            log.update_metric(f\"{self.name}_valid\", valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior Sampler\n",
    "\n",
    "`PriorSampler` allows for sampling based on latent vectors from a specific prior distribution. If desired, this prior can also be optimized during the fit loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class PriorSampler(ModelSampler):\n",
    "    '''\n",
    "    PriorSampler - sampler class to draw samples \n",
    "    from latent vectors generated by `prior`\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `vocab Vocab`: vocabulary for reconstructing samples\n",
    "    \n",
    "    - `model GenerativeModel`: model to sample from\n",
    "    \n",
    "    - `prior nn.Module`: prior to sample latent vectors from\n",
    "    \n",
    "    - `name str`: sampler name\n",
    "    \n",
    "    - `buffer_size int`: number of samples to generate during `build_buffer`\n",
    "    \n",
    "    - `p_batch float`: what percentage of batch \n",
    "    samples should come from this sampler\n",
    "    \n",
    "    - `genbatch int`: generation batch size\n",
    "    \n",
    "    - `track bool`: if metrics from this sampler should be tracked\n",
    "    \n",
    "    - `track_losses bool`: if prior losses should be tracked \n",
    "    (ignored if no prior loss is given)\n",
    "    \n",
    "    - `train bool`: if the prior should be trained (requires prior loss)\n",
    "    \n",
    "    - `train_all bool`: if the prior should be trained on all\n",
    "    samples in a batch or just ones from this specific sampler\n",
    "    \n",
    "    - `prior_loss Optional`: loss function for prior. See \n",
    "    `PriorLoss` for an example\n",
    "    \n",
    "    - `temperature float`: sampeling temperature\n",
    "    '''\n",
    "    def __init__(self, vocab, model, prior, name, buffer_size, \n",
    "                 p_batch, genbatch, track=True, track_losses=True, train=True,\n",
    "                 train_all=False, prior_loss=None,\n",
    "                 temperature=1., opt_kwargs={}):\n",
    "        super().__init__(vocab, \n",
    "                         model, \n",
    "                         name, \n",
    "                         buffer_size, \n",
    "                         p_batch, \n",
    "                         genbatch, \n",
    "                         track, \n",
    "                         temperature)\n",
    "        \n",
    "        self.train = train\n",
    "        self.track_losses = track_losses\n",
    "        self.train_all = train_all\n",
    "        self.set_prior(prior, opt_kwargs)\n",
    "        self.prior_loss = prior_loss\n",
    "        \n",
    "    def setup(self):\n",
    "        super().setup()\n",
    "        if self.train and self.prior_loss is not None:\n",
    "            self.environment.log.add_log(self.name+'_loss')\n",
    "            if self.track_losses:\n",
    "                self.environment.log.add_metric(self.name+'_loss')\n",
    "        \n",
    "        \n",
    "    def set_prior(self, prior, opt_kwargs):\n",
    "        self.prior = to_device(prior)\n",
    "        if self.train:\n",
    "            self.opt = optim.Adam(self.prior.parameters(), **opt_kwargs)\n",
    "            \n",
    "    def zero_grad(self):\n",
    "        if self.train:\n",
    "            self.opt.zero_grad()\n",
    "        \n",
    "    def step(self):\n",
    "        if self.train:\n",
    "            self.opt.step()\n",
    "            \n",
    "    def _build_buffer(self, sl): \n",
    "        buffer_size = self.buffer_size\n",
    "        outputs = []\n",
    "        to_generate = buffer_size\n",
    "        \n",
    "        if buffer_size > 0:\n",
    "            for batch in range(int(np.ceil(buffer_size/self.genbatch))):\n",
    "                current_bs = min(self.genbatch, to_generate)\n",
    "                \n",
    "                z = self.prior.sample(current_bs)\n",
    "                preds, _ = self.model.sample_no_grad(current_bs, sl, z=z, multinomial=True,\n",
    "                                                     temperature=self.temperature)\n",
    "                sequences = [self.vocab.reconstruct(i) for i in preds]\n",
    "                sequences = list(set(sequences))\n",
    "                outputs += sequences\n",
    "                outputs = list(set(outputs))\n",
    "                to_generate = buffer_size - len(outputs)\n",
    "                \n",
    "        return outputs\n",
    "    \n",
    "    def sample_batch(self):\n",
    "        env = self.environment\n",
    "        bs = env.bs\n",
    "        sl = env.sl\n",
    "        sequences, sample_latents = self._sample_batch(bs, sl)\n",
    "        \n",
    "        env.batch_state[f'{self.name}_raw'] = sequences\n",
    "        env.batch_state.latent_data[self.name] = sample_latents\n",
    "        \n",
    "        if sequences:\n",
    "            env.batch_state.samples += sequences\n",
    "            env.batch_state.sources += [self.name]*len(sequences)\n",
    "    \n",
    "    def _sample_batch(self, bs, sl):\n",
    "        bs = int(bs * self.p_batch)\n",
    "        sequences = []\n",
    "        sample_latents = []\n",
    "        \n",
    "        if bs > 0:\n",
    "            \n",
    "            z = self.prior.rsample(bs)\n",
    "\n",
    "            preds, _ = self.model.sample_no_grad(bs, sl, z=z, multinomial=True,\n",
    "                                                temperature=self.temperature)\n",
    "            sequences = [self.vocab.reconstruct(i) for i in preds]\n",
    "                            \n",
    "        return sequences, z\n",
    "    \n",
    "    def compute_loss(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        \n",
    "        if self.train and self.prior_loss is not None:\n",
    "            if self.train_all:\n",
    "                subset_name = None\n",
    "            else:\n",
    "                subset_name = self.name\n",
    "                \n",
    "            loss = self.prior_loss.from_batch_state(batch_state, subset_name)\n",
    "\n",
    "            if self.track_losses:\n",
    "                \n",
    "                loss_d = loss.detach().cpu()\n",
    "                \n",
    "                if self.train_all:\n",
    "                    mean_loss = loss_d.mean()\n",
    "                else:\n",
    "                    mean_loss = loss_d[loss_d.nonzero()[0]].mean()\n",
    "                    \n",
    "                self.environment.log.update_metric(self.name+'_loss', mean_loss.numpy())\n",
    "\n",
    "            self.environment.batch_state.loss += loss.mean()\n",
    "            self.environment.batch_state[self.name+'_loss'] = loss.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Sampler\n",
    "\n",
    "`LatentSampler` allows for sampling based on specific latent vectors. If desired, the latent vectors can also be optimized during the fit loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export          \n",
    "                  \n",
    "class LatentSampler(ModelSampler):\n",
    "    '''\n",
    "    ModelSampler - sampler class to draw samples from a `GenerativeModel`\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `vocab Vocab`: vocabulary for reconstructing samples\n",
    "    \n",
    "    - `model GenerativeModel`: model to sample from\n",
    "    \n",
    "    - `latents torch.FloatTensor[n_latents, d_latents]`: \n",
    "    tensor of latent vectors. `n_latents` can be any value\n",
    "    \n",
    "    - `name str`: sampler name\n",
    "    \n",
    "    - `buffer_size int`: number of samples to generate during `build_buffer`\n",
    "    \n",
    "    - `p_batch float`: what percentage of batch \n",
    "    samples should come from this sampler\n",
    "    \n",
    "    - `genbatch int`: generation batch size\n",
    "    \n",
    "    - `track bool`: if metrics from this sampler should be tracked\n",
    "    \n",
    "    - `train bool`: if the latent vectors should be trained \n",
    "    \n",
    "    - `temperature float`: sampeling temperature\n",
    "    '''\n",
    "    def __init__(self, vocab, model, latents, name, buffer_size, \n",
    "                 p_batch, genbatch, track=True, train=True,\n",
    "                 temperature=1., opt_kwargs={}):\n",
    "        super().__init__(vocab, \n",
    "                         model, \n",
    "                         name, \n",
    "                         buffer_size, \n",
    "                         p_batch, \n",
    "                         genbatch, \n",
    "                         track, \n",
    "                         temperature)\n",
    "        \n",
    "        self.train = train\n",
    "        self.set_latents(latents, opt_kwargs)\n",
    "        \n",
    "    def set_latents(self, latents, opt_kwargs):\n",
    "        self.latents = to_device(latents)\n",
    "        if self.train:\n",
    "            self.latents = nn.Parameter(self.latents)\n",
    "            self.opt = optim.Adam([self.latents], **opt_kwargs)\n",
    "        else:\n",
    "            self.latents._requires_grad(False)\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        if self.train:\n",
    "            self.opt.zero_grad()\n",
    "        \n",
    "    def step(self):\n",
    "        if self.train:\n",
    "            self.opt.step()\n",
    "        \n",
    "    def _build_buffer(self, sl): \n",
    "        return []\n",
    "    \n",
    "    def sample_batch(self):\n",
    "        env = self.environment\n",
    "        bs = env.bs\n",
    "        sl = env.sl\n",
    "        sequences, sample_latents = self._sample_batch(bs, sl)\n",
    "        \n",
    "        env.batch_state[f'{self.name}_raw'] = sequences\n",
    "        env.batch_state.latent_data[self.name] = sample_latents\n",
    "        \n",
    "        if sequences:\n",
    "            env.batch_state.samples += sequences\n",
    "            env.batch_state.sources += [self.name]*len(sequences)\n",
    "    \n",
    "    def _sample_batch(self, bs, sl):\n",
    "        bs = int(bs * self.p_batch)\n",
    "        sequences = []\n",
    "        sample_latents = []\n",
    "        \n",
    "        if bs > 0:\n",
    "            \n",
    "            latent_idxs = torch.randint(0, self.latents.shape[0]-1, (bs,))\n",
    "            sample_latents = self.latents[latent_idxs]\n",
    "\n",
    "            preds, _ = self.model.sample_no_grad(bs, sl, z=sample_latents, multinomial=True,\n",
    "                                                temperature=self.temperature)\n",
    "            sequences = [self.vocab.reconstruct(i) for i in preds]\n",
    "                            \n",
    "        return sequences, sample_latents\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive Sampler \n",
    "\n",
    "So far samplers have focused on drawing individual samples from a model, dataset, or other source.\n",
    "\n",
    "Contrastive sampling looks at the task of generating a new sample based on an old sample. For example, we could want to train a model to take in a compound and produce different compound with a high similarity to the original compound but with a better score based on some metric.\n",
    "\n",
    "In these cases, the samples we create will be a tuple in the form `(source_sample, target_sample)`. When training a contrastive metric, we may have a pre-made dataset of `source, target` pairs to use. However, if such paired data doesn't exist, we need some way to generate it on the fly. This is where the `ContrastiveSampler` class comes in.\n",
    "\n",
    "`ContrastiveSampler` turns any normal Sampler into a contrastive sampler. The contrastive sampler uses a `base_sampler` to generate an initial set of `source` samples. Then the contrastive sampler uses an `output_model` to generate `target` samples on the fly.\n",
    "\n",
    "This generation process can be run during `build_buffer` or `sample_batch`.\n",
    "\n",
    "Note that the `ContrastiveSampler` does not do any batch or buffer filtering to ensure `source, target` pairs match external constraints like minimum similarity. This must be handled by other callbacks, like `ContrastiveTemplate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class ContrastiveSampler(Sampler):\n",
    "    '''\n",
    "    ContrastiveSampler - contrastive sampling wrapper. Uses \n",
    "    `base_sampler` to generate source sequences. Then uses \n",
    "    `output_model` to generate target sequences. Adds tuple \n",
    "    pairs of `(source, target)` to batch/buffer\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `base_sampler Sampler`: base sampler to generate \n",
    "    source samples\n",
    "    \n",
    "    - `vocab Vocab`: vocab for reconstruction\n",
    "    \n",
    "    - `dataset Base_Dataset`: dataset for tensorizing samples\n",
    "    \n",
    "    - `output_moodel GenerativeModel`: model to generate \n",
    "    target samples\n",
    "    \n",
    "    - `bs int`: batch size for contrastive generation\n",
    "    \n",
    "    - `repeats int`: how many target samples to draw from \n",
    "    each source sample\n",
    "    '''\n",
    "    def __init__(self, base_sampler, vocab, dataset, output_model, bs, repeats=1):\n",
    "        super().__init__(base_sampler.name, base_sampler.buffer_size,\n",
    "                         base_sampler.p_batch, base_sampler.track)\n",
    "        \n",
    "        self.base_sampler = base_sampler\n",
    "        self.vocab = vocab\n",
    "        self.dataset = dataset\n",
    "        self.output_model = output_model\n",
    "        self.bs = bs\n",
    "        self.repeats = repeats\n",
    "        \n",
    "    def __call__(self, event_name):\n",
    "        \n",
    "        event = getattr(self, event_name, None)\n",
    "        \n",
    "        if event is not None:\n",
    "            output = event()\n",
    "        else:\n",
    "            output = None\n",
    "            \n",
    "        if not (event_name in ['build_buffer', 'sample_batch']):\n",
    "            _ = self.base_sampler(event_name)\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    def setup(self):\n",
    "        self.base_sampler.environment = self.environment\n",
    "        \n",
    "    def sample_outputs(self, sequences, sl):   \n",
    "        sequences = list(sequences)\n",
    "        if self.repeats>1:\n",
    "            sequences = sequences*self.repeats\n",
    "        pairs = [(i,'') for i in sequences]\n",
    "        batch_ds = self.dataset.new(pairs)\n",
    "        \n",
    "        if len(batch_ds)<self.bs:\n",
    "        \n",
    "            batch = batch_ds.collate_function([batch_ds[i] for i in range(len(batch_ds))])\n",
    "            batch = to_device(batch)\n",
    "            x,_ = batch\n",
    "            z = self.output_model.x_to_latent(x)\n",
    "            preds, _ = self.output_model.sample_no_grad(z.shape[0], sl, z=z)\n",
    "            new_sequences = [self.vocab.reconstruct(i) for i in preds]\n",
    "            \n",
    "        else:\n",
    "            batch_dl = batch_ds.dataloader(self.bs, shuffle=False)\n",
    "\n",
    "            new_sequences = []\n",
    "\n",
    "            for i, batch in enumerate(batch_dl):\n",
    "                batch = to_device(batch)\n",
    "                x,_ = batch\n",
    "                z = self.output_model.x_to_latent(x)\n",
    "                preds, _ = self.output_model.sample_no_grad(z.shape[0], sl, z=z)\n",
    "                new_sequences += [self.vocab.reconstruct(i) for i in preds]\n",
    "        \n",
    "        outputs = [(sequences[i], new_sequences[i]) for i in range(len(sequences))]\n",
    "              \n",
    "        return outputs\n",
    "        \n",
    "    def _build_buffer(self):\n",
    "        outputs = self.base_sampler._build_buffer()\n",
    "        env = self.environment\n",
    "        sl = env.sl\n",
    "        if outputs:\n",
    "            outputs = self.sample_outputs(outputs, sl)\n",
    "        return outputs\n",
    "    \n",
    "    def _sample_batch(self):\n",
    "        outputs = self.base_sampler._sample_batch()\n",
    "        env = self.environment\n",
    "        sl = env.sl\n",
    "        if outputs:\n",
    "            outputs = self.sample_outputs(outputs, sl)\n",
    "            env.batch_state[f'{self.name}_raw_contrastive'] = outputs\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Samplers\n",
    "\n",
    "Log samplers pull high scoring samples out of the log for retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LogSampler(Sampler):\n",
    "    '''\n",
    "    LogSampler - pulls samples from log \n",
    "    based on `percentile`\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `sample_name str`: what column in `Log.df` to pull \n",
    "    samples from\n",
    "    \n",
    "    - `lookup_name str`: what column in `Log.df` to use \n",
    "    to find high scoring samples\n",
    "    \n",
    "    - `start_iter int`: iteration to start drawing from log\n",
    "    \n",
    "    - `percentile int`: value 1-100 percentile of \n",
    "    log data to sample from\n",
    "    \n",
    "    - `buffer_size int`: number of samples to generate during `build_buffer`\n",
    "    '''\n",
    "    def __init__(self, sample_name, lookup_name, start_iter, percentile, buffer_size):\n",
    "        super().__init__(sample_name+'_sample', buffer_size, p_batch=0.)\n",
    "        self.start_iter = start_iter\n",
    "        self.percentile = percentile\n",
    "        self.sample_name = sample_name\n",
    "        self.lookup_name = lookup_name\n",
    "        \n",
    "    def build_buffer(self):\n",
    "        env = self.environment\n",
    "        iterations = self.environment.log.iterations\n",
    "        df = env.log.df\n",
    "        \n",
    "        outputs = self._build_buffer(iterations, df)\n",
    "        if outputs:\n",
    "            self.environment.buffer.add(outputs, self.name)\n",
    "                    \n",
    "    def _build_buffer(self, iterations, df):\n",
    "        outputs = []\n",
    "\n",
    "        if iterations > self.start_iter:\n",
    "            bs = self.buffer_size\n",
    "            if bs > 0:\n",
    "                \n",
    "                subset = df[df[self.lookup_name]>np.percentile(df[self.lookup_name].values, \n",
    "                                                               self.percentile)]\n",
    "                outputs = list(subset.sample(n=min(bs, subset.shape[0]))[self.sample_name].values)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class TokenSwapSampler(LogSampler):\n",
    "    '''\n",
    "    TokenSwapSampler - samples high scoring samples from \n",
    "    `Log.df` and enumerates variants by swapping tokens. \n",
    "    Note that token swapped samples are not guaranteed to be \n",
    "    chemically valid. This sampler works best with SELFIES \n",
    "    representation\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `sample_name str`: what column in `Log.df` to pull \n",
    "    samples from\n",
    "    \n",
    "    - `lookup_name str`: what column in `Log.df` to use \n",
    "    to find high scoring samples\n",
    "    \n",
    "    - `start_iter int`: iteration to start drawing from log\n",
    "    \n",
    "    - `percentile int`: value 1-100 percentile of \n",
    "    log data to sample from\n",
    "    \n",
    "    - `buffer_size int`: number of samples to generate during `build_buffer`\n",
    "    \n",
    "    - `vocab Vocab`: vocab to numericalize samples\n",
    "    \n",
    "    - `swap_percent float`: percent of tokens to swap per sample\n",
    "    '''\n",
    "    # TODO: make version of this where tokens to swap are selected based on log probs\n",
    "    def __init__(self, sample_name, lookup_name, start_iter, percentile, buffer_size, \n",
    "                 vocab, swap_percent):\n",
    "        super().__init__(sample_name, lookup_name, start_iter, percentile, buffer_size)\n",
    "        self.name = sample_name+'_tokswap'\n",
    "        self.vocab = vocab\n",
    "        self.swap_percent = swap_percent\n",
    "        \n",
    "    def _build_buffer(self, iterations, log):\n",
    "        samples = super()._build_buffer(iterations, log)\n",
    "        \n",
    "        new_samples = []\n",
    "        \n",
    "        for sample in samples:\n",
    "            tokens = self.vocab.tokenize(sample)\n",
    "            num_swaps = int(self.swap_percent*len(tokens))\n",
    "            swap_idxs = np.random.choice(np.arange(len(tokens)), num_swaps, replace=False)\n",
    "            new_tokens = np.random.choice(self.vocab.itos, num_swaps, replace=True)\n",
    "            for idx, new_token in zip(swap_idxs, new_tokens):\n",
    "                tokens[idx] = new_token\n",
    "            \n",
    "            sample = self.vocab.join_tokens(tokens)\n",
    "            sample = self.vocab.postprocess(sample)\n",
    "            new_samples.append(sample)\n",
    "            \n",
    "        return new_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LogEnumerator(LogSampler):\n",
    "    '''\n",
    "    LogEnumerator - pulls high scoring samples \n",
    "    from `Log.df` and performs simple enumeration \n",
    "    by adding one atom or one bond to the sample. \n",
    "    Note that this proccess can create a large number \n",
    "    samples and the value of `buffer_size` should \n",
    "    accordingly be low (3-5). See `add_atom_combi` and \n",
    "    `add_bond_combi` for more details on how the \n",
    "    enumeration is done\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `sample_name str`: what column in `Log.df` to pull \n",
    "    samples from\n",
    "    \n",
    "    - `lookup_name str`: what column in `Log.df` to use \n",
    "    to find high scoring samples\n",
    "    \n",
    "    - `start_iter int`: iteration to start drawing from log\n",
    "    \n",
    "    - `percentile int`: value 1-100 percentile of \n",
    "    log data to sample from\n",
    "    \n",
    "    - `buffer_size int`: number of samples to generate during `build_buffer`\n",
    "    \n",
    "    - `atom_types list`: list of allowed atom types to swap\n",
    "    '''\n",
    "    def __init__(self, sample_name, lookup_name, start_iter, percentile, \n",
    "                 buffer_size, atom_types=None):\n",
    "        super().__init__(sample_name, lookup_name, start_iter, percentile, buffer_size)\n",
    "\n",
    "        self.name = sample_name+'_enum'\n",
    "        if atom_types is None:\n",
    "            atom_types = ['C', 'N', 'O', 'F', 'S', 'Br', 'Cl', -1]\n",
    "            \n",
    "        self.atom_types = atom_types\n",
    "        \n",
    "    def _build_buffer(self, iterations, df):\n",
    "        samples = super()._build_buffer(iterations, df)\n",
    "        \n",
    "        new_samples = []\n",
    "        \n",
    "        for sample in samples:\n",
    "            \n",
    "            new_smiles = add_atom_combi(sample, self.atom_types) + add_bond_combi(sample)\n",
    "            new_smiles = [i for i in new_smiles if i is not None]\n",
    "            new_smiles = [i for i in new_smiles if not '.' in i]\n",
    "            new_samples += new_smiles\n",
    "\n",
    "        return new_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Timeout(Callback):\n",
    "    '''\n",
    "    Timeout - puts samples in \"timeout\" to prevent \n",
    "    training on the same sample too frequently. Samples \n",
    "    are only allowed to be trained on once every \n",
    "    `timeout_length` batches\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `timeout_ength int`: number of batches to put \n",
    "    molecule in timeout\n",
    "    \n",
    "    - `timeout_function Optional[Callable]`: preprocessing \n",
    "    function for samples\n",
    "    \n",
    "    - `track bool`: if metrics from this callback should be tracked\n",
    "    \n",
    "    - `name str`: callback name\n",
    "    '''\n",
    "    def __init__(self, timeout_length, timeout_function=None, \n",
    "                 track=True, name='timeout'):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.timeout_length = timeout_length\n",
    "        self.timeout_dict = {}\n",
    "        self.timeout_function = timeout_function\n",
    "        self.track = track\n",
    "        \n",
    "    def setup(self):\n",
    "        if self.track:\n",
    "            log = self.environment.log\n",
    "            log.add_metric(self.name)            \n",
    "        \n",
    "    def filter_batch(self):\n",
    "        batch_state = self.environment.batch_state\n",
    "        samples = batch_state.samples\n",
    "        \n",
    "        valids = []\n",
    "        \n",
    "        for sample in samples:\n",
    "            if self.timeout_function is not None:\n",
    "                sample = self.timeout_function(sample)\n",
    "            \n",
    "            if sample in self.timeout_dict.keys():\n",
    "                valids.append(False)\n",
    "                \n",
    "            else:\n",
    "                valids.append(True)\n",
    "                self.timeout_dict[sample] = self.timeout_length + 1\n",
    "                \n",
    "        self._filter_batch(valids)\n",
    "        \n",
    "        to_remove = []\n",
    "        for k,v in self.timeout_dict.items():\n",
    "            new_v = v-1\n",
    "            self.timeout_dict[k] = new_v\n",
    "            if new_v <= 0:\n",
    "                to_remove.append(k)\n",
    "                \n",
    "        for key in to_remove:\n",
    "            self.timeout_dict.pop(key)\n",
    "            \n",
    "        pct_valid = np.array(valids).mean()\n",
    "        \n",
    "        if self.track:\n",
    "            log = self.environment.log\n",
    "            log.update_metric(self.name, pct_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class MurckoTimeout(Timeout):\n",
    "    '''\n",
    "    MurckoTimeout - puts samples in \"timeout\" to prevent \n",
    "    training on the same sample too frequently. Samples \n",
    "    are only allowed to be trained on once every \n",
    "    `timeout_length` batches. `MurckoTimeout` \n",
    "    identifies samples by their Murcko scaffold\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `timeout_ength int`: number of batches to put \n",
    "    molecule in timeout\n",
    "    \n",
    "    - `generic bool`: if True, Murcko scaffolds will be \n",
    "    made generic (all carbon, single bonds) before evaluuation\n",
    "\n",
    "    - `track bool`: if metrics from this callback should be tracked\n",
    "    \n",
    "    - `name str`: callback name\n",
    "    '''\n",
    "    def __init__(self, timeout_length, generic=False, track=True, name='murcko_timeout'):\n",
    "        timeout_function = partial(murcko_scaffold, generic=generic)\n",
    "        super().__init__(timeout_length, timeout_function, track, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrl",
   "language": "python",
   "name": "mrl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
