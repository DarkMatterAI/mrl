{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# all_tutorial\n",
    "! [ -e /content ] && pip install -Uqq mrl-pypi  # upgrade mrl on colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial - RL Train Cycle Overview\n",
    "\n",
    ">Overview of the RL training cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Train Cycle Overview\n",
    "\n",
    "The goal of this tutorial is to walk through the RL fit cycle to familiarize ourselves with the `Events` cycle and get a better understanding of how `Callback` and `Environment` classes work.\n",
    "\n",
    "## Performance Notes\n",
    "\n",
    "The workflow in this notebook is more CPU-constrained than GPU-constrained due to the need to evaluate samples on CPU. If you have a multi-core machine, it is recommended that you uncomment and run the `set_global_pool` cells in the notebook. This will trigger the use of multiprocessing, which will result in 2-4x speedups.\n",
    "\n",
    "This notebook may run slow on Collab due to CPU limitations.\n",
    "\n",
    "If running on Collab, remember to change the runtime to GPU\n",
    "\n",
    "## High Level Overview\n",
    "\n",
    "### The Environment\n",
    "\n",
    "At the highest level, we have the `Environment` class. The `Environment` holds together several sub-modules and orchestrates them during the fit loop. The following are contained in the `Environment`:\n",
    "- `agent` - This is the actual model we're training\n",
    "- `template_cb` - this holds a `Template` class that we use to define our chemical space\n",
    "- `samplers` - samplers generate new samples to train on\n",
    "- `buffer` - the buffer collects and distributes samples from all the `samplers`\n",
    "- `rewards` - rewards score samples\n",
    "- `losses` - losses generate values we can backpropagate through \n",
    "- `log` - the log holds a record of all samples in the training process\n",
    "\n",
    "### Callbacks and the Event Cycle\n",
    "\n",
    "Each one of the above items is a `Callback`. A `Callback` is a a general class that can hook into the `Environment` fit cycle at a number of pre-defined `Events`. When the `Environment` calls a specific `Event`, the event name is passed to every callback in the `Environment`. If a given `Callback` has a defined function named after the event, that function is called. This creates a very flexible system for customizing training loops.\n",
    "\n",
    "We'll be looking more at `Events` later. For now, we'll just list them in brief. These are the events called during the RL training cycle in the order they are executed:\n",
    "\n",
    "- `setup` - called when the `Environment` is created, used to set up values\n",
    "- `before_train` - called before training is started\n",
    "- `build_buffer` - draws samples from `samplers` into the `buffer`\n",
    "- `filter_buffer` - filters samples in the buffer\n",
    "- `after_build_buffer` - called after buffer filtering. Used for cleanup, logging, etc\n",
    "- `before_batch` - called before a batch starts, used to set up the `batch state`\n",
    "- `sample_batch` - samples are drawn from `sampers` and `buffer` into the `batch state`\n",
    "- `before_filter_batch` - allows preprocessing of samples before filtering\n",
    "- `filter_batch` - filters samples in `batch state`\n",
    "- `after_sample` - used for calculating sampling metrics\n",
    "- `before_compute_reward` - used to set up any values needed for reward computation \n",
    "- `compute_reward` - used by `rewards` to compute rewards for all samples in the `batch state`\n",
    "- `after_compute_reward` - used for logging reward metrics\n",
    "- `reward_modification` - modify rewards in ways not tracked by the log\n",
    "- `after_reward_modification` - log reward modification metrics\n",
    "- `get_model_outputs` - generate necessary tensors from the model\n",
    "- `after_get_model_outputs` - used for any processing required prior to loss calculation \n",
    "- `compute_loss` - compute loss values\n",
    "- `zero_grad` - zero grad\n",
    "- `before_step` - used for computation before optimizer step (ie gradient clipping)\n",
    "- `step` - step optimizer\n",
    "- `after_batch` - compute batch stats\n",
    "- `after_train` - final event after all training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmai/miniconda3/envs/mrl/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for boost::shared_ptr<RDKit::FilterCatalogEntry const> already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from mrl.imports import *\n",
    "from mrl.core import *\n",
    "from mrl.chem import *\n",
    "from mrl.templates.all import *\n",
    "\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *\n",
    "from mrl.layers import *\n",
    "from mrl.dataloaders import *\n",
    "from mrl.g_models.all import *\n",
    "from mrl.vocab import *\n",
    "from mrl.policy_gradient import *\n",
    "from mrl.train.all import *\n",
    "from mrl.model_zoo import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_global_pool(min(10, os.cpu_count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "We start by creating all the components we need to train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n",
    "\n",
    "The `Agent` is the actual model we want to train. For this example, we will use the `LSTM_LM_Small_ZINC` model, which is a `LSTM_LM` model trained on a chunk of the ZINC database.\n",
    "\n",
    "The agent will actually contain two versions of the model. The main model that we will train with every update iteration, and a baseline model which is updated as an exponentially weighted moving average of the main model. Both models are used in the RL training algorithm we will set up later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = LSTM_LM_Small_ZINC(drop_scale=0.5,opt_kwargs={'lr':5e-5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template\n",
    "\n",
    "The `Template` class is used to conrol the chemical space. We can set parameters on what molecular properties we want to allow. For this example, we set the following:\n",
    "\n",
    "- Hard Filters - must have qualities\n",
    "    - `ValidityFilter` - must be a valid chemical structure\n",
    "    - `SingleCompoundFilter` - samples must be single compounds\n",
    "    - `RotBondFilter` - compounds can have at most 8 rotatable bonds\n",
    "    - `ChargeFilter` - compounds must have no net charge\n",
    "- Soft Filters - nice to have qualities\n",
    "    - `QEDFilter` - Compounds get a score bonus of +1 if their QED value is greater than 0.5\n",
    "    - `SAFilter` - compounds get a score bonus of + if their SA score is less than 5\n",
    "    \n",
    "We then pass the `Template` to the `TemplateCallback` which integrates the template into the fit loop. Note that we pass `prefilter=True` to the `TemplateCallback`, which ensures compounds that don't meet our hard filters are removed from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = Template([ValidityFilter(), \n",
    "                     SingleCompoundFilter(), \n",
    "                     RotBondFilter(None, 8),\n",
    "                     ChargeFilter(0, 0)],\n",
    "                    [QEDFilter(0.5, None, score=1.),\n",
    "                     SAFilter(None, 5, score=1.)])\n",
    "\n",
    "template_cb = TemplateCallback(template, prefilter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward\n",
    "\n",
    "For the reward, we will load a scikit-learn linear regression model. This model was trained to predict affinity against erbB1 using molecular fingerprints as inputs\n",
    "\n",
    "This score function is extremely simple and likely won't translate well to real affinity. It is used as a lightweight example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FP_Regression_Score():\n",
    "    def __init__(self, fname):\n",
    "        self.model = torch.load(fname)\n",
    "        self.fp_function = partial(failsafe_fp, fp_function=ECFP6)\n",
    "        \n",
    "    def __call__(self, samples):\n",
    "        mols = to_mols(samples)\n",
    "        fps = maybe_parallel(self.fp_function, mols)\n",
    "        fps = [fp_to_array(i) for i in fps]\n",
    "        x_vals = np.stack(fps)\n",
    "        preds = self.model.predict(x_vals)\n",
    "        return preds\n",
    "\n",
    "# if in the repo\n",
    "reward_function = FP_Regression_Score('../files/erbB1_regression.sklearn')\n",
    "\n",
    "# if in Collab:\n",
    "# download_files()\n",
    "# reward_function = FP_Regression_Score('files/erbB1_regression.sklearn')\n",
    "\n",
    "reward = Reward(reward_function, weight=1.)\n",
    "\n",
    "aff_reward = RewardCallback(reward, 'aff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of the score function as a black box that takes in samples (SMILES strings) and returns a single numeric score for each sample. Any score function that follows this paradigm can be integrated into MRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.33797993, 6.17020286])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = ['Brc1cc2c(NCc3cccs3)ncnc2s1',\n",
    "           'Brc1cc2c(NCc3ccncc3)ncnc2s1']\n",
    "\n",
    "reward_function(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "For our loss, we will use the `PPO` reinforcement learning algorithm. See the [PPO](arxiv.org/pdf/1707.06347.pdf) paper for full details.\n",
    "\n",
    "The gist of it is the loss function takes a batch of samples and directs he model to increase the probability of above-average samples (relative to the batch mean) and decrease he probability of below-average samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg = PPO(0.99,\n",
    "        0.5,\n",
    "        lam=0.95,\n",
    "        v_coef=0.5,\n",
    "        cliprange=0.3,\n",
    "        v_cliprange=0.3,\n",
    "        ent_coef=0.01,\n",
    "        kl_target=0.03,\n",
    "        kl_horizon=3000,\n",
    "        scale_rewards=True)\n",
    "\n",
    "loss = PolicyLoss(pg, 'PPO', \n",
    "                   value_head=ValueHead(256), \n",
    "                   v_update_iter=2, \n",
    "                   vopt_kwargs={'lr':1e-3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samplers\n",
    "\n",
    "`Samplers` fill the role of generating samples to train on. We will use four samplers for this run:\n",
    "\n",
    "- `sampler1`: `ModelSampler` - this sampler will draw samples from the main model in the `Agent`. We set `buffer_size=1000`, which means we will generate 1000 samples every time we build the buffer. We set `p_batch=0.5`, which means during training, 50% of each batch will be sampled on the fly from the main model and the rest of the batch will come from the buffer\n",
    "- `sampler2`: `ModelSampler` - this sampler is the same as `sampler1`, but we draw from the baseline model instead of the main model. We set `p_batch=0.`, so this sampler will only contribute to the buffer\n",
    "- `sampler3`: `LogSampler` - this sampler looks through the log of previous samples. Based on our input arguments, it grabs the top `95` percentile of samples in the log, and randomly selects `100` samples from that subset\n",
    "- `sampler4`: `DatasetSampler` - this sampler is seeded wih erbB1 training data used to train the score function. This sampler will randomly select 4 samples from the dataset to add to the buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_bs = 1500\n",
    "\n",
    "# if in the repo\n",
    "df = pd.read_csv('../files/erbB1_affinity_data.csv')\n",
    "\n",
    "# if in Collab\n",
    "# download_files()\n",
    "# df = pd.read_csv('files/erbB1_affinity_data.csv')\n",
    "\n",
    "df = df[df.neg_log_ic50>9.2]\n",
    "\n",
    "sampler1 = ModelSampler(agent.vocab, agent.model, 'live', 1000, 0.5, gen_bs)\n",
    "sampler2 = ModelSampler(agent.vocab, agent.base_model, 'base', 1000, 0., gen_bs)\n",
    "sampler3 = LogSampler('samples', 'rewards', 10, 95, 100)\n",
    "sampler4 = DatasetSampler(df.smiles.values, 'erbB1_data', buffer_size=4)\n",
    "\n",
    "samplers = [sampler1, sampler2, sampler3, sampler4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Callbacks\n",
    "\n",
    "We'll add three more callbacks:\n",
    "\n",
    "- `MaxCallback`: this will grab the max reward within a batch that came from the source `live`. `live` is the name we gave to `sampler1` above. This means the max callback will grab all outputs from `sampler1` corresponding to samples from the live model and add the largest to the batch metrics\n",
    "- `PercentileCallback`: this does the same as `MaxCallback` but instead of printing the maximum score, it prints the 90th percentile score\n",
    "- `NoveltyReward`: this is reward modification that gives a bonus score of `0.05` to new samples (ie samples that haven't appeared before in training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live_max = MaxCallback('rewards', 'live')\n",
    "live_p90 = PercentileCallback('rewards', 'live', 90)\n",
    "new_cb = NoveltyReward(weight=0.05)\n",
    "\n",
    "cbs = [new_cb, live_p90, live_max]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Walkthrough\n",
    "\n",
    "Now we will step through the training cycle looking at how each callback event is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "The first event occurs when we create our `Environment` using the callbacks we set up before. Instantiating the `Environment` registers all callbacks and runs the `setup` event. Many callbacks use the `setup` event to add terms to the batch log or the metrics log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(agent, template_cb, samplers=samplers, rewards=[aff_reward], losses=[loss],\n",
    "                 cbs=cbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the environment, we just created a `Buffer` and a `Log`.\n",
    "\n",
    "The `Buffer` holds a list of samples, which is currently empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "buffer"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.buffer.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Log` holds a number of containers for tracking training outputs\n",
    "\n",
    "- `metrics`: dictionary of batch metrics. Each key maps to a list where each value in the list is the metric term for  given batch\n",
    "- `batch_log`: dictionary of batch items. Each key maps to a list. Each element in that list is a list containing the batch values for that key in a given batch\n",
    "- `unique_samples`: dictionary of unique samples and the rewards for those samples. Useful for looking up if a sample has been seen before\n",
    "- `df`: dataframe of unique samples and all associated values stored in the `batch_log`\n",
    "\n",
    "We can see that these log terms have already been populated during the `setup` event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rewards': [],\n",
       " 'rewards_final': [],\n",
       " 'new': [],\n",
       " 'diversity': [],\n",
       " 'bs': [],\n",
       " 'template': [],\n",
       " 'valid': [],\n",
       " 'live_diversity': [],\n",
       " 'live_valid': [],\n",
       " 'live_rewards': [],\n",
       " 'live_new': [],\n",
       " 'aff': [],\n",
       " 'novel': [],\n",
       " 'PPO': [],\n",
       " 'rewards_live_p90': [],\n",
       " 'rewards_live_max': []}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.log.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'samples': [],\n",
       " 'sources': [],\n",
       " 'rewards': [],\n",
       " 'rewards_final': [],\n",
       " 'template': [],\n",
       " 'aff': [],\n",
       " 'novel': [],\n",
       " 'PPO': []}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.log.batch_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samples</th>\n",
       "      <th>sources</th>\n",
       "      <th>rewards</th>\n",
       "      <th>rewards_final</th>\n",
       "      <th>template</th>\n",
       "      <th>aff</th>\n",
       "      <th>novel</th>\n",
       "      <th>PPO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [samples, sources, rewards, rewards_final, template, aff, novel, PPO]\n",
       "Index: []"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.log.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keys in the above dictionaries were added by the associated callbacks. For example, look at the `setup` method in `ModelSampler`, the type of sampler we used for `sampler1`:\n",
    "\n",
    "```\n",
    "    def setup(self):\n",
    "        if self.p_batch>0. and self.track:\n",
    "            log = self.environment.log\n",
    "            log.add_metric(f'{self.name}_diversity')\n",
    "            log.add_metric(f'{self.name}_valid')\n",
    "            log.add_metric(f'{self.name}_rewards')\n",
    "            log.add_metric(f'{self.name}_new')\n",
    "```\n",
    "\n",
    "We gave `sampler1` the name `live`. As a result, the terms `live_diversity`, `live_valid`, `live_rewards` and `live_new` were added to the metrics.\n",
    "\n",
    "We can also look at the `setup` method of our loss function `loss`:\n",
    "\n",
    "```\n",
    "    def setup(self):\n",
    "        if self.track:\n",
    "            log = self.environment.log\n",
    "            log.add_metric(self.name)\n",
    "            log.add_log(self.name)\n",
    "```\n",
    "\n",
    "This is responsible for the `PPO` terms in the `batch_log` and the `metrics`. The PPO metrics term will store the average PPO loss value across a batch, while the PPO batch log term will store the PPO value for each item in a batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Fit Cycle\n",
    "\n",
    "At this point, we could start training using `Environment.fit`. We could call `env.fit(200, 90, 10, 2)` to train for 10 batches with a batch size of 200. For this tutorial, we will step through each part of the fit cycle and observe what is happening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before Train\n",
    "\n",
    "The first stage of the fit cycle is the `before_train` stage. This sets the batch size and sequence length based on the inputs to `Environment.fit` (which we will set manually) and prints the top of the log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.bs = 200 # batch size of 200\n",
    "env.sl = 90 # max sample length of 90 steps\n",
    "mb = master_bar(range(1))\n",
    "env.log.pbar = mb\n",
    "env.report = 1\n",
    "env.log.report = 1 # report stats every batch\n",
    "env('before_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Buffer\n",
    "\n",
    "The next stage of the cycle is the `build_buffer` stage. This consists of the following events:\n",
    "- `build_buffer`: samplers add items to the buffer\n",
    "- `filter_buffer`: the buffer is filtered\n",
    "- `after_build_buffer`: use as needed\n",
    "\n",
    "Going into this stage, our buffer is empty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.buffer.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build_buffer\n",
    "\n",
    "By calling the `build_buffer` event, our samplers will add items to the buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env('build_buffer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 2004 items in the buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2004"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(env.buffer.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `buffer_sources` attribute to see where each item came from. We have 1000 items from `live_buffer` which corresponds to `sampler1`, sampling from the main model.\n",
    "\n",
    "We have 1000 items from `base_buffer` which corresponds to `sampler2`, sampling from the baseline model.\n",
    "\n",
    "We have 4 items from `erbB1_data_buffer`, our dataset sampler (`sampler4`).\n",
    "\n",
    "Our log sampler, `sampler3` was set to start sampling after 10 training iterations, so we don't currently have any samples from that sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'live_buffer': 1000, 'base_buffer': 1000, 'erbB1_data_buffer': 4})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(env.buffer.buffer_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter_buffer\n",
    "\n",
    "It's likely some of these samples don't match our compound requirements defined in the `Template` we used, so we want to filter the buffer for passing compounds. This is what the `filter_buffer` does. For this current example, the only callback doing any buffer filtering is the template callback. However, the `filter_buffer` can be used to implement any form of buffer filtering.\n",
    "\n",
    "Any callback that passes a list of boolean values to `Buffer._filter_buffer` can filter the buffer.\n",
    "\n",
    "After filtering, we have 1829 remaining samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env('filter_buffer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1829"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(env.buffer.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'live_buffer': 922, 'base_buffer': 905, 'erbB1_data_buffer': 2})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(env.buffer.buffer_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### after_build_buffer\n",
    "\n",
    "Next is the `after_build_buffer` event. None of our current callbacks make use of this event, but it exists to allow for evaluation/postprocessing/whatever after buffer creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Batch\n",
    "\n",
    "The next event stage is the `sample_batch` stage. This consists of the following events:\n",
    "\n",
    "- `before_batch`: set up/refresh any required state prior to batch sampling\n",
    "- `sample_batch`: draw one batch of samples\n",
    "- `before_filter_batch`: evaluate unfiltered batch\n",
    "- `filter_batch`: filter batch\n",
    "- `after_sample`: compute sample based metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### before_batch\n",
    "\n",
    "This event is used to create a new `BatchState` for the environment. The batch state is a container designed to hold any values required by the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.batch_state = BatchState()\n",
    "env('before_batch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the batch state only has placeholder values for commonly generated terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'samples': [],\n",
       " 'sources': [],\n",
       " 'rewards': tensor(0., device='cuda:0'),\n",
       " 'loss': tensor(0., device='cuda:0', grad_fn=<CopyBackwards>),\n",
       " 'latent_data': {}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.batch_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample_batch\n",
    "\n",
    "Now we actually draw samples to form a batch. All of our `Sampler` objects have a `p_batch` value, which designated what percentage of the batch should come from that sampler. Batch sampling is designed such that individual sampler `p_batch` values are respected, and any remaining batch percentage comes from the buffer.\n",
    "\n",
    "Only `sampler1` has `p_batch>0.`, with a value of `p_batch=0.5`. This means 50% of the batch will be sampled on he fly from `sampler1`, and the remaining 50% of the batch will come from the buffer.\n",
    "\n",
    "Using a hybrid of live sampling and buffer sampling seems to work best. That said, it is possible to have every batch be 100% buffer samples (like offline RL), or have 100% be live samples (like online RL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env('sample_batch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see we've populated several terms in the batch state. `BatchState.samples` now has a list of samples. `BatchState.sources` has the source of each sample.\n",
    "\n",
    "We also added `BatchState.live_raw` and `BatchState.base_raw`. These terms hold the outputs of `sampler1` and `sampler2`. When we filter `BatchState.samples`, we can refer to the `_raw` terms to see what samples were removed.\n",
    "\n",
    "Note that `BatchState.base_raw` is an empty list since `sampler2.p_batch=0.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['samples', 'sources', 'rewards', 'loss', 'latent_data', 'live_raw', 'base_raw'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.batch_state.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BatchState.sources` holds the source of each sample. We have 100 samples from `live`, which corresponds to our on the fly samples from `sampler1`. The remaining 100 samples come from `live_buffer` and `base_buffer`. This means they came from either `sampler1` (live) or `sampler2` (base) by way of being sampled from the buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'live_buffer': 49, 'base_buffer': 51, 'live': 100})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(env.batch_state['sources'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COc1ccc2c(c1)OC[C@@H]2CC(=O)N(C)CCN1C(=O)c2ccccc2C1=O',\n",
       " 'CC(C)CC[C@@](C)(O)CNC(=O)[C@H]1CC[C@H](C(C)C)CC1',\n",
       " 'CCOCc1nnc(N2CCC(C#N)CC2)n1Cc1coc2c(C)cccc12',\n",
       " 'N#CC1(NC(=O)[C@H]2CC23CCN(CCOc2ccccc2F)CC3)CCC1',\n",
       " 'O=C(N[C@H](C1CCC1)C1CC1)C(=O)N1CCn2c(cnc2C(F)(F)F)C1']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.batch_state['samples'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['live_buffer', 'live_buffer', 'live_buffer', 'base_buffer', 'live_buffer']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.batch_state['sources'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cc1cc(-c2noc(C[C@H](NC(=O)OC(C)(C)C)c3ccccc3)n2)cc(C)c1Br',\n",
       " 'O=C(OC[C@@H]1CCC[C@@H](O)C1)c1cnc2ccccn2c1=O',\n",
       " 'COc1ccc(C[C@@H](C)CNC(=O)N[C@H]2CCc3[nH]ncc3C2)cc1OC',\n",
       " 'CC[C@@H](C)N1C[C@@]2(CC1=O)COCCN(C(=O)Cc1c(C)noc1Cl)C2',\n",
       " 'CSc1nccnc1C(=O)N(CCO)C1CCSCC1']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.batch_state['live_raw'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.batch_state['base_raw']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### before_filter_batch\n",
    "\n",
    "This event is not used by any of our current callbacks. It provides a hook to influence the batch state prior to filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter_batch\n",
    "\n",
    "Now the batch will be filtered by our `Template`, as well as any other callbacks with a `filter_batch` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env('filter_batch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 13 of our 200 samples were removed by filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(env.batch_state['samples'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the values in `BatchState.samples` and `BatchState.live_raw` to see what was filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87, 100)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_samples = env.batch_state['live_raw']\n",
    "filtered_samples = [env.batch_state['samples'][i] for i in range(len(env.batch_state['samples'])) \n",
    "                    if env.batch_state.sources[i]=='live']\n",
    "\n",
    "len(filtered_samples), len(raw_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CC[C@@H](C)N1C[C@@]2(CC1=O)COCCN(C(=O)Cc1c(C)noc1Cl)C2',\n",
       " 'CC[C@@H](Cn1c(CCc2ccccc2)nnc1N(C)Cc1ccc(Cl)s1)N1CCCC1',\n",
       " 'CCO[C@H]1C[C@H](NC(=O)NCCCc2nc3ccccc3[nH]2)C1(CC)CC',\n",
       " 'CCOCCCC(=O)N1C[C@@H](C)[C@H](NC(=O)C[C@H](C)c2cnn(C)c2)C1',\n",
       " 'Cc1nnc(NC(=O)C2(c3cccc(C)c3)CC2)s1',\n",
       " 'CCN(CC)C[C@H](F)C(=O)N(C)C[C@H]1CCN1C(=O)CC1CC1',\n",
       " 'CCN(CC)[C@H](CNC(=O)C(=O)NCCC(=O)O)CC(C)C',\n",
       " 'Cn1cnnc1CN1C[C@@H](O)[C@H](NC(=O)c2ncnc3sccc32)C1',\n",
       " 'CCC[C@@H](C(=O)NC[C@](C)(NC(=O)CC)C1CC1)c1ccccn1',\n",
       " 'CC(C)(C(=O)N1CC[C@]2(C1)CN(CC#N)CCO2)c1ccccc1',\n",
       " 'CCC[C@@H](C)C(=O)N1CCC2(CN(C(=O)[C@H](C)C3CC3)C2)C1',\n",
       " 'CN(C)S(=O)(=O)CCNC(=O)Nc1ccc(OC(F)F)c(Cc2ccccc2)c1',\n",
       " 'CCC(CC)(C(=O)N[C@@H](CO)C[C@@H](O)c1ccccc1)c1ccc(OC)cc1',\n",
       " 'C=C[C@H](CC)CC(=O)NC[C@H]1C[C@@H](NCc2cc(C)no2)C1',\n",
       " 'CC(C)[C@H]1C[C@H](CC(=O)NCc2ccccc2CN2CCC(C)CC2)CCO1',\n",
       " 'O=C(COC(=O)c1cccc(COc2ccccc2)c1)NCCOc1ccc(F)cc1',\n",
       " 'C[C@H](CCCNC[C@@H]1CCCCO1)NC(=O)C(C)(C)C1CCCC1',\n",
       " 'CCCc1nc(CNC[C@@H](C)NC(=O)[C@@H]2C[C@@H]3O[C@H]2[C@H]2C[C@H]23)cs1',\n",
       " 'CC[C@@H](NC(=O)C(=O)N1CCC2(C1)CCOCC2)c1cccc(S(N)(=O)=O)c1',\n",
       " 'CC(C)c1cc(C(=O)N2C[C@@H](C)[C@H](Nc3nc4ccccc4s3)C2)no1',\n",
       " 'CNS(=O)(=O)CCn1c(-c2cnn(C)c2)nnc1N(C)Cc1ccc(N2CCOCC2)cc1',\n",
       " 'COC[C@H](C)N1CC2(C1)CCN(C(=O)[C@@H]1CCCOC1)CC2',\n",
       " 'O=C(CN1CCC(=O)NC1=O)N[C@H]1CCN(CCO)CC1(C)C',\n",
       " 'CCCN(CCC)C(=O)c1cccc(C(=O)O[C@H](c2nc(C)no2)c2ccccc2)c1']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtered compounds\n",
    "[i for i in raw_samples if not i in filtered_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### after_sample\n",
    "\n",
    "The `after_sample` event is used to calculate metrics related to sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env('after_sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that several values have been added to `Environment.log.metrics`\n",
    "\n",
    "- `new`: percent of samples that have not been seen before\n",
    "- `diversity`: number of unique samples relative to the number of total samples\n",
    "- `bs`: true batch size after filtering\n",
    "- `valid`: percent of samples that passed filtering\n",
    "- `live_diversity`: number of unique samples relative to the number of total samples from `sampler1`\n",
    "- `live_valid`: percent of samples that passed filtering from `sampler1`\n",
    "- `live_new`: percent of samples that have not been seen before from `sampler1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rewards': [],\n",
       " 'rewards_final': [],\n",
       " 'new': [1.0],\n",
       " 'diversity': [1.0],\n",
       " 'bs': [187],\n",
       " 'template': [],\n",
       " 'valid': [0.935],\n",
       " 'live_diversity': [1.0],\n",
       " 'live_valid': [0.87],\n",
       " 'live_rewards': [],\n",
       " 'live_new': [1.0],\n",
       " 'aff': [],\n",
       " 'novel': [],\n",
       " 'PPO': [],\n",
       " 'rewards_live_p90': [],\n",
       " 'rewards_live_max': []}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.log.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Reward\n",
    "\n",
    "After we sample a batch, we enter the `compute_reward` stage. This consists of the following events:\n",
    "\n",
    "- `before_compute_reward` - used to set up any values needed for reward computation \n",
    "- `compute_reward` - used by `rewards` to compute rewards for all samples in the `batch state`\n",
    "- `after_compute_reward` - used for logging reward metrics\n",
    "- `reward_modification` - modify rewards in ways not tracked by the log\n",
    "- `after_reward_modification` - log reward modification metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### before_compute_reward\n",
    "\n",
    "This event can be used to set up any values needed for reward computation. Most rewards only need the raw samples as inputs, but rewards can use other inputs if needed. The only requirement for a reward is that it returns a tensor with one value per batch item.\n",
    "\n",
    "By default, the `Agent` class will tensorize the samples present at this step. Our `PPO` loss will also add placeholder values for the terms needed by that function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env('before_compute_reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of new items have populated the batch state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['samples', 'sources', 'rewards', 'loss', 'latent_data', 'live_raw', 'base_raw', 'model_gathered_logprobs', 'base_gathered_logprobs', 'mask', 'trajectory_rewards', 'model_logprobs', 'base_logprobs', 'value_input', 'x', 'y', 'bs', 'lengths', 'sl'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.batch_state.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 23, 28,  ...,  2,  2,  2],\n",
       "        [ 0, 23, 23,  ...,  2,  2,  2],\n",
       "        [ 0, 23, 23,  ...,  2,  2,  2],\n",
       "        ...,\n",
       "        [ 0, 23, 31,  ...,  2,  2,  2],\n",
       "        [ 0, 23, 31,  ...,  2,  2,  2],\n",
       "        [ 0, 23, 27,  ...,  2,  2,  2]], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.batch_state.x # x tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[23, 28, 34,  ...,  2,  2,  2],\n",
       "        [23, 23,  5,  ...,  2,  2,  2],\n",
       "        [23, 23, 28,  ...,  2,  2,  2],\n",
       "        ...,\n",
       "        [23, 31, 23,  ...,  2,  2,  2],\n",
       "        [23, 31, 23,  ...,  2,  2,  2],\n",
       "        [23, 27,  5,  ...,  2,  2,  2]], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.batch_state.y # y tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False],\n",
       "        ...,\n",
       "        [ True,  True,  True,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False],\n",
       "        [ True,  True,  True,  ..., False, False, False]], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.batch_state.mask # padding mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compute_reward\n",
    "\n",
    "This step actually computes rewards. The `BatchState` has a tensor of 0s as a placeholder for reward values. Rewards will compute a numeric score for each item in the batch and add it to `BatchState.rewards`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.batch_state.rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env('compute_reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7.4464,  6.9985,  8.6470,  8.5587,  6.4126,  6.3626,  5.4491,  8.5913,\n",
       "         7.6887,  7.8485,  7.3070,  8.1959,  7.4571,  4.8321,  7.7518,  6.9646,\n",
       "         9.0067,  8.7635,  6.6427,  7.1914,  7.6669,  8.5002,  5.4258,  5.9034,\n",
       "         5.8713,  7.9348,  6.8055,  7.3766,  5.3981,  8.8464, 10.2083,  7.7705,\n",
       "         4.0919,  5.4432,  7.3088,  7.7280,  9.6633,  8.1869,  6.4295,  4.5877,\n",
       "         5.8513,  8.9504,  6.9817,  7.2481,  8.7868,  6.8186,  8.5599,  5.9329,\n",
       "         7.7897,  9.1975,  4.8209,  6.8187,  6.1769,  7.6650,  5.1367,  7.6710,\n",
       "         7.6972,  8.7095,  6.5353, 11.3099,  5.6652, 10.3340,  9.7048,  9.0019,\n",
       "         5.3304,  7.3670,  9.6277,  8.0019,  8.7727,  8.0397,  7.7085,  7.4089,\n",
       "         7.5608,  7.7316,  8.3243,  7.4006,  8.1880,  4.9502,  5.2859,  7.4890,\n",
       "         6.8271,  8.1306,  6.1215,  7.2989,  5.9260,  8.4519,  8.8245,  5.2587,\n",
       "         9.2377,  8.6317,  7.1252,  9.7453,  6.5998,  6.4446,  5.7345,  5.8627,\n",
       "         8.8814,  6.4167,  6.9583,  6.4811,  5.5591,  4.6644,  8.7930,  7.7654,\n",
       "         7.6589,  8.2830,  7.4545,  7.1201,  8.6511,  8.1746,  7.3143,  6.9259,\n",
       "         6.7407,  8.6857,  4.5039,  5.8502,  9.2962,  6.1733,  8.3172,  3.5688,\n",
       "         5.6853,  5.2566,  8.1857,  4.6914, 10.1267,  8.0808,  6.0151,  5.8964,\n",
       "         6.9508,  4.9608,  4.1395,  6.6439,  6.1228,  5.7914,  5.4085,  8.6273,\n",
       "         8.1571,  7.5381,  8.7961,  7.3255,  6.3455,  8.1302,  6.9034,  9.5043,\n",
       "         7.9387,  7.6071,  5.7581, 10.3549,  6.5539,  6.8969,  7.5937,  7.9687,\n",
       "         6.1901,  2.9819,  8.2540,  6.9759,  9.0442,  6.0747,  5.9112,  4.8929,\n",
       "         8.3603,  5.3070,  8.4804,  8.1893,  9.2698,  9.5042,  8.8843,  7.7454,\n",
       "         7.7210,  6.5514,  8.9875,  7.7331,  9.7721,  6.0056,  7.1964,  6.9201,\n",
       "         6.0658,  6.8281,  6.1915,  4.9288,  7.2785,  8.8447,  4.9078,  8.3447,\n",
       "         6.4719,  7.8901,  7.1313], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.batch_state.rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So where did these rewards come from?\n",
    "\n",
    "One reward term comes from our `Template`. We specified soft rewards for compounds with `QED>=0.5` and `SA<=5`. Compounds could score a maximum of 2 from the template.\n",
    "\n",
    "We also have the reward from the erbB1 regression model we set up earlier.\n",
    "\n",
    "The specific rewards from each of these sources are logged in the `BatchState`\n",
    "\n",
    "For the `Template`, we have `BatchState.template` and `BatchState.template_passes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['samples', 'sources', 'rewards', 'loss', 'latent_data', 'live_raw', 'base_raw', 'model_gathered_logprobs', 'base_gathered_logprobs', 'mask', 'trajectory_rewards', 'model_logprobs', 'base_logprobs', 'value_input', 'x', 'y', 'bs', 'lengths', 'sl', 'template', 'template_passes', 'aff'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.batch_state.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Template scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 1., 2., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 1.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.batch_state.template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BatchState.template_passes` shows which samples passed the hard filters. Since we decided to prefilter with our template earlier, all remaining samples are passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.batch_state.template_passes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we have the erbB2 regression scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.4464, 4.9985, 6.6470, 6.5587, 4.4126, 4.3626, 3.4491, 6.5913, 5.6887,\n",
       "        5.8485, 5.3070, 6.1959, 5.4571, 2.8321, 5.7518, 4.9646, 7.0067, 6.7635,\n",
       "        4.6427, 5.1914, 5.6669, 6.5002, 4.4258, 3.9034, 3.8713, 5.9348, 4.8055,\n",
       "        5.3766, 3.3981, 6.8464, 8.2083, 6.7705, 2.0919, 3.4432, 5.3088, 5.7280,\n",
       "        7.6633, 6.1869, 4.4295, 2.5877, 3.8513, 6.9504, 4.9817, 5.2481, 6.7868,\n",
       "        4.8186, 6.5599, 3.9329, 5.7897, 7.1975, 3.8209, 4.8187, 4.1769, 5.6650,\n",
       "        3.1367, 5.6710, 5.6972, 6.7095, 4.5353, 9.3099, 3.6652, 8.3340, 8.7048,\n",
       "        7.0019, 3.3304, 5.3670, 7.6277, 6.0019, 6.7727, 6.0397, 5.7085, 5.4089,\n",
       "        5.5608, 5.7316, 6.3243, 5.4006, 7.1880, 2.9502, 3.2859, 5.4890, 4.8271,\n",
       "        6.1306, 4.1215, 5.2989, 3.9260, 6.4519, 6.8245, 3.2587, 7.2377, 6.6317,\n",
       "        5.1252, 7.7453, 4.5998, 4.4446, 3.7345, 3.8627, 6.8814, 4.4167, 4.9583,\n",
       "        4.4811, 4.5591, 2.6644, 6.7930, 5.7654, 5.6589, 6.2830, 5.4545, 5.1201,\n",
       "        6.6511, 6.1746, 5.3143, 4.9259, 4.7407, 6.6857, 2.5039, 3.8502, 7.2962,\n",
       "        4.1733, 6.3172, 1.5688, 3.6853, 3.2566, 6.1857, 2.6914, 8.1267, 6.0808,\n",
       "        4.0151, 3.8964, 4.9508, 2.9608, 2.1395, 4.6439, 4.1228, 3.7914, 4.4085,\n",
       "        6.6273, 6.1571, 5.5381, 6.7961, 5.3255, 4.3455, 6.1302, 4.9034, 7.5043,\n",
       "        5.9387, 5.6071, 3.7581, 8.3549, 4.5539, 4.8969, 5.5937, 5.9687, 4.1901,\n",
       "        1.9819, 6.2540, 4.9759, 7.0442, 4.0747, 3.9112, 2.8929, 6.3603, 3.3070,\n",
       "        6.4804, 6.1893, 7.2698, 7.5042, 6.8843, 5.7454, 5.7210, 4.5514, 6.9875,\n",
       "        5.7331, 7.7721, 4.0056, 5.1964, 4.9201, 4.0658, 4.8281, 4.1915, 2.9288,\n",
       "        5.2785, 6.8447, 2.9078, 6.3447, 4.4719, 5.8901, 5.1313],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.batch_state.aff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### after_compute_reward\n",
    "\n",
    "This event is used to calculate metrics on the rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env('after_compute_reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rewards': [7.2516804],\n",
       " 'rewards_final': [],\n",
       " 'new': [1.0],\n",
       " 'diversity': [1.0],\n",
       " 'bs': [187],\n",
       " 'template': [1.9572192513368984],\n",
       " 'valid': [0.935],\n",
       " 'live_diversity': [1.0],\n",
       " 'live_valid': [0.87],\n",
       " 'live_rewards': [7.1182923],\n",
       " 'live_new': [1.0],\n",
       " 'aff': [array(5.294461, dtype=float32)],\n",
       " 'novel': [],\n",
       " 'PPO': [],\n",
       " 'rewards_live_p90': [8.845569610595703],\n",
       " 'rewards_live_max': [10.354888]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.log.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reward_modification\n",
    "\n",
    "The reward modification event can be thought of as a second reward that isn't logged. The reason for including this is to allow for transient, \"batch context\" rewards that don't affect logged values.\n",
    "\n",
    "When we set up our callbacks earlier, we had a term\n",
    "\n",
    "`new_cb = NoveltyReward(weight=0.05)`\n",
    "\n",
    "Which would add a bonus score of 0.05 to new, never before seen samples. The point of this callback is to give the model a soft incentive to generate novel samples. \n",
    "\n",
    "We want this score to impact our current batch. However, if we treated it the same as our actual rewards, the samples would be saved into `env.log` with their scores inflated by 0.05. Later, when our `LogSampler` samples from the log, the sampling would be influenced by a score that was only supposed to be given once.\n",
    "\n",
    "Separating out rewards and reward modifications lets us avoid this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env('reward_modification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n",
       "        0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n",
       "        0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n",
       "        0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n",
       "        0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n",
       "        0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n",
       "        0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n",
       "        0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n",
       "        0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n",
       "        0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n",
       "        0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n",
       "        0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n",
       "        0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n",
       "        0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n",
       "        0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n",
       "        0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n",
       "        0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n",
       "        0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n",
       "        0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n",
       "        0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n",
       "        0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.batch_state.novel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### after_reward_modification\n",
    "\n",
    "Similar to `after_compute_reward`, this event can be used to compute stats on reward modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env('after_reward_modification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rewards': [7.2516804],\n",
       " 'rewards_final': [7.30168],\n",
       " 'new': [1.0],\n",
       " 'diversity': [1.0],\n",
       " 'bs': [187],\n",
       " 'template': [1.9572192513368984],\n",
       " 'valid': [0.935],\n",
       " 'live_diversity': [1.0],\n",
       " 'live_valid': [0.87],\n",
       " 'live_rewards': [7.1182923],\n",
       " 'live_new': [1.0],\n",
       " 'aff': [array(5.294461, dtype=float32)],\n",
       " 'novel': [array(0.05, dtype=float32)],\n",
       " 'PPO': [],\n",
       " 'rewards_live_p90': [8.845569610595703],\n",
       " 'rewards_live_max': [10.354888]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.log.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Model Outputs\n",
    "\n",
    "After computing rewards, we move to set up our loss calculation. The `get_model_outputs` stage is based on generating the values that we will be backpropagating through. This stage consists of the following events:\n",
    "\n",
    "- `get_model_outputs` - generate necessary tensors from the model\n",
    "- `after_get_model_outputs` - used for any processing required prior to loss calculation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_model_outputs\n",
    "\n",
    "This is where we generate tensor values used for loss computation.\n",
    "\n",
    "The specifics of what happens here depends on the type of model used. For autoregressive models, this step involves taking the `x` and `y` tensors we generated during the `before_compute_reward` event and doing a forward pass.\n",
    "\n",
    "`x` is a tensor of size `(bs, sl)`. Running `x` through the model will give a set of log probabilities of size `(bs, sl, d_vocab)`. We then use `y` to gather the relevant log probs to get a gathered log prob tensor of size `(bs, sl)`.\n",
    "\n",
    "We generate these values from both the main model and the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env('get_model_outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['samples', 'sources', 'rewards', 'loss', 'latent_data', 'live_raw', 'base_raw', 'model_gathered_logprobs', 'base_gathered_logprobs', 'mask', 'trajectory_rewards', 'model_logprobs', 'base_logprobs', 'value_input', 'x', 'y', 'bs', 'lengths', 'sl', 'template', 'template_passes', 'aff', 'rewards_final', 'novel', 'model_output', 'model_encoded', 'model_latent', 'y_gumbel', 'base_output', 'base_encoded', 'base_latent', 'state_values', 'ref_state_values'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.batch_state.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([187, 74, 47]), torch.Size([187, 74]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.batch_state.model_logprobs.shape, env.batch_state.model_gathered_logprobs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### after_get_model_outputs\n",
    "\n",
    "This event is not used by any of our current callbacks, but can be used for any sort of post-processing needed before loss computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Loss\n",
    "\n",
    "Now we actually compute a loss value and do an optimizer update. See the `PPO` class for a description of the policy gradient algorithm used.\n",
    "\n",
    "Loss computation consists of the following steps:\n",
    "\n",
    "- `compute_loss` - compute loss values\n",
    "- `zero_grad` - zero grad\n",
    "- `before_step` - used for computation before optimizer step (ie gradient clipping)\n",
    "- `step` - step optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compute_loss\n",
    "\n",
    "When we first created our `BatchState`, there was a placehoder value for `loss`. This is the value that will ulimately be backpropagated through. This means we can run any sort of loss configuration, so long as the final values end up in `BatchState.loss`.\n",
    "\n",
    "For example, the `PPO` policy gradient algorithm we are using involved a `ValueHead` that predicts values at every time step. This model is held in the `PolicyLoss` callback that holds the `PPO` class. During the `compute_loss` event, `PPO` computes an additional loss for the value head that is added to `BatchState.loss`.  `PolicyLoss` also holds an optimizer for the `ValueHead` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.batch_state.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env('compute_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4805, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.batch_state.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### zero_grad\n",
    "\n",
    "This is an event to zero gradients of all optimizers in play. We currently have one optimizer in `Agent` for our generative model and one in `PolicyLoss` for the `ValueHead` of our policy gradient algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env('zero_grad')\n",
    "env.batch_state.loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### before_step\n",
    "\n",
    "This is an event before the actual optimizer step. This is used for things like gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env('before_step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step\n",
    "\n",
    "This is the actual optimizer step. This will step both the `Agent` and `PolicyLoss` optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env('step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Batch\n",
    "\n",
    "The `after_batch` stage consists of a single `after_batch` event. This is used for any updates at the end of the batch.\n",
    "\n",
    "In particular, the `Log` will update `Log.df` and the `Agent` will update he baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env('after_batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samples</th>\n",
       "      <th>sources</th>\n",
       "      <th>rewards</th>\n",
       "      <th>rewards_final</th>\n",
       "      <th>template</th>\n",
       "      <th>aff</th>\n",
       "      <th>novel</th>\n",
       "      <th>PPO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COc1ccc2c(c1)OC[C@@H]2CC(=O)N(C)CCN1C(=O)c2ccc...</td>\n",
       "      <td>live_buffer</td>\n",
       "      <td>7.446445</td>\n",
       "      <td>7.496445</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.446445</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.112296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC(C)CC[C@@](C)(O)CNC(=O)[C@H]1CC[C@H](C(C)C)CC1</td>\n",
       "      <td>live_buffer</td>\n",
       "      <td>6.998550</td>\n",
       "      <td>7.048550</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.998550</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.199071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCOCc1nnc(N2CCC(C#N)CC2)n1Cc1coc2c(C)cccc12</td>\n",
       "      <td>live_buffer</td>\n",
       "      <td>8.646967</td>\n",
       "      <td>8.696967</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.646966</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.360880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N#CC1(NC(=O)[C@H]2CC23CCN(CCOc2ccccc2F)CC3)CCC1</td>\n",
       "      <td>base_buffer</td>\n",
       "      <td>8.558744</td>\n",
       "      <td>8.608745</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.558744</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.420598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O=C(N[C@H](C1CCC1)C1CC1)C(=O)N1CCn2c(cnc2C(F)(...</td>\n",
       "      <td>live_buffer</td>\n",
       "      <td>6.412636</td>\n",
       "      <td>6.462636</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.412636</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.700054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>O=C(Nc1cccc(C2CCC2)c1)c1ccc2c(c1)COC2</td>\n",
       "      <td>live</td>\n",
       "      <td>4.907760</td>\n",
       "      <td>4.957760</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.907760</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2.387913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>CC(C)c1ccc([C@H](NC(=O)c2ccccc2Cl)C(C)C)cc1</td>\n",
       "      <td>live</td>\n",
       "      <td>8.344660</td>\n",
       "      <td>8.394660</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.344660</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.408492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>C[C@@H](CNC(=O)Cc1c[nH]c2cnccc12)NCCC(F)(F)F</td>\n",
       "      <td>live</td>\n",
       "      <td>6.471912</td>\n",
       "      <td>6.521913</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.471912</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.576185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>C[C@@](CO)(NC(=O)c1ccc(=O)n(Cc2ccccc2)n1)c1ccc...</td>\n",
       "      <td>live</td>\n",
       "      <td>7.890116</td>\n",
       "      <td>7.940116</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.890116</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.261187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>CN(CCCCNC(=O)[C@@H]1CC1(C)C)Cc1ccccc1</td>\n",
       "      <td>live</td>\n",
       "      <td>7.131267</td>\n",
       "      <td>7.181267</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.131267</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.066176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>187 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               samples      sources   rewards  \\\n",
       "0    COc1ccc2c(c1)OC[C@@H]2CC(=O)N(C)CCN1C(=O)c2ccc...  live_buffer  7.446445   \n",
       "1     CC(C)CC[C@@](C)(O)CNC(=O)[C@H]1CC[C@H](C(C)C)CC1  live_buffer  6.998550   \n",
       "2          CCOCc1nnc(N2CCC(C#N)CC2)n1Cc1coc2c(C)cccc12  live_buffer  8.646967   \n",
       "3      N#CC1(NC(=O)[C@H]2CC23CCN(CCOc2ccccc2F)CC3)CCC1  base_buffer  8.558744   \n",
       "4    O=C(N[C@H](C1CCC1)C1CC1)C(=O)N1CCn2c(cnc2C(F)(...  live_buffer  6.412636   \n",
       "..                                                 ...          ...       ...   \n",
       "182              O=C(Nc1cccc(C2CCC2)c1)c1ccc2c(c1)COC2         live  4.907760   \n",
       "183        CC(C)c1ccc([C@H](NC(=O)c2ccccc2Cl)C(C)C)cc1         live  8.344660   \n",
       "184       C[C@@H](CNC(=O)Cc1c[nH]c2cnccc12)NCCC(F)(F)F         live  6.471912   \n",
       "185  C[C@@](CO)(NC(=O)c1ccc(=O)n(Cc2ccccc2)n1)c1ccc...         live  7.890116   \n",
       "186              CN(CCCCNC(=O)[C@@H]1CC1(C)C)Cc1ccccc1         live  7.131267   \n",
       "\n",
       "     rewards_final  template       aff  novel       PPO  \n",
       "0         7.496445       2.0  5.446445   0.05 -0.112296  \n",
       "1         7.048550       2.0  4.998550   0.05  0.199071  \n",
       "2         8.696967       2.0  6.646966   0.05 -0.360880  \n",
       "3         8.608745       2.0  6.558744   0.05 -0.420598  \n",
       "4         6.462636       2.0  4.412636   0.05  0.700054  \n",
       "..             ...       ...       ...    ...       ...  \n",
       "182       4.957760       2.0  2.907760   0.05  2.387913  \n",
       "183       8.394660       2.0  6.344660   0.05 -0.408492  \n",
       "184       6.521913       2.0  4.471912   0.05  0.576185  \n",
       "185       7.940116       2.0  5.890116   0.05 -0.261187  \n",
       "186       7.181267       2.0  5.131267   0.05  0.066176  \n",
       "\n",
       "[187 rows x 8 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.log.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Train\n",
    "\n",
    "The `after_train` event can be used to calculate any final statistics or other values as desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env('after_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "Hopefully walking through the training process step by step has made he process more understandable. We conclude by simply running `Environment.fit` so we don't have to go through things step by step anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>iterations</th>\n",
       "      <th>rewards</th>\n",
       "      <th>rewards_final</th>\n",
       "      <th>new</th>\n",
       "      <th>diversity</th>\n",
       "      <th>bs</th>\n",
       "      <th>template</th>\n",
       "      <th>valid</th>\n",
       "      <th>live_diversity</th>\n",
       "      <th>live_valid</th>\n",
       "      <th>live_rewards</th>\n",
       "      <th>live_new</th>\n",
       "      <th>aff</th>\n",
       "      <th>novel</th>\n",
       "      <th>PPO</th>\n",
       "      <th>rewards_live_p90</th>\n",
       "      <th>rewards_live_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.312</td>\n",
       "      <td>7.362</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>194</td>\n",
       "      <td>1.964</td>\n",
       "      <td>0.970</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.940</td>\n",
       "      <td>7.095</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5.348</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.638</td>\n",
       "      <td>9.296</td>\n",
       "      <td>11.366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>7.305</td>\n",
       "      <td>7.355</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>189</td>\n",
       "      <td>1.942</td>\n",
       "      <td>0.945</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.890</td>\n",
       "      <td>7.512</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5.363</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.504</td>\n",
       "      <td>9.157</td>\n",
       "      <td>11.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>7.270</td>\n",
       "      <td>7.320</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>189</td>\n",
       "      <td>1.968</td>\n",
       "      <td>0.945</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.890</td>\n",
       "      <td>7.160</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5.302</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.485</td>\n",
       "      <td>9.258</td>\n",
       "      <td>11.448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>7.299</td>\n",
       "      <td>7.349</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>191</td>\n",
       "      <td>1.974</td>\n",
       "      <td>0.955</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.910</td>\n",
       "      <td>7.226</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5.325</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.545</td>\n",
       "      <td>9.175</td>\n",
       "      <td>11.390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>7.307</td>\n",
       "      <td>7.356</td>\n",
       "      <td>0.984</td>\n",
       "      <td>1.000</td>\n",
       "      <td>193</td>\n",
       "      <td>1.969</td>\n",
       "      <td>0.965</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.930</td>\n",
       "      <td>7.433</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5.338</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.470</td>\n",
       "      <td>8.971</td>\n",
       "      <td>10.498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>7.382</td>\n",
       "      <td>7.431</td>\n",
       "      <td>0.974</td>\n",
       "      <td>1.000</td>\n",
       "      <td>195</td>\n",
       "      <td>1.979</td>\n",
       "      <td>0.975</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.950</td>\n",
       "      <td>7.321</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5.403</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.569</td>\n",
       "      <td>9.201</td>\n",
       "      <td>11.858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>7.403</td>\n",
       "      <td>7.452</td>\n",
       "      <td>0.964</td>\n",
       "      <td>1.000</td>\n",
       "      <td>194</td>\n",
       "      <td>1.964</td>\n",
       "      <td>0.970</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.940</td>\n",
       "      <td>7.190</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5.440</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.507</td>\n",
       "      <td>8.878</td>\n",
       "      <td>11.578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>7.385</td>\n",
       "      <td>7.433</td>\n",
       "      <td>0.969</td>\n",
       "      <td>1.000</td>\n",
       "      <td>191</td>\n",
       "      <td>1.963</td>\n",
       "      <td>0.955</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.910</td>\n",
       "      <td>7.334</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5.421</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.488</td>\n",
       "      <td>8.888</td>\n",
       "      <td>11.747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>7.294</td>\n",
       "      <td>7.343</td>\n",
       "      <td>0.974</td>\n",
       "      <td>1.000</td>\n",
       "      <td>191</td>\n",
       "      <td>1.979</td>\n",
       "      <td>0.955</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.910</td>\n",
       "      <td>7.212</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5.315</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.571</td>\n",
       "      <td>8.949</td>\n",
       "      <td>11.737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>7.057</td>\n",
       "      <td>7.106</td>\n",
       "      <td>0.984</td>\n",
       "      <td>1.000</td>\n",
       "      <td>189</td>\n",
       "      <td>1.968</td>\n",
       "      <td>0.945</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.890</td>\n",
       "      <td>6.976</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5.089</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.445</td>\n",
       "      <td>8.795</td>\n",
       "      <td>9.695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>7.448</td>\n",
       "      <td>7.497</td>\n",
       "      <td>0.985</td>\n",
       "      <td>1.000</td>\n",
       "      <td>196</td>\n",
       "      <td>1.959</td>\n",
       "      <td>0.980</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.960</td>\n",
       "      <td>7.661</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5.489</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.619</td>\n",
       "      <td>9.440</td>\n",
       "      <td>12.977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>7.430</td>\n",
       "      <td>7.479</td>\n",
       "      <td>0.979</td>\n",
       "      <td>1.000</td>\n",
       "      <td>188</td>\n",
       "      <td>1.968</td>\n",
       "      <td>0.940</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.880</td>\n",
       "      <td>7.287</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5.462</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.371</td>\n",
       "      <td>9.150</td>\n",
       "      <td>10.677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.fit(200, 90, 50, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
