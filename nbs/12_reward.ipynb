{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c92780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a774717",
   "metadata": {},
   "source": [
    "# Reward\n",
    "\n",
    "> Reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0482fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d9b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/envs/mrl/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for boost::shared_ptr<RDKit::FilterCatalogEntry const> already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "from mrl.imports import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *\n",
    "from mrl.chem import *\n",
    "from mrl.templates import *\n",
    "from mrl.agent import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b646ed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Reward():\n",
    "    def __init__(self, template=None, reward_modules=[], trajectory_modules=[]):\n",
    "        \n",
    "        if template == None:\n",
    "            template = Template([])\n",
    "            \n",
    "        self.template = template\n",
    "        self.reward_modules = reward_modules\n",
    "        self.trajectory_modules = trajectory_modules\n",
    "        self.mean_reward = None\n",
    "        \n",
    "    def __call__(self, model_output):\n",
    "        \n",
    "        template_passes = np.array(np.array(self.template(model_output['sequences'])))\n",
    "        template_rewards = np.array(self.template.eval_mols(model_output['sequences']))\n",
    "        \n",
    "        rewards = self.compute_rewards(model_output, template_passes)\n",
    "        trajectory_rewards = self.compute_trajectory_reward(model_output, template_passes)\n",
    "        \n",
    "        rewards = template_rewards + rewards\n",
    "        \n",
    "        if self.mean_reward is None:\n",
    "            self.mean_reward = rewards.mean()\n",
    "        else:\n",
    "            self.mean_reward = (1-reward_decay)*rewards.mean() + reward_decay*self.mean_reward\n",
    "            \n",
    "        rewards_scaled = rewards - self.mean_rewards\n",
    "        \n",
    "        model_output['rewards'] = rewards\n",
    "        model_output['rewards_scaled'] = rewards_scaled\n",
    "        model_output['trajectory_rewards'] = trajectory_rewards\n",
    "        \n",
    "        return model_output\n",
    "    \n",
    "    def compute_trajectory_reward(self, model_output, template_passes):\n",
    "        \n",
    "        all_rewards = []\n",
    "        \n",
    "        for rm in self.trajectory_modules:\n",
    "            all_rewards.append(rm(model_output, template_passes))\n",
    "            \n",
    "        all_rewards = np.stack(all_rewards, -1)\n",
    "        all_rewards = all_rewards.sum(-1)\n",
    "        return all_rewards\n",
    "    \n",
    "    def compute_rewards(self, model_output, template_passes):\n",
    "        \n",
    "        all_rewards = []\n",
    "        \n",
    "        for rm in self.reward_modules:\n",
    "            all_rewards.append(rm(model_output, template_passes))\n",
    "            \n",
    "        all_rewards = np.stack(all_rewards, -1)\n",
    "        all_rewards = all_rewards.sum(-1)\n",
    "        return all_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072a5fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def trajectory_wrapper(inputs, function):\n",
    "    return np.array([function(i) for i in inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad5f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class RewardModule():\n",
    "    \n",
    "    def __call__(self, model_output, template_passes=None):\n",
    "        \n",
    "        reward_inputs = self.prepare_reward_inputs(model_output, template_passes)\n",
    "        reward_outputs = self.reward_function(reward_inputs)\n",
    "        final_reward = self.aggregate_reward(reward_outputs, model_output, template_passes)\n",
    "        return final_reward\n",
    "        \n",
    "    def aggregate_reward(self, reward_outputs, model_output, template_passes=None):\n",
    "        pass\n",
    "        \n",
    "    def prepare_reward_inputs(self, model_output, template_passes=None):\n",
    "        pass\n",
    "    \n",
    "    def reward_function(self, inputs):\n",
    "        pass\n",
    "    \n",
    "class MolReward(RewardModule):\n",
    "    def __init__(self, mol_function, trajectory=False):\n",
    "        self.mol_function = mol_function\n",
    "        self.trajectory = trajectory\n",
    "        \n",
    "    def aggregate_reward(self, reward_outputs, model_output, template_passes=None):\n",
    "        \n",
    "        if template_passes is not None:\n",
    "            passed_idxs = np.array([i for i in range(len(template_passes)) if template_passes[i]])\n",
    "            bs = len(template_passes)\n",
    "        else:\n",
    "            passed_idxs = np.arange(len(reward_outputs))\n",
    "            bs = len(reward_outputs)\n",
    "        \n",
    "        if self.trajectory:\n",
    "            outputs = np.zeros((bs, model_output['sl']))\n",
    "            \n",
    "            for i, idx in enumerate(passed_idxs):\n",
    "                traj = reward_outputs[i]\n",
    "                traj_len = len(traj)\n",
    "                outputs[idx, :traj_len] = traj\n",
    "                \n",
    "        else:\n",
    "            outputs = np.zeros((bs))\n",
    "            outputs[passed_idxs] = reward_outputs\n",
    "            \n",
    "        return outputs\n",
    "        \n",
    "    def prepare_reward_inputs(self, model_output, template_passes=None):\n",
    "        \n",
    "        if self.trajectory:\n",
    "            inputs = model_output['sequence_trajectories']\n",
    "        else:\n",
    "            inputs = model_output['sequences']\n",
    "            \n",
    "        output = np.zeros((len(inputs)))\n",
    "        \n",
    "        if template_passes is not None:\n",
    "            inputs = [inputs[i] for i in range(len(inputs)) if template_passes[i]]\n",
    "            \n",
    "        return inputs\n",
    "    \n",
    "    def reward_function(self, inputs):\n",
    "        if self.trajectory:\n",
    "            func = partial(trajectory_wrapper, function=self.mol_function)\n",
    "        else:\n",
    "            func = self.mol_function\n",
    "            \n",
    "        return maybe_parallel(func, inputs)\n",
    "    \n",
    "class MLReward():\n",
    "    def __init__(self, model, trajectory=False):\n",
    "        self.model = model\n",
    "        self.trajectory = trajectory\n",
    "        \n",
    "    def reward_function(self, inputs):\n",
    "        if not type(inputs)==list:\n",
    "            inputs = [inputs]\n",
    "        return np.array(self.model(*inputs).detach().cpu())\n",
    "    \n",
    "    def prepare_reward_inputs(self, model_output, template_passes=None):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def aggregate_reward(self, reward_outputs, model_output, template_passes=None):\n",
    "        if template_passes is not None:\n",
    "            passed_idxs = np.array([i for i in range(len(template_passes)) if template_passes[i]])\n",
    "            bs = len(template_passes)\n",
    "        else:\n",
    "            passed_idxs = np.arange(len(reward_outputs))\n",
    "            bs = len(reward_outputs)\n",
    "        \n",
    "        if reward_outputs.ndim==2:\n",
    "            output = np.zeros((bs, reward_outputs.shape[-1]))\n",
    "            output[passed_idxs] = reward_output\n",
    "            \n",
    "        else:\n",
    "            output = np.zeros((bs,))\n",
    "            output[passed_idxs] = reward_outputs\n",
    "            \n",
    "        return output\n",
    "    \n",
    "class FPModelReward(MLReward):\n",
    "    def __init__(self, model, fp_func, trajectory=False):\n",
    "        super().__init__(model, trajectory)\n",
    "        self.fp_func = fp_func\n",
    "        \n",
    "    def prepare_reward_inputs(self, model_output, template_passes=None):\n",
    "        \n",
    "        smiles = model_output['sequences']\n",
    "        fps = np.stack(maybe_parallel(self.fp_func, smiles))\n",
    "        fps = to_device(torch.from_numpy(fps).float())\n",
    "        return fps\n",
    "    \n",
    "class SequenceModelReward(MLReward):\n",
    "\n",
    "    def prepare_reward_inputs(self, model_output, template_passes=None):\n",
    "        \n",
    "        return model_output['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae01c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8ab2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mf(smile):\n",
    "    mol = to_mol(smile)\n",
    "    if mol is None:\n",
    "        output = 0.\n",
    "    else:\n",
    "        output = qed(mol)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6477b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = MolReward(mf, trajectory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fdd0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mo = ModelOutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71afe759",
   "metadata": {},
   "outputs": [],
   "source": [
    "mo['sequences'] = ['C', 'CCC', 'CCCC']\n",
    "mo['sequence_trajectories'] = [['C'], ['C', 'CC', 'CCC'], ['C', 'CC', 'CCC', 'CCCC']]\n",
    "mo['sl'] = 4\n",
    "template_passes = np.array([True, False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9457afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.35978494, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.35978494, 0.37278556, 0.38547066, 0.43102436]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r(mo, template_passes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b9ff90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc59fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['C'], ['C', 'CC', 'CCC'], ['C', 'CC', 'CCC', 'CCCC']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.prepare_reward_inputs(mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc4ea59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r.reward_function(r.prepare_reward_inputs(mo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1ff0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[5]]).ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159f9ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([['C'], ['C', 'CC', 'CCC'], ['C', 'CC', 'CCC', 'CCCC']], dtype=object).ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d668bc01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9288b920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c6eed87",
   "metadata": {},
   "source": [
    "parallel reward\n",
    "    parallel process calculation on one sequence at a time\n",
    "    \n",
    "batch reward\n",
    "    parallel featurize\n",
    "    batch\n",
    "    compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f21943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
