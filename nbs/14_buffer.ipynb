{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "! [ -e /content ] && pip install -Uqq mrl-pypi  # upgrade mrl on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp train.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buffer\n",
    "\n",
    "> Callbacks for buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from mrl.imports import *\n",
    "from mrl.core import *\n",
    "from mrl.train.callback import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buffer\n",
    "\n",
    "The `Buffer` class holds samples generated during the `BuildBuffer` event. Samples added to the buffer can be any python object that is hashable.\n",
    "\n",
    "Sample hashing is used to determine unique samples. For this reason, samples should avoid using containers like pytorch Tensors which are hashed on the tensor object level rather than the numeric level.\n",
    "\n",
    "```\n",
    "set([torch.tensor(0.), torch.tensor(0.)])\n",
    ">> {tensor(0.), tensor(0.)}\n",
    "\n",
    "set([0., 0.])\n",
    ">> {0.0}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Buffer(Callback):\n",
    "    '''\n",
    "    Buffer - training buffer\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `p_total float`: batch percentage for `sample_batch`\n",
    "    '''\n",
    "    def __init__(self, p_total):\n",
    "        super().__init__(name='buffer', order=0)\n",
    "        \n",
    "        self.buffer = []\n",
    "        self.buffer_sources = []\n",
    "        self.p_total = p_total\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def add(self, item, name=''):\n",
    "        \n",
    "        if type(item)==list:\n",
    "            for i in item:\n",
    "                self.add(i, name=name)\n",
    "        else:\n",
    "            self.buffer.append(item)\n",
    "            self.buffer_sources.append(name+'_buffer')\n",
    "            \n",
    "    def sample(self, n):\n",
    "        \n",
    "        idxs = np.random.choice(np.arange(len(self.buffer)), min(n, len(self.buffer)), \n",
    "                                replace=False)\n",
    "        batch = [self.buffer[i] for i in idxs]\n",
    "        sources = [self.buffer_sources[i] for i in idxs]\n",
    "        for idx in sorted(idxs, reverse=True):\n",
    "            self.buffer.pop(idx)\n",
    "            self.buffer_sources.pop(idx)\n",
    "\n",
    "        return batch, sources\n",
    "    \n",
    "    def _filter_buffer(self, valids):\n",
    "        \n",
    "        self.buffer = [self.buffer[i] for i in range(len(self.buffer)) if valids[i]]\n",
    "        self.buffer_sources = [self.buffer_sources[i] \n",
    "                               for i in range(len(self.buffer_sources)) if valids[i]]\n",
    "        \n",
    "    def filter_buffer(self):\n",
    "        if self.buffer:\n",
    "            seen = set()\n",
    "            unique = []\n",
    "            for item in self.buffer:\n",
    "                if item in seen:\n",
    "                    unique.append(False)\n",
    "                else:\n",
    "                    seen.add(item)\n",
    "                    unique.append(True)\n",
    "                    \n",
    "            self._filter_buffer(np.array(unique))\n",
    "    \n",
    "    def sample_batch(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        \n",
    "        bs = int(env.bs * self.p_total)\n",
    "        if bs>0:\n",
    "            sample, sources = self.sample(bs)\n",
    "            batch_state.samples += sample\n",
    "            batch_state.sources += sources\n",
    "        elif bs==-1:\n",
    "            sample = self.buffer\n",
    "            sources = self.buffer_sources\n",
    "            batch_state.samples += sample\n",
    "            batch_state.sources += sources\n",
    "            self.buffer = []\n",
    "            self.buffer_sources = []\n",
    "            \n",
    "    def filter_batch(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        samples = batch_state.samples\n",
    "        \n",
    "        unique_samples = set()\n",
    "        unique = []\n",
    "        \n",
    "        for sample in samples:\n",
    "            if sample in unique_samples:\n",
    "                unique.append(False)\n",
    "            else:\n",
    "                unique_samples.add(sample)\n",
    "                unique.append(True)\n",
    "                \n",
    "        unique = np.array(unique)\n",
    "        self._filter_batch(unique)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class WeightedBuffer(Buffer):\n",
    "    '''\n",
    "    WeightedBuffer - base class for buffer with \n",
    "    weighted sampling\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `p_total float`: batch percentage for `sample_batch`\n",
    "    \n",
    "    - `refresh_predictions int`: how often to generate \n",
    "    new prdictions for all items in the buffer\n",
    "    \n",
    "    - `pct_argmax float[0., 1.]`: percent of samples to draw \n",
    "    with argmax over the calculated weight versus weighted \n",
    "    random sampling\n",
    "    '''\n",
    "    def __init__(self, p_total, refresh_predictions, pct_argmax=0.):\n",
    "        super().__init__(p_total)\n",
    "        \n",
    "        self.weights = []\n",
    "        self.refresh_predictions = refresh_predictions\n",
    "        self.pct_argmax = pct_argmax\n",
    "        self.name = 'weighted_buffer'\n",
    "        \n",
    "    def add(self, item, name=''):\n",
    "        \n",
    "        if type(item)==list:\n",
    "            for i in item:\n",
    "                self.add(i, name=name)\n",
    "        else:\n",
    "            self.buffer.append(item)\n",
    "            self.buffer_sources.append(name+'_buffer')\n",
    "            self.weights.append(None)\n",
    "\n",
    "    def compute_weights(self, samples):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def _filter_buffer(self, valids):\n",
    "\n",
    "        self.buffer = [self.buffer[i] for i in range(len(self.buffer)) if valids[i]]\n",
    "        self.buffer_sources = [self.buffer_sources[i]\n",
    "                               for i in range(len(self.buffer_sources)) if valids[i]]\n",
    "        self.weights = [self.weights[i] for i in range(len(self.weights)) if valids[i]]\n",
    "        \n",
    "    def sample(self, n):\n",
    "        weights = np.array(self.weights)\n",
    "        \n",
    "        all_idxs = np.arange(len(self.buffer))\n",
    "        idxs = []\n",
    "        \n",
    "        if self.pct_argmax>0.:\n",
    "            n_argmax = int(n*self.pct_argmax)\n",
    "            idxs_sorted = np.argsort(weights)\n",
    "            argmax_idxs = idxs_sorted[-n_argmax:]\n",
    "            idxs += list(argmax_idxs)\n",
    "            n = n - n_argmax\n",
    "            all_idxs = idxs_sorted[:-n_argmax]\n",
    "            weights = weights[all_idxs]\n",
    "            \n",
    "        if weights.shape[0]>0:\n",
    "            weights = weights - weights.min()\n",
    "            weights = weights / weights.sum()\n",
    "\n",
    "            sampled_idxs = np.random.choice(all_idxs, min(n, len(all_idxs)), \n",
    "                                            replace=False, p=weights)\n",
    "            idxs += list(sampled_idxs)\n",
    "        \n",
    "        batch = [self.buffer[i] for i in idxs]\n",
    "        sources = [self.buffer_sources[i] for i in idxs]\n",
    "        weights = [self.weights[i] for i in idxs]\n",
    "        for idx in sorted(idxs, reverse=True):\n",
    "            self.buffer.pop(idx)\n",
    "            self.buffer_sources.pop(idx)\n",
    "            self.weights.pop(idx)\n",
    "\n",
    "        return batch, sources, weights\n",
    "    \n",
    "    def before_batch(self):\n",
    "        \n",
    "        weights = np.array(self.weights)\n",
    "\n",
    "        idxs = np.arange(weights.shape[0])\n",
    "        to_score = weights==None\n",
    "        to_score_idxs = idxs[to_score]\n",
    "        to_score_samples = [self.buffer[i] for i in to_score_idxs]\n",
    "\n",
    "        if to_score_samples:\n",
    "            scored_weights = self.compute_weights(to_score_samples)\n",
    "            weights[to_score_idxs] = scored_weights\n",
    "        \n",
    "        weights = list(weights)\n",
    "        self.weights = weights\n",
    "            \n",
    "    def sample_batch(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        \n",
    "        bs = int(env.bs * self.p_total)\n",
    "        if bs>0:\n",
    "            sample, sources, weights = self.sample(bs)\n",
    "            batch_state.samples += sample\n",
    "            batch_state.sources += sources\n",
    "            batch_state.buffer_weights = weights\n",
    "        elif bs==-1:\n",
    "            sample = self.buffer\n",
    "            sources = self.buffer_sources\n",
    "            weights = self.weights\n",
    "            batch_state.samples += sample\n",
    "            batch_state.sources += sources\n",
    "            batch_state.buffer_weights = weights\n",
    "            self.buffer = []\n",
    "            self.buffer_sources = []\n",
    "            self.weights = []\n",
    "            \n",
    "    def after_batch(self):\n",
    "        env = self.environment\n",
    "        iterations = env.log.iterations\n",
    "        \n",
    "        if iterations>0 and iterations%self.refresh_predictions==0:\n",
    "            weights = self.compute_weights(self.buffer)\n",
    "            \n",
    "            self.weights = list(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class PredictiveBuffer(WeightedBuffer):\n",
    "    '''\n",
    "    PredictiveBuffer - buffer with active learning \n",
    "    score prediction\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `p_total float`: batch percentage for `sample_batch`\n",
    "    \n",
    "    - `refresh_predictions int`: how often to generate \n",
    "    new prdictions for all items in the buffer\n",
    "    \n",
    "    - `predictive_agent PredictiveAgent`: active learning \n",
    "    agent to train\n",
    "    \n",
    "    - `pred_bs int`: prediction batch size for `predictive_agent`\n",
    "    \n",
    "    - `supervised_frequency int`: how often to run \n",
    "    offline supervised training of the predictive agent\n",
    "    \n",
    "    - `supervised_epochs int`: how many epochs to run \n",
    "    during offline supervised training\n",
    "    \n",
    "    - `supervised_bs int`: batch size for offline \n",
    "    supervised training\n",
    "    \n",
    "    - `supervised_lr float`: learning rate for \n",
    "    offline supervised training\n",
    "    \n",
    "    - `train_silent bool`: if True, offline supervised training\n",
    "    results are printed\n",
    "    \n",
    "    - `pct_argmax float[0., 1.]`: percent of samples to draw \n",
    "    with argmax over the calculated weight versus weighted \n",
    "    random sampling\n",
    "    \n",
    "    - `track bool`: if True, predictive buffer metrics \n",
    "    are added to the environment printout\n",
    "    '''\n",
    "    def __init__(self, p_total, refresh_predictions, predictive_agent, pred_bs, \n",
    "                 supervised_frequency, supervised_epochs, \n",
    "                 supervised_bs, supervised_lr, train_silent=True,\n",
    "                 pct_argmax=0., track=True):\n",
    "        super().__init__(p_total=p_total, \n",
    "                         refresh_predictions=refresh_predictions,\n",
    "                         pct_argmax=pct_argmax)\n",
    "        \n",
    "        self.predictive_agent = predictive_agent\n",
    "        unfreeze(self.predictive_agent.model)\n",
    "        self.pred_bs = pred_bs\n",
    "        \n",
    "        self.supervised_frequency = supervised_frequency\n",
    "        self.supervised_epochs = supervised_epochs\n",
    "        self.supervised_bs = supervised_bs\n",
    "        self.supervised_lr = supervised_lr\n",
    "        self.train_silent = train_silent\n",
    "        \n",
    "        self.track = track\n",
    "        self.name = 'predictive_buffer'\n",
    "        \n",
    "    def setup(self):\n",
    "        if self.track:\n",
    "            log = self.environment.log\n",
    "            log.add_metric(self.name+'_loss')\n",
    "            log.add_metric(self.name+'_preds')\n",
    "            log.add_log(self.name+'_preds')\n",
    "        \n",
    "    def compute_weights(self, samples):\n",
    "        with torch.no_grad():\n",
    "            if len(samples) < self.pred_bs:\n",
    "                scores = self.predictive_agent.predict_data(samples).squeeze()\n",
    "                scores = scores.detach().cpu().numpy()\n",
    "            else:\n",
    "                scores = self.predictive_agent.predict_data_batch(samples, self.pred_bs).squeeze()\n",
    "                scores = scores.detach().cpu().numpy()\n",
    "        return scores\n",
    "    \n",
    "    def get_model_outputs(self):\n",
    "        env = self.environment\n",
    "        samples = env.batch_state.samples\n",
    "        preds = self.predictive_agent.predict_data(samples).squeeze()\n",
    "        env.batch_state[self.name+'_preds'] = preds\n",
    "        \n",
    "        if self.track:\n",
    "            env.log.update_metric(self.name+'_preds', preds.mean().detach().cpu().numpy())\n",
    "    \n",
    "    def compute_loss(self):\n",
    "        env = self.environment\n",
    "        rewards = env.batch_state.rewards\n",
    "        preds = env.batch_state[self.name+'_preds']\n",
    "        loss = self.predictive_agent.loss_function(preds, rewards)\n",
    "        \n",
    "        if self.track:\n",
    "            env.log.update_metric(self.name+'_loss', loss.mean().detach().cpu().numpy())\n",
    "            \n",
    "        env.batch_state.loss += loss.mean()\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.predictive_agent.zero_grad()\n",
    "        \n",
    "    def before_step(self):\n",
    "        self.predictive_agent.before_step()\n",
    "        \n",
    "    def step(self):\n",
    "        self.predictive_agent.step()\n",
    "        \n",
    "    def after_batch(self):\n",
    "        env = self.environment\n",
    "        iterations = self.environment.log.iterations\n",
    "        \n",
    "        if iterations>0 and iterations%self.supervised_frequency==0 and self.supervised_frequency>0:\n",
    "            self.train_model()\n",
    "                    \n",
    "        if iterations>0 and iterations%self.refresh_predictions==0:\n",
    "            weights = self.compute_weights(self.buffer)\n",
    "            \n",
    "            self.weights = list(weights)\n",
    "        \n",
    "    def train_model(self):\n",
    "        env = self.environment\n",
    "        df = env.log.df[['samples', 'rewards']]\n",
    "        self.predictive_agent.update_dataset_from_inputs(df.samples.values, df.rewards.values)\n",
    "        self.predictive_agent.train_supervised(self.supervised_bs, self.supervised_epochs,\n",
    "                                              self.supervised_lr, silent=self.train_silent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class BufferSizeCallback(Callback):\n",
    "    '''\n",
    "    BufferSizeCallback - print out \n",
    "    current buffer size during training\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__(name='buffer size')\n",
    "\n",
    "    def setup(self):\n",
    "        log = self.environment.log\n",
    "        log.add_metric(self.name)\n",
    "\n",
    "    def after_sample(self):\n",
    "        env = self.environment\n",
    "        buffer_size = len(env.buffer)\n",
    "        self.environment.log.update_metric(self.name, buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
