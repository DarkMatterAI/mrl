{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7755907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp g_models.lstm_lm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b908af",
   "metadata": {},
   "source": [
    "# LSTM Language Model\n",
    "\n",
    "> LSTM-based language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9dc269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78e2e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from mrl.imports import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *\n",
    "from mrl.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32221c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LSTM_LM(nn.Module):\n",
    "    def __init__(self, d_vocab, d_embedding, d_hidden, n_layers, \n",
    "                 lstm_drop=0., lin_drop=0., bos_idx=0, bidir=False, tie_weights=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.block = LSTM_Block(d_vocab, d_embedding, d_hidden, d_embedding, n_layers,\n",
    "                                lstm_drop=lstm_drop, lin_drop=lin_drop, bidir=bidir)\n",
    "        self.bos_idx = bos_idx\n",
    "        \n",
    "        if tie_weights:\n",
    "            self.block.embedding.weight = self.block.head.weight\n",
    "        \n",
    "    def forward(self, x, hiddens=None):\n",
    "        output, hiddens, encoded = self.block(x, hiddens)\n",
    "        return output\n",
    "    \n",
    "    def encode(self, x, hiddens=None):\n",
    "        output, hiddens, encoded = self.block(x, hiddens)\n",
    "        return encoded\n",
    "    \n",
    "    def sample(self, bs, sl, temperature=1., multinomial=True):\n",
    "        \n",
    "        preds = idxs = to_device(torch.tensor([self.bos_idx]*bs).long().unsqueeze(-1))\n",
    "        lps = []\n",
    "\n",
    "        hiddens = None\n",
    "        \n",
    "        for i in range(sl):\n",
    "            x, hiddens, encoded = self.block(idxs, hiddens)\n",
    "            x.div_(temperature)\n",
    "            \n",
    "            idxs, lp = x_to_preds(x, multinomial=multinomial)\n",
    "            \n",
    "            lps.append(lp)            \n",
    "            preds = torch.cat([preds, idxs], -1)\n",
    "            \n",
    "        return preds[:, 1:], torch.cat(lps,-1)\n",
    "    \n",
    "    def sample_no_grad(self, bs, sl, temperature=1., multinomial=True):\n",
    "        with torch.no_grad():\n",
    "            return self.sample(bs, sl, temperature=temperature, multinomial=multinomial)\n",
    "        \n",
    "    def get_rl_tensors(self, x, y, temperature=1., latent=None):\n",
    "        output, hiddens, encoded = self.block(x)\n",
    "        output.div_(temperature)\n",
    "        lps = F.log_softmax(output, -1)\n",
    "        lps_gathered = gather_lps(lps, y)\n",
    "        return output, lps, lps_gathered, encoded\n",
    "    \n",
    "    def swap_head(self, new_head):\n",
    "        self.block.head = new_head\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a567187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LSTM_LM(32, 64, 256, 2)\n",
    "ints = torch.randint(0, 31, (16, 10))\n",
    "x = ints[:,:-1]\n",
    "y = ints[:,1:]\n",
    "out = lm(x)\n",
    "o,lp,lpg,e = lm.get_rl_tensors(x,y)\n",
    "_ = lm.sample(8, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa0d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Conditional_LSTM_LM(Encoder_Decoder):\n",
    "    def __init__(self, encoder, d_vocab, d_embedding, d_hidden, d_latent, n_layers,\n",
    "                 lstm_drop=0., lin_drop=0., bidir=False,\n",
    "                 condition_hidden=True, condition_output=False, bos_idx=0, prior=None):\n",
    "        \n",
    "        transition = Norm_Transition(d_latent)\n",
    "        \n",
    "        decoder = Conditional_LSTM_Block(d_vocab, d_embedding, d_hidden, d_embedding,\n",
    "                                d_latent, n_layers, lstm_drop=lstm_drop, lin_drop=lin_drop, \n",
    "                                condition_hidden=condition_hidden, condition_output=condition_output)\n",
    "        \n",
    "        if prior is None:\n",
    "            prior = SphericalPrior(torch.zeros((encoder.d_latent)), torch.zeros((encoder.d_latent)), \n",
    "                                trainable=False)\n",
    "        \n",
    "        super().__init__(encoder, decoder, transition, prior)\n",
    "        \n",
    "        self.bos_idx = bos_idx\n",
    "        \n",
    "    def forward(self, x, condition, hiddens=None):\n",
    "        z = self.encoder(condition)\n",
    "        z = self.transition(z)\n",
    "        x, hiddens, encoded = self.decoder(x, z, hiddens)\n",
    "        return x\n",
    "    \n",
    "    def encode(self, x, condition, hiddens=None):\n",
    "        z = self.to_latent(condition)\n",
    "        x, hiddens, encoded = self.decoder(x, z, hiddens)\n",
    "        return encoded\n",
    "    \n",
    "    def to_latent(self, condition):\n",
    "        z = self.encoder(condition)\n",
    "        z = self.transition(z)\n",
    "        return z\n",
    "\n",
    "    def sample(self, bs, sl, z=None, temperature=1., multinomial=True):\n",
    "        \n",
    "        if z is None:\n",
    "            if self.prior is not None:\n",
    "                z = to_device(self.prior.rsample([bs]))\n",
    "            else:\n",
    "                z = to_device(torch.randn((bs, self.encoder.d_latent)))\n",
    "                z = self.transition(z)\n",
    "        else:\n",
    "            z = self.transition(z)\n",
    "            bs = z.shape[0]\n",
    "        \n",
    "        preds = idxs = to_device(torch.tensor([self.bos_idx]*bs).long().unsqueeze(-1))\n",
    "        lps = []\n",
    "\n",
    "        hiddens = self.decoder.lstm.latent_to_hidden(z)\n",
    "        \n",
    "        for i in range(sl):\n",
    "            \n",
    "            x, hiddens, encoded = self.decoder(idxs,z,hiddens)\n",
    "            x.div_(temperature)\n",
    "            \n",
    "            idxs, lp = x_to_preds(x, multinomial=multinomial)\n",
    "            \n",
    "            lps.append(lp)            \n",
    "            preds = torch.cat([preds, idxs], -1)\n",
    "            \n",
    "        return preds[:, 1:], torch.cat(lps,-1)\n",
    "    \n",
    "    def sample_no_grad(self, bs, sl, z=None, temperature=1., multinomial=True):\n",
    "        with torch.no_grad():\n",
    "            return self.sample(bs, sl, z=z, temperature=temperature, multinomial=multinomial)\n",
    "        \n",
    "    def get_rl_tensors(self, x, y, temperature=1., latent=None):\n",
    "        x,c = x\n",
    "        if latent not None:\n",
    "            latent = self.encoder(c)\n",
    "        z = self.transition(latent)\n",
    "        output, hiddens, encoded = self.decoder(x,z)\n",
    "        output.div_(temperature)\n",
    "        lps = F.log_softmax(output, -1)\n",
    "        \n",
    "        if self.prior.trainable:\n",
    "            prior_lps = self.prior.log_prob(z)\n",
    "            prior_lps = prior_lps.mean(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "            pass_through = torch.zeros(prior_lps.shape).float().to(prior_lps.device)\n",
    "            pass_through = pass_through + prior_lps - prior_lps.detach() # add to gradient chain\n",
    "            lps = lps + pass_through\n",
    "        \n",
    "        lps_gathered = gather_lps(lps, y)\n",
    "        return output, lps, lps_gathered, encoded\n",
    "        \n",
    "    def set_prior_from_latent(self, z, logvar, trainable=False):\n",
    "        z = z.detach()\n",
    "        logvar = logvar.detach()\n",
    "        self.prior = SphericalPrior(z, logvar, trainable)\n",
    "        \n",
    "    def set_prior_from_encoder(self, condition, logvar, trainable=False):\n",
    "        assert condition.shape[0]==1\n",
    "        z = self.transition(self.encoder(condition))\n",
    "        z = z.squeeze(0)\n",
    "        self.set_prior_from_latent(z, logvar, trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad86afaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = MLP_Encoder(128, [64, 32], 16, [0.1, 0.1])\n",
    "\n",
    "lm = Conditional_LSTM_LM(encoder, 32, 64, 128, 16, 2)\n",
    "\n",
    "ints = torch.randint(0, 31, (8, 10))\n",
    "x = ints[:,:-1]\n",
    "y = ints[:,1:]\n",
    "\n",
    "condition = torch.randn((8,128))\n",
    "\n",
    "_ = lm(x, condition)\n",
    "\n",
    "o,lp,lpg,e = lm.get_rl_tensors([x,condition],y)\n",
    "\n",
    "_ = lm.sample(3, 80)\n",
    "\n",
    "lm.set_prior(SphericalPrior(torch.zeros((encoder.d_latent,)), \n",
    "                            torch.zeros((encoder.d_latent,)), True))\n",
    "\n",
    "o,lp,lpg,e = lm.get_rl_tensors([x,condition],y)\n",
    "\n",
    "loss = lpg.mean()\n",
    "\n",
    "assert lm.prior.loc.grad is None\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "assert lm.prior.loc.grad is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad4dfaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1417009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88632359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
