{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7755907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp g_models.lstm_lm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b908af",
   "metadata": {},
   "source": [
    "# LSTM Language Model\n",
    "\n",
    "> LSTM-based language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9dc269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78e2e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from mrl.imports import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *\n",
    "from mrl.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32221c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LSTM_LM(nn.Module):\n",
    "    def __init__(self, d_vocab, d_embedding, d_hidden, n_layers, \n",
    "                 lstm_drop=0., lin_drop=0., bos_idx=0, bidir=False, tie_weights=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.block = LSTM_Block(d_vocab, d_embedding, d_hidden, d_embedding, n_layers,\n",
    "                                lstm_drop=lstm_drop, lin_drop=lin_drop, bidir=bidir)\n",
    "        self.bos_idx = bos_idx\n",
    "        \n",
    "        if tie_weights:\n",
    "            self.block.embedding.weight = self.block.head.weight\n",
    "        \n",
    "    def forward(self, x, hiddens=None):\n",
    "        output, hiddens, pre_output = self.block(x, hiddens)\n",
    "        return output\n",
    "    \n",
    "    def sample(self, bs, sl, temperature=1., multinomial=True):\n",
    "        \n",
    "        preds = idxs = to_device(torch.tensor([self.bos_idx]*bs).long().unsqueeze(-1))\n",
    "        lps = []\n",
    "\n",
    "        hiddens = None\n",
    "        \n",
    "        for i in range(sl):\n",
    "            x, hiddens = self.block(idxs, hiddens)\n",
    "            x.div_(temperature)\n",
    "            \n",
    "            idxs, lp = x_to_preds(x, multinomial=multinomial)\n",
    "            \n",
    "            lps.append(lp)            \n",
    "            preds = torch.cat([preds, idxs], -1)\n",
    "            \n",
    "        return preds[:, 1:], torch.cat(lps,-1)\n",
    "    \n",
    "    def sample_no_grad(self, bs, sl, temperature=1., multinomial=True):\n",
    "        with torch.no_grad():\n",
    "            return self.sample(bs, sl, temperature=temperature, multinomial=multinomial)\n",
    "        \n",
    "    def get_lps(self, x, y, temperature=1.):\n",
    "        x = self.forward(x)\n",
    "        x.div_(temperature)\n",
    "        \n",
    "        lps = F.log_softmax(x, -1)\n",
    "        lps = lps.gather(2, y.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        return lps, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a567187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LSTM_LM(32, 64, 256, 2)\n",
    "ints = torch.randint(0, 31, (16, 10))\n",
    "x = ints[:,:-1]\n",
    "y = ints[:,1:]\n",
    "out = lm(x)\n",
    "lp,_ = lm.get_lps(x,y)\n",
    "_ = lm.sample(8, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa0d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Conditional_LSTM_LM(Encoder_Decoder):\n",
    "    def __init__(self, encoder, d_vocab, d_embedding, d_hidden, d_latent, n_layers,\n",
    "                 lstm_drop=0., lin_drop=0., bidir=False,\n",
    "                 condition_hidden=True, condition_output=False, bos_idx=0, prior=None):\n",
    "        \n",
    "        transition = Norm_Transition(d_latent)\n",
    "        \n",
    "        decoder = Conditional_LSTM_Block(d_vocab, d_embedding, d_hidden, d_embedding,\n",
    "                                d_latent, n_layers, lstm_drop=lstm_drop, lin_drop=lin_drop, \n",
    "                                condition_hidden=condition_hidden, condition_output=condition_output)\n",
    "        \n",
    "        if prior is None:\n",
    "            prior = SphericalPrior(torch.zeros((encoder.d_latent)), torch.zeros((encoder.d_latent)), \n",
    "                                trainable=False)\n",
    "        \n",
    "        super().__init__(encoder, decoder, transition, prior)\n",
    "        \n",
    "        self.bos_idx = bos_idx\n",
    "        \n",
    "    def forward(self, x, condition, hiddens=None):\n",
    "        z = self.encoder(condition)\n",
    "        z = self.transition(z)\n",
    "        x, hiddens = self.decoder(x, z, hiddens)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def sample(self, bs, sl, z=None, temperature=1., multinomial=True):\n",
    "        \n",
    "        if z is None:\n",
    "            if self.prior is not None:\n",
    "                z = to_device(self.prior.rsample([bs]))\n",
    "            else:\n",
    "                z = to_device(torch.randn((bs, self.encoder.d_latent)))\n",
    "                z = self.transition(z)\n",
    "        else:\n",
    "            bs = z.shape[0]\n",
    "        \n",
    "        preds = idxs = to_device(torch.tensor([self.bos_idx]*bs).long().unsqueeze(-1))\n",
    "        lps = []\n",
    "\n",
    "        hiddens = self.decoder.lstm.latent_to_hidden(z)\n",
    "        \n",
    "        for i in range(sl):\n",
    "            \n",
    "            x, hiddens = self.decoder(idxs,z,hiddens)\n",
    "            x.div_(temperature)\n",
    "            \n",
    "            idxs, lp = x_to_preds(x, multinomial=multinomial)\n",
    "            \n",
    "            lps.append(lp)            \n",
    "            preds = torch.cat([preds, idxs], -1)\n",
    "            \n",
    "        return preds[:, 1:], torch.cat(lps,-1)\n",
    "    \n",
    "    def sample_no_grad(self, bs, sl, z=None, temperature=1., multinomial=True):\n",
    "        with torch.no_grad():\n",
    "            return self.sample(bs, sl, z=z, temperature=temperature, multinomial=multinomial)\n",
    "        \n",
    "    def get_lps(self, x, y, temperature=1.):\n",
    "        x, c = x\n",
    "        z = self.transition(self.encoder(c))\n",
    "        x,_ = self.decoder(x, z)\n",
    "        \n",
    "        x.div_(temperature)\n",
    "        \n",
    "        lps = F.log_softmax(x, -1)\n",
    "        lps = lps.gather(2, y.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        if self.prior.trainable:\n",
    "            prior_lps = self.prior.log_prob(z).mean(-1, keepdim=True)\n",
    "            prior_lps = torch.zeros(prior_lps.shape).float().to(prior_lps.device) + prior_lps - prior_lps.detach()\n",
    "            lps += prior_lps\n",
    "        \n",
    "        return lps, x\n",
    "        \n",
    "    def set_prior_from_latent(self, z, logvar, trainable=False):\n",
    "        z = z.detach()\n",
    "        logvar = logvar.detach()\n",
    "        self.prior = SphericalPrior(z, logvar, trainable)\n",
    "        \n",
    "    def set_prior_from_encoder(self, condition, logvar, trainable=False):\n",
    "        assert condition.shape[0]==1\n",
    "        z = self.transition(self.encoder(condition))\n",
    "        z = z.squeeze(0)\n",
    "        self.set_prior_from_latent(z, logvar, trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad86afaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = MLP_Encoder(128, [64, 32], 16, [0.1, 0.1])\n",
    "\n",
    "lm = Conditional_LSTM_LM(encoder, 32, 64, 128, 16, 2)\n",
    "\n",
    "ints = torch.randint(0, 31, (8, 10))\n",
    "x = ints[:,:-1]\n",
    "y = ints[:,1:]\n",
    "\n",
    "condition = torch.randn((8,128))\n",
    "\n",
    "_ = lm(x, condition)\n",
    "\n",
    "_ = lm.get_lps([x,condition],y)\n",
    "\n",
    "_ = lm.sample(3, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9a8e86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1417009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88632359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
