{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "! [ -e /content ] && pip install -Uqq mrl-pypi  # upgrade mrl on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp train.template_cb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template Callback\n",
    "\n",
    "> Callbacks for templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmai/miniconda3/envs/mrl/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for boost::shared_ptr<RDKit::FilterCatalogEntry const> already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "from mrl.train.callback import *\n",
    "from mrl.chem import *\n",
    "from mrl.templates.all import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template Callback\n",
    "\n",
    "The `TemplateCallback` class is used by the `Environment` to interface with a `Template` during training.\n",
    "\n",
    "Templates serve two roles during training - filtering and scoring of samples\n",
    "\n",
    "### Filtering\n",
    "\n",
    "Templates filter all samples added to the `Buffer` and sampled during each batch. If the argument `prefilter=True` is passed to the template callback, the template will remove all samples that fail the template's hard filters. If `prefilter=False` is passed, `Template.validate` will be used to remove invalid compounds, but will ignore compounds that violate the hard filters\n",
    "\n",
    "### Scoring\n",
    "\n",
    "If the Template has any soft filters, those filters will be used to score compounds each batch. The aggregate soft filter score will be multiplied by `TemplateCallback.weight` and added to the total reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "        \n",
    "class TemplateCallback(Callback):\n",
    "    '''\n",
    "    TemplateCallback - callback wrapper for `Template` class\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `template Template`: template to use\n",
    "    \n",
    "    - `sample_name str`: `BatchState` attribute to grab samples from\n",
    "    \n",
    "    - `weight float`: weight used to scale template soft filter score\n",
    "    \n",
    "    - `track bool`: if True, template results are added to the batch log \n",
    "    and metric log\n",
    "    \n",
    "    - `prefilter bool`: if True, samples that fail hard filters in the \n",
    "    template are removed\n",
    "    \n",
    "    - `do_filter bool`: controls if filtering is done at aall\n",
    "    '''\n",
    "    def __init__(self, template=None, sample_name='samples', weight=1., \n",
    "                 track=True, prefilter=True, do_filter=True):\n",
    "        super().__init__(order=-1)\n",
    "        self.template = template\n",
    "        self.track = track\n",
    "        self.name = 'template'\n",
    "        self.prefilter = prefilter\n",
    "        self.weight = weight\n",
    "        self.sample_name = sample_name\n",
    "        self.do_filter = do_filter\n",
    "        \n",
    "    def setup(self):\n",
    "        if self.track:\n",
    "            log = self.environment.log\n",
    "            log.add_metric(self.name)\n",
    "            log.add_log(self.name)\n",
    "            log.add_metric(f'valid')\n",
    "            \n",
    "            if isinstance(self.template, BlockTemplate):\n",
    "                log.add_log('samples_fused')\n",
    "\n",
    "    def filter_buffer(self):\n",
    "        if self.do_filter:\n",
    "            env = self.environment\n",
    "            buffer = env.buffer\n",
    "            if buffer.buffer:\n",
    "                buffer.buffer = self.standardize(buffer.buffer)\n",
    "                valids = self.filter_sequences(buffer.buffer, return_array=True)\n",
    "                buffer._filter_buffer(valids)\n",
    "                \n",
    "    def filter_batch(self):\n",
    "        valid = 1.\n",
    "        env = self.environment\n",
    "        if self.do_filter:\n",
    "            batch_state = env.batch_state\n",
    "\n",
    "            samples = batch_state[self.sample_name]\n",
    "            samples = self.standardize(samples)\n",
    "            batch_state[self.sample_name] = samples\n",
    "\n",
    "            valids = self.filter_sequences(samples, return_array=True)\n",
    "\n",
    "            self._filter_batch(valids)\n",
    "            \n",
    "            valid = valids.mean()\n",
    "\n",
    "        if self.track:\n",
    "            env.log.update_metric('valid', valid)\n",
    "            \n",
    "                            \n",
    "    def compute_reward(self):\n",
    "        env = self.environment\n",
    "        state = env.batch_state\n",
    "        \n",
    "        if isinstance(self.template, BlockTemplate):\n",
    "            outputs = self.template.recurse_fragments(state[self.sample_name])\n",
    "            rewards = np.array([i[3] for i in outputs])\n",
    "            state[self.sample_name+'_fused'] = [i[1] for i in outputs]\n",
    "            \n",
    "        elif self.template is not None:\n",
    "            rewards = np.array(self.template.eval_mols(state[self.sample_name]))\n",
    "            \n",
    "        else:\n",
    "            rewards = np.array([0.]*len(state[self.sample_name]))\n",
    "        \n",
    "        hps = self.get_hps(state[self.sample_name])\n",
    "        state[self.name] = rewards\n",
    "        rewards = rewards*self.weight\n",
    "        \n",
    "        if self.track:\n",
    "            env.log.update_metric(self.name, rewards.mean())\n",
    "            \n",
    "        state.template_passes = hps\n",
    "        state.rewards += to_device(torch.from_numpy(rewards).float())\n",
    "        \n",
    "    def get_hps(self, sequences):\n",
    "        if self.template is not None:\n",
    "            hps = np.array(self.template(sequences))\n",
    "        else:\n",
    "            hps = np.array([True]*len(sequences))\n",
    "            \n",
    "        return hps\n",
    "        \n",
    "    def filter_sequences(self, sequences, return_array=False):\n",
    "        if self.prefilter:\n",
    "            passes = self.get_hps(sequences)\n",
    "        else:\n",
    "            passes = self.validate(sequences)\n",
    "        \n",
    "        if return_array:\n",
    "            output = passes\n",
    "        else:\n",
    "            output  = [sequences[i] for i in range(len(sequences)) if passes[i]]\n",
    "        return output\n",
    "    \n",
    "    def standardize(self, sequences):\n",
    "        if self.template is not None:\n",
    "            sequences = self.template.standardize(sequences)\n",
    "        \n",
    "        return sequences\n",
    "    \n",
    "    def validate(self, sequences):\n",
    "        if self.template is not None:\n",
    "            valid = np.array(self.template.validate(sequences))\n",
    "        else:\n",
    "            valid = np.array([True]*len(sequences))\n",
    "            \n",
    "        return valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive Template\n",
    "\n",
    "The `ContrastiveTemplate` class applies a template to tasks based around comparing input and output compounds (ie making relative improvements to a compound's properties).\n",
    "\n",
    "During filtering, the contrastive template will keep samples where both input and output compounds pass the filters.\n",
    "\n",
    "Contrastive templates also use a similarity function to impose a similarity constraint on sample pairs (ie output compound must have a similarity of X to the input compound).\n",
    "\n",
    "One consideration in using contrastive scores is how to properly scale contrastive scores. If we have a score with a maximum value of `1`, a contrastive sample pair where the score goes from `0.8` to `1` should get the same reward as a sample pair where the score goes from `0.6` to `1`. In both cases, the model maximized the output score to the greatest extent possible. To do this, we can scale the reward differences by the maximum possible reward different (ie `reward = (output_reward - input_reward)/(max_reward - input_reward)`). Passing a value to `max_score` will cause the contrastive template to perform this scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "    \n",
    "class ContrastiveTemplate(TemplateCallback):\n",
    "    '''\n",
    "    ContrastiveTemplate -  contrasttive callback wrapper for `Template` class\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `similarity_function SimilarityFunction`: evaluates similarity between \n",
    "    source and targe samples\n",
    "    \n",
    "    - `sample_name str`: `BatchState` attribute to grab samples from\n",
    "    \n",
    "    - `max_score Optional[float]`: maximum template reward. If given, will be \n",
    "    used to scale contrastive scores\n",
    "    \n",
    "    - `template Template`: template to use\n",
    "    \n",
    "    - `weight float`: weight used to scale template soft filter score\n",
    "    \n",
    "    - `track bool`: if True, template results are added to the batch log \n",
    "    and metric log\n",
    "    \n",
    "    - `prefilter bool`: if True, samples that fail hard filters in the \n",
    "    template are removed\n",
    "    \n",
    "    - `do_filter bool`: controls if filtering is done at aall\n",
    "    '''\n",
    "    def __init__(self, similarity_function, sample_name='samples',\n",
    "                 max_score=None, template=None, \n",
    "                 weight=1., track=True, prefilter=True, do_filter=True):\n",
    "        super().__init__(template=template, \n",
    "                         sample_name=sample_name, \n",
    "                         weight=weight, \n",
    "                         track=track, \n",
    "                         prefilter=prefilter,\n",
    "                         do_filter=do_filter)\n",
    "        \n",
    "        self.similarity_function = similarity_function\n",
    "        self.max_score = max_score\n",
    "        \n",
    "    def setup(self):\n",
    "        if self.track:\n",
    "            log = self.environment.log\n",
    "            log.add_metric(self.name)\n",
    "            log.add_metric(self.name+'_temp')\n",
    "            log.add_metric(self.name+'_sim')\n",
    "            log.add_metric(f'valid')\n",
    "            \n",
    "            log.add_log(self.name)\n",
    "            log.add_log(self.name+'_temp')\n",
    "            log.add_log(self.name+'_sim')\n",
    "            \n",
    "            if isinstance(self.template, BlockTemplate):\n",
    "                log.add_log('samples_fused')\n",
    "        \n",
    "    def compute_reward(self):\n",
    "        env = self.environment\n",
    "        state = env.batch_state\n",
    "        \n",
    "        samples = state[self.sample_name]\n",
    "        source_samples = [i[0] for i in samples]\n",
    "        target_samples = [i[1] for i in samples]\n",
    "        hps = self.get_hps(samples)\n",
    "        \n",
    "        if self.template is not None:\n",
    "            \n",
    "            if isinstance(self.template, BlockTemplate):\n",
    "                source_outputs = self.template.recurse_fragments(source_samples)\n",
    "                target_outputs = self.template.recurse_fragments(target_samples)\n",
    "                state[self.sample_name+'_fused'] = [(source_outputs[i][1], target_outputs[i][1])\n",
    "                                      for i in range(len(source_outputs))]\n",
    "                \n",
    "                source_rewards = np.array([i[3] for i in source_outputs])\n",
    "                target_rewards = np.array([i[3] for i in target_outputs])\n",
    "                \n",
    "            else:\n",
    "                source_rewards = np.array(self.template.eval_mols(source_samples))\n",
    "                target_rewards = np.array(self.template.eval_mols(target_samples))\n",
    "            \n",
    "            rewards = target_rewards - source_rewards\n",
    "            if self.max_score is not None:\n",
    "                rewards = rewards / (self.max_score-source_rewards)\n",
    "            \n",
    "        else:\n",
    "            rewards = np.array([0.]*len(state[self.sample_name]))\n",
    "\n",
    "        sim_scores = self.similarity_function.score(source_samples, target_samples)\n",
    "        \n",
    "        state.template = rewards\n",
    "        state.template_sim = sim_scores\n",
    "        \n",
    "        full_rewards = rewards + sim_scores\n",
    "        full_rewards = full_rewards*self.weight\n",
    "        \n",
    "        if self.track:\n",
    "            env.log.update_metric(self.name, full_rewards.mean())\n",
    "            env.log.update_metric(self.name+'_temp', rewards.mean())\n",
    "            env.log.update_metric(self.name+'_sim', sim_scores.mean())\n",
    "            \n",
    "        state[self.name] = full_rewards\n",
    "        state[self.name+'_temp'] = rewards\n",
    "        state[self.name+'_sim'] = sim_scores\n",
    "            \n",
    "        state.template_passes = hps\n",
    "        state.rewards += to_device(torch.from_numpy(full_rewards).float())\n",
    "        \n",
    "    def standardize(self, sequences):\n",
    "        if self.template is not None:\n",
    "            sources = self.template.standardize([i[0] for i in sequences])\n",
    "            targets = self.template.standardize([i[1] for i in sequences])\n",
    "            sequences = [(sources[i], targets[i]) for i in range(len(sources))]\n",
    "        \n",
    "        return sequences\n",
    "        \n",
    "    def get_hps(self, sequences):\n",
    "        \n",
    "        if type(sequences[0])==str:\n",
    "            hps = super().get_hps(sequences)\n",
    "        else:\n",
    "            source_sequences = [i[0] for i in sequences]\n",
    "            target_sequences = [i[1] for i in sequences]\n",
    "            s_hps = super().get_hps(source_sequences)\n",
    "            t_hps = super().get_hps(target_sequences)\n",
    "            sim_bools = self.similarity_function.bools(source_sequences, target_sequences)\n",
    "            hps = s_hps*t_hps*sim_bools\n",
    "            \n",
    "        return hps\n",
    "    \n",
    "    def validate(self, sequences):\n",
    "        if type(sequences[0])==str:\n",
    "            valid = super().validate(sequences)\n",
    "        else:\n",
    "            s_val = super().validate([i[0] for i in sequences])\n",
    "            t_val = super().validate([i[1] for i in sequences])\n",
    "            valid = s_val*t_val\n",
    "            \n",
    "        return valid\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class SimilarityFunction():\n",
    "    '''\n",
    "    SimilarityFunction - compares similarity between source \n",
    "    and target samples\n",
    "    '''\n",
    "    def score(self, source_smiles, target_smiles):\n",
    "        return [0. for i in source_smiles]\n",
    "    \n",
    "    def bools(self, source_smiles, target_smiles):\n",
    "        return [True for i in source_smiles]\n",
    "\n",
    "class FPSimilarity(SimilarityFunction):\n",
    "    '''\n",
    "    FPSimilarity - computes paired sample similarity using fingerprint \n",
    "    similarity\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `fp_function Callable`: Fingerprint function, ie `ECFP6`. Should \n",
    "    return a fingerprint when called\n",
    "    \n",
    "    - `similarity_function Callable`: fingerprint similarity function, \n",
    "    ie `tanimoto`\n",
    "    \n",
    "    - `min_sim float`: minimum similarity between paired samples\n",
    "    \n",
    "    - `max_sim float`: maximum similarity between samples\n",
    "    \n",
    "    - `passscore float`: score for passing samples\n",
    "    \n",
    "    - `failscore float`: score for failling compounds\n",
    "    \n",
    "    - `soft_min Optional[float]`: if given, this value is used as \n",
    "    the minimum similarity cutoff during scoring but not for filtering\n",
    "    \n",
    "    - `soft_max Optional[float]`: if given, this value is used as \n",
    "    the maximum similarity cutoff during scoring but not for filtering\n",
    "    '''\n",
    "    def __init__(self, fp_function, similarity_function, min_sim, max_sim, \n",
    "                 passscore, failscore, soft_min=None, soft_max=None):\n",
    "        self.fp_function = fp_function\n",
    "        self.similarity_function = similarity_function\n",
    "        self.min_sim = min_sim\n",
    "        self.max_sim = max_sim\n",
    "        self.passscore = passscore\n",
    "        self.failscore = failscore\n",
    "        self.soft_min = soft_min\n",
    "        self.soft_max = soft_max\n",
    "        \n",
    "    def get_sims(self, source_smiles, target_smiles):\n",
    "        source_fps = maybe_parallel(self.fp_function, source_smiles)\n",
    "        target_fps = maybe_parallel(self.fp_function, target_smiles)\n",
    "        \n",
    "        sims = np.array([self.similarity_function(source_fps[i], [target_fps[i]])[0] \n",
    "                 for i in range(len(source_smiles))])\n",
    "        return sims\n",
    "        \n",
    "    def score(self, source_smiles, target_smiles, sims=None):\n",
    "        if sims is None:\n",
    "            sims = self.get_sims(source_smiles, target_smiles)\n",
    "            \n",
    "        min_sim = self.min_sim if self.soft_min is None else self.soft_min\n",
    "        max_sim = self.max_sim if self.soft_max is None else self.soft_max\n",
    "        bools = (min_sim<sims) & (sims<max_sim)\n",
    "            \n",
    "        return bools*self.passscore + (~bools)*self.failscore\n",
    "            \n",
    "    def bools(self, source_smiles, target_smiles, sims=None):\n",
    "        if sims is None:\n",
    "            sims = self.get_sims(source_smiles, target_smiles)\n",
    "        \n",
    "        bools = (self.min_sim<sims) & (sims<self.max_sim)\n",
    "        return bools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrl",
   "language": "python",
   "name": "mrl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
