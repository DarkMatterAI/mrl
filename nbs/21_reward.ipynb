{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "! [ -e /content ] && pip install -Uqq mrl-pypi  # upgrade mrl on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp train.reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward\n",
    "\n",
    "> Rewards - non-differentiable scores for samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Rewards are non-differentiable score functions for evaluating samples. Rewards should generally follow the format `reward = reward_function(sample)`\n",
    "\n",
    "Rewards in MRL occupy five events in the fit loop:\n",
    "- `before_compute_reward` - set up necessary values prior to reward calculation (if needed)\n",
    "- `compute_reward` - compute reward\n",
    "- `after_compute_reward` - compute metrics (if needed)\n",
    "- `reward_modification` - adjust rewards\n",
    "- `after_reward_modification` - compute metrics (if needed)\n",
    "\n",
    "### Rewards vs Reward Modifications\n",
    "\n",
    "MRl breaks rewards up into two phases - rewards and reward modifications. The difference between the two phases is that __reward__ values are saved in the batch log, while __reward_modifications__ are not. \n",
    "\n",
    "In this framework, rewards are absolute scores for samples that are used to evaluate the sample relative to all other samples in the log. Reward modifications are transient scores that depend on the current training context.\n",
    "\n",
    "A reward modification might be something like adding a score bonus to compounds the first time they are created during training to encourage diversity, or penalizing compounds if they appear more than 3 times in the last 5 batches. These types of reward modifications allow us to influence the behavior of the generative model without having these scores effect the true rewards we save in the log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from mrl.imports import *\n",
    "from mrl.core import *\n",
    "from mrl.train.callback import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Class\n",
    "\n",
    "As mentioned above, rewards generally follow the format `reward = reward_function(sample)`. The `Reward` class acts as a wrapper around the `reward_function` to provide some convenience functions. `Reward` maintains a lookup table of `sample : reward` values to avoid repeat computation. `Reward` handles batching novel samples (ie not in the lookup table), sending them to `reward_function`, and merging the outputs with the lookup table values.\n",
    "\n",
    "Creating a custom reward involves creating a callable function or object that can take in a list of `samples` and return a list of reward values. For example:\n",
    "\n",
    "```\n",
    "class MyRewardFunction():\n",
    "    def __call__(self, samples):\n",
    "        rewards = self.do_reward_calculation(samples)\n",
    "        return rewards\n",
    "        \n",
    "reward_function = MyRewardFunction()\n",
    "reward = Reward(reward_function, weight=0.5, log=True)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Reward():\n",
    "    '''\n",
    "    Reward - wrapper for `reward_function`. Handles batching \n",
    "    and value lookup\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `reward_function Callable`: function with the format \n",
    "    `rewards = reward_function(samples)`\n",
    "    \n",
    "    - `weight float`: weight to scale rewards\n",
    "    \n",
    "    - `bs Optional[int]`: if given, samples will be batched into \n",
    "    chunks of size `bs` and sent to `reward_function` as batches\n",
    "    \n",
    "    - `device Optional[bool]`: if True, reward function output is \n",
    "    mapped to device. see `to_device`\n",
    "    \n",
    "    - `log bool`: if True, keeps aa lookup table of \n",
    "    `sample : reward` values to avoid repeat computation\n",
    "    '''\n",
    "    def __init__(self, reward_function, weight=1, bs=None, device=False, log=True):\n",
    "        \n",
    "        self.reward_function = reward_function\n",
    "        self.weight = weight\n",
    "        self.bs = bs\n",
    "        self.device = device\n",
    "        self.score_log = {}\n",
    "        self.log = log\n",
    "        \n",
    "    def load_data(self, samples, values):\n",
    "        for i in range(len(samples)):\n",
    "            self.score_log[samples[i]] = values[i]\n",
    "            \n",
    "    def __call__(self, samples, **reward_kwargs):\n",
    "        \n",
    "        rewards = np.array([0. for i in samples])\n",
    "        \n",
    "        to_score = []\n",
    "        to_score_idxs = []\n",
    "        \n",
    "        for i in range(len(samples)):\n",
    "                \n",
    "            if self.log:\n",
    "                if samples[i] in self.score_log:\n",
    "                    rewards[i] = self.score_log[samples[i]]\n",
    "                else:\n",
    "                    to_score.append(samples[i])\n",
    "                    to_score_idxs.append(i)\n",
    "\n",
    "            else:\n",
    "                to_score.append(samples[i])\n",
    "                to_score_idxs.append(i)\n",
    "                    \n",
    "        if to_score:\n",
    "            new_rewards = self.compute_batched_reward(to_score, **reward_kwargs)\n",
    "\n",
    "            for i in range(len(to_score)):\n",
    "                batch_idx = to_score_idxs[i]\n",
    "                reward = new_rewards[i]\n",
    "                rewards[batch_idx] = reward\n",
    "\n",
    "                if self.log:\n",
    "                    self.score_log[to_score[i]] = reward\n",
    "                \n",
    "        rewards = torch.tensor(rewards).float().squeeze()\n",
    "        rewards = rewards * self.weight\n",
    "        \n",
    "        if self.device:\n",
    "            rewards = to_device(rewards)\n",
    "\n",
    "        return rewards\n",
    "            \n",
    "    def _compute_reward(self, samples, **reward_kwargs):\n",
    "        return self.reward_function(samples, **reward_kwargs)\n",
    "    \n",
    "    def compute_batched_reward(self, samples, **reward_kwargs):\n",
    "        if self.bs is not None:\n",
    "            sample_chunks = chunk_list(samples, self.bs)\n",
    "            rewards = []\n",
    "            for chunk in sample_chunks:\n",
    "                rewards_iter = self._compute_reward(chunk, **reward_kwargs)\n",
    "                if isinstance(rewards_iter, torch.Tensor):\n",
    "                    rewards_iter = rewards_iter.detach().cpu()\n",
    "                    \n",
    "                rewards += list(rewards_iter)\n",
    "            \n",
    "        else:\n",
    "            rewards = self._compute_reward(samples, **reward_kwargs)\n",
    "            if isinstance(rewards, torch.Tensor):\n",
    "                rewards = rewards.detach().cpu()\n",
    "            \n",
    "        return rewards\n",
    "    \n",
    "    def add_data_to_log(self, samples, rewards):\n",
    "        for i in range(len(samples)):\n",
    "            self.score_log[samples[i]] = rewards[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Callback\n",
    "\n",
    "`RewardCallback` handles it loop integration and metric logging for a given `Reward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class RewardCallback(Callback):\n",
    "    '''\n",
    "    RewardCallback - callback wrapper for `Reward` \n",
    "    used during `compute_reward` event\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `reward Reward`: reward to use\n",
    "    \n",
    "    - `name str`: reward name\n",
    "    \n",
    "    - `sample_name str`: sample name to grab from \n",
    "    `BatchState` to send to `reward`\n",
    "    \n",
    "    - `order int`: callback order\n",
    "    \n",
    "    - `track bool`: if metrics should be tracked \n",
    "    from this callback\n",
    "    '''\n",
    "    def __init__(self, reward, name, sample_name='samples',\n",
    "                order=10, track=True):\n",
    "        super().__init__(name=name, order=order)\n",
    "        \n",
    "        self.reward = reward\n",
    "        self.sample_name = sample_name\n",
    "        self.track = track\n",
    "        \n",
    "    def setup(self):\n",
    "        log = self.environment.log\n",
    "        log.add_log(self.name)\n",
    "        if self.track:\n",
    "            log.add_metric(self.name)\n",
    "            \n",
    "    def compute_reward(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        samples = batch_state[self.sample_name]\n",
    "        \n",
    "        if samples:\n",
    "            rewards = self.reward(samples)\n",
    "        else:\n",
    "            rewards = to_device(torch.tensor(0.))\n",
    "\n",
    "        batch_state.rewards += rewards\n",
    "        batch_state[self.name] = rewards\n",
    "        \n",
    "        if self.track:\n",
    "            env.log.update_metric(self.name, rewards.mean().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For greater flexibility, `GenericRewardCallback` will pass the entire `BatchState` to `reward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class GenericRewardCallback(RewardCallback):\n",
    "    '''\n",
    "    GenericRewardCallback - generic reward \n",
    "    wrapper\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `reward Callable`: reward function. Reward \n",
    "    will be passed the entire batch state\n",
    "    \n",
    "    - `name str`: reward name\n",
    "    \n",
    "    - `order int`: callback order\n",
    "    \n",
    "    - `track bool`: if metrics should be tracked \n",
    "    from this callback\n",
    "    '''\n",
    "    def __init__(self, reward, name, \n",
    "                order=10, track=True):\n",
    "        super().__init__(reward,\n",
    "                         name,\n",
    "                         order=order,\n",
    "                         track=track\n",
    "                        )\n",
    "        \n",
    "    def compute_reward(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        rewards = self.reward(batch_state)\n",
    "        \n",
    "        batch_state.rewards += rewards\n",
    "        batch_state[self.name] = rewards\n",
    "        \n",
    "        if self.track:\n",
    "            env.log.update_metric(self.name, rewards.mean().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Modification\n",
    "\n",
    "As discussed above, reward modifications apply changes to rewards based on some sort of transient batch context. These are rewards that will influence a given batch, but not the logged rewards.\n",
    "\n",
    "Reward modifications should update the value `BatchState.rewards_final`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class RewardModification(Callback):\n",
    "    '''\n",
    "    RewardModification - callback wrapper for `Reward` \n",
    "    used during `reward_modification` event\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `reward Reward`: reward to use\n",
    "    \n",
    "    - `name str`: reward name\n",
    "    \n",
    "    - `sample_name str`: sample name to grab from \n",
    "    `BatchState` to send to `reward`\n",
    "    \n",
    "    - `order int`: callback order\n",
    "    \n",
    "    - `track bool`: if metrics should be tracked \n",
    "    from this callback\n",
    "    '''\n",
    "    def __init__(self, reward, name, sample_name='samples',\n",
    "                order=10, track=True):\n",
    "        super().__init__(name=name, order=order)\n",
    "        \n",
    "        self.reward = reward\n",
    "        self.sample_name = sample_name\n",
    "        self.track = track\n",
    "        \n",
    "    def setup(self):\n",
    "        log = self.environment.log\n",
    "        log.add_log(self.name)\n",
    "        if self.track:\n",
    "            log.add_metric(self.name)\n",
    "            \n",
    "    def reward_modification(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        samples = batch_state[self.sample_name]\n",
    "        \n",
    "        if samples:\n",
    "            rewards = self.reward(samples)\n",
    "        else:\n",
    "            rewards = 0.\n",
    "\n",
    "        batch_state.rewards_final += rewards\n",
    "        batch_state[self.name] = rewards\n",
    "        \n",
    "        if self.track:\n",
    "            env.log.update_metric(self.name, rewards.mean().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class NoveltyReward(Callback):\n",
    "    '''\n",
    "    NoveltyReward - gives a reward bonus \n",
    "    for new samples. Rewards are given a \n",
    "    bonus of `weight`\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `weight float`: novelty score weight\n",
    "    \n",
    "    - `track bool`: if metrics should be tracked \n",
    "    from this callback\n",
    "    '''\n",
    "    def __init__(self, weight=1., track=True):\n",
    "        super().__init__(name='novel')\n",
    "        \n",
    "        self.weight = weight\n",
    "        self.track = track\n",
    "        \n",
    "    def setup(self):\n",
    "        log = self.environment.log\n",
    "        log.add_log(self.name)\n",
    "        if self.track:\n",
    "            log.add_metric(self.name)\n",
    "            \n",
    "    def reward_modification(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        samples = batch_state.samples\n",
    "        \n",
    "        df = env.log.df\n",
    "        new = (~pd.Series(samples).isin(df.samples)).values\n",
    "        \n",
    "        rewards = np.array([float(i) for i in new])*self.weight\n",
    "        rewards = to_device(torch.from_numpy(rewards).float())\n",
    "\n",
    "        batch_state.rewards_final += rewards\n",
    "        batch_state[self.name] = rewards\n",
    "        \n",
    "        if self.track:\n",
    "            env.log.update_metric(self.name, rewards.mean().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive Reward\n",
    "\n",
    "Similar to `ContrastiveTemplate`, `ContrastiveReward` provides a wrapper around a `RewardCallback` to adapt it for the task of contrastive generation.\n",
    "\n",
    "For contrastive generation, we want the model to ingest a source sample and produce a target sample that receives a higher reward than the source sample. `ContrastiveReward` takes some `base_reward` and computes the values of that base reward for both source and target samples, and returns the difference between those rewards.\n",
    "\n",
    "Optionally, the contrastive reward will scale the relative reward based on a given `max_score` (ie `reward = (target_reward - source_reward)/(max_reward - source_reward)`). This scales the contrastive reward relative to the maximum possible reward \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class ContrastiveReward(RewardCallback):\n",
    "    '''\n",
    "    ContrastiveReward - contrastive wrapper for \n",
    "    reward callbacks\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `base_reward RewardCallback`: base reward callback\n",
    "    \n",
    "    - `max_score Optional[float]`: maximum possible score. \n",
    "    If given, contrastive rewards are scaled following \n",
    "    `reward = (target_reward - source_reward)/(max_reward - source_reward)`\n",
    "    '''\n",
    "    def __init__(self, base_reward, max_score=None):\n",
    "        super().__init__(reward = base_reward.reward,\n",
    "                         name = base_reward.name,\n",
    "                         sample_name = base_reward.sample_name,\n",
    "                         order = base_reward.order,\n",
    "                         track = base_reward.track)\n",
    "        \n",
    "        self.base_reward = base_reward\n",
    "        self.max_score = max_score\n",
    "    \n",
    "    def setup(self):\n",
    "        self.base_reward.environment = self.environment\n",
    "        \n",
    "    def __call__(self, event_name):\n",
    "        \n",
    "        event = getattr(self, event_name, None)\n",
    "        \n",
    "        if event is not None:\n",
    "            output = event()\n",
    "        else:\n",
    "            output = None\n",
    "            \n",
    "        if not event_name=='compute_reward':\n",
    "            _ = self.base_reward(event_name)\n",
    "            \n",
    "        return output\n",
    "        \n",
    "    def compute_and_clean(self, samples):\n",
    "        rewards = self.base_reward.reward(samples)\n",
    "        if isinstance(rewards, torch.Tensor):\n",
    "            rewards = rewards.detach().cpu()\n",
    "            \n",
    "        rewards = np.array(rewards)\n",
    "        return rewards\n",
    "        \n",
    "    def _compute_reward(self, samples):\n",
    "        source_samples = [i[0] for i in samples]\n",
    "        target_samples = [i[1] for i in samples]\n",
    "        \n",
    "        source_rewards = self.compute_and_clean(source_samples)\n",
    "        target_rewards = self.compute_and_clean(target_samples)\n",
    "        \n",
    "        rewards = target_rewards - source_rewards\n",
    "        if self.max_score is not None:\n",
    "            rewards = rewards / (self.max_score-source_rewards)\n",
    "            \n",
    "        rewards = to_device(torch.from_numpy(rewards).float())\n",
    "            \n",
    "        return rewards\n",
    "    \n",
    "    def compute_reward(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        samples = batch_state[self.sample_name]\n",
    "        \n",
    "        rewards = self._compute_reward(samples)\n",
    "        \n",
    "        batch_state.rewards += rewards\n",
    "        batch_state[self.name] = rewards\n",
    "        \n",
    "        if self.track:\n",
    "            env.log.update_metric(self.name, rewards.mean().detach().cpu().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
