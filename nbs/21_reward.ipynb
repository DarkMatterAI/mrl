{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward\n",
    "\n",
    "> Rewards - non-differentiable scores for samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Rewards are non-differentiable score functions for evaluating samples. Rewards should generally follow the format `reward = f(sample)`\n",
    "\n",
    "Rewards in MRL occupy five events in the fit loop:\n",
    "- `before_compute_reward` - set up necessary values prior to reward calculation (if needed)\n",
    "- `compute_reward` - compute reward\n",
    "- `after_compute_reward` - compute metrics (if needed)\n",
    "- `reward_modification` - adjust rewards\n",
    "- `after_reward_modification` - compute metrics (if needed)\n",
    "\n",
    "### Rewards vs Reward Modifications\n",
    "\n",
    "MRl breaks rewards up into two phases - rewards and reward modifications. The difference between the two phases is that __reward__ values are saved in the batch log, while __reward_modifications__ are not. \n",
    "\n",
    "In this framework, rewards are absolute scores for samples that are used to evaluate the sample relative to all other samples in the log. Reward modifications are transient scores that depend on the current training context.\n",
    "\n",
    "A reward modification might be something like adding a score bonus to compounds the first time they are created during training to encourage diversity, or penalizing compounds if they appear more than 3 times in the last 5 batches. These types of reward modifications allow us to influence the behavior of the generative model without having these scores effect the true rewards we save in the log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from mrl.imports import *\n",
    "from mrl.core import *\n",
    "from mrl.callbacks import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Callbacks\n",
    "\n",
    "Rewards are used to generate non-differentiable scores from samples. Differentiable scores should be implemented as a `LossCallback`.\n",
    "\n",
    "Rewards are calculated after sample creation but before generating model outputs. Use the `before_compute_reward` event to create any values needed for your reward calculation.\n",
    "\n",
    "\n",
    "Rewards are called after sample creation. At this point in the fit loop, rewards have access to the samples themselves and any other attributes generated during the sample process. Rewards do not have access to model outputs\n",
    "\n",
    "LossCallback\n",
    "\n",
    "before_compute_reward\n",
    "\n",
    "\n",
    "Loss function callbacks compute some loss value from the current batch state and add the resulting value to `BatchState.loss`.\n",
    "\n",
    "`LossCallback` provides a simple hook for custom loss funcions. Any object with a `from_batch_state` method that returns a scalar value can be passed to `LossCallback`. Ex:\n",
    "\n",
    "```\n",
    "class MyLoss():\n",
    "    def from_batch_state(self, batch_state):\n",
    "        loss = self.do_loss_calculation()\n",
    "        return loss\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Reward():\n",
    "    def __init__(self, reward_function, weight=1, bs=None, log=True):\n",
    "        \n",
    "        self.reward_function = reward_function\n",
    "        self.weight = weight\n",
    "        self.bs = bs\n",
    "        self.score_log = {}\n",
    "        self.log = log\n",
    "        \n",
    "    def load_data(self, samples, values):\n",
    "        for i in range(len(samples)):\n",
    "            self.score_log[samples[i]] = values[i]\n",
    "            \n",
    "    def __call__(self, samples):\n",
    "        \n",
    "        rewards = np.array([0. for i in samples])\n",
    "        \n",
    "        to_score = []\n",
    "        to_score_idxs = []\n",
    "        \n",
    "        for i in range(len(samples)):\n",
    "                \n",
    "            if self.log:\n",
    "                if samples[i] in self.score_log:\n",
    "                    rewards[i] = self.score_log[samples[i]]\n",
    "                else:\n",
    "                    to_score.append(samples[i])\n",
    "                    to_score_idxs.append(i)\n",
    "\n",
    "            else:\n",
    "                to_score.append(samples[i])\n",
    "                to_score_idxs.append(i)\n",
    "                    \n",
    "        if to_score:\n",
    "            new_rewards = self.compute_batched_reward(to_score)\n",
    "\n",
    "            for i in range(len(to_score)):\n",
    "                batch_idx = to_score_idxs[i]\n",
    "                reward = new_rewards[i]\n",
    "                rewards[batch_idx] = reward\n",
    "\n",
    "                if self.log:\n",
    "                    self.score_log[to_score[i]] = reward\n",
    "                \n",
    "        rewards = to_device(torch.tensor(rewards).float()).squeeze()\n",
    "        rewards = rewards * self.weight\n",
    "\n",
    "        return rewards\n",
    "            \n",
    "    def _compute_reward(self, samples):\n",
    "        return self.reward_function(samples)\n",
    "    \n",
    "    def compute_batched_reward(self, samples):\n",
    "        if self.bs is not None:\n",
    "            sample_chunks = chunk_list(samples, self.bs)\n",
    "            rewards = []\n",
    "            for chunk in sample_chunks:\n",
    "                rewards_iter = self._compute_reward(chunk)\n",
    "                if isinstance(rewards_iter, torch.Tensor):\n",
    "                    rewards_iter = rewards_iter.detach().cpu()\n",
    "                    \n",
    "                rewards += list(rewards_iter)\n",
    "            \n",
    "        else:\n",
    "            rewards = self._compute_reward(samples)\n",
    "            if isinstance(rewards, torch.Tensor):\n",
    "                rewards = rewards.detach().cpu()\n",
    "            \n",
    "        return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class RewardCallback(Callback):\n",
    "    def __init__(self, reward, name, sample_name='samples',\n",
    "                order=10, track=True):\n",
    "        super().__init__(name=name, order=order)\n",
    "        \n",
    "        self.reward = reward\n",
    "        self.sample_name = sample_name\n",
    "        self.track = track\n",
    "        \n",
    "    def setup(self):\n",
    "        log = self.environment.log\n",
    "        log.add_log(self.name)\n",
    "        if self.track:\n",
    "            log.add_metric(self.name)\n",
    "            \n",
    "    def compute_reward(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        samples = batch_state[self.sample_name]\n",
    "        \n",
    "        if samples:\n",
    "            rewards = self.reward(samples)\n",
    "        else:\n",
    "            rewards = 0.\n",
    "\n",
    "        batch_state.rewards += rewards\n",
    "        batch_state[self.name] = rewards\n",
    "        \n",
    "        if self.track:\n",
    "            env.log.update_metric(self.name, rewards.mean().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class RewardModification(Callback):\n",
    "    def __init__(self, reward, name, sample_name='samples',\n",
    "                order=10, track=True):\n",
    "        super().__init__(name=name, order=order)\n",
    "        \n",
    "        self.reward = reward\n",
    "        self.sample_name = sample_name\n",
    "        self.track = track\n",
    "        \n",
    "    def setup(self):\n",
    "        log = self.environment.log\n",
    "        log.add_log(self.name)\n",
    "        if self.track:\n",
    "            log.add_metric(self.name)\n",
    "            \n",
    "    def reward_modification(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        samples = batch_state[self.sample_name]\n",
    "        \n",
    "        if samples:\n",
    "            rewards = self.reward(samples)\n",
    "        else:\n",
    "            rewards = 0.\n",
    "\n",
    "        batch_state.rewards += rewards\n",
    "        batch_state[self.name] = rewards\n",
    "        \n",
    "        if self.track:\n",
    "            env.log.update_metric(self.name, rewards.mean().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class NoveltyReward(Callback):\n",
    "    def __init__(self, weight=1., track=True):\n",
    "        super().__init__(name='novel')\n",
    "        \n",
    "        self.weight = weight\n",
    "        self.track = track\n",
    "        \n",
    "    def setup(self):\n",
    "        log = self.environment.log\n",
    "        log.add_log(self.name)\n",
    "        if self.track:\n",
    "            log.add_metric(self.name)\n",
    "            \n",
    "    def reward_modification(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        samples = batch_state.samples\n",
    "        \n",
    "        df = env.log.df\n",
    "        new = (~pd.Series(samples).isin(df.samples)).values\n",
    "        \n",
    "        rewards = np.array([float(i) for i in new])*self.weight\n",
    "        rewards = to_device(torch.from_numpy(rewards).float())\n",
    "\n",
    "        batch_state.rewards += rewards\n",
    "        batch_state[self.name] = rewards\n",
    "        \n",
    "        if self.track:\n",
    "            env.log.update_metric(self.name, rewards.mean().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class ContrastiveReward(RewardCallback):\n",
    "    def __init__(self, base_reward, max_score=None):\n",
    "        super().__init__(reward = base_reward.reward,\n",
    "                         name = base_reward.name,\n",
    "                         sample_name = base_reward.sample_name,\n",
    "                         order = base_reward.order,\n",
    "                         track = base_reward.track)\n",
    "        \n",
    "        self.base_reward = base_reward\n",
    "        self.max_score = max_score\n",
    "    \n",
    "    def setup(self):\n",
    "        self.base_reward.environment = self.environment\n",
    "        \n",
    "    def __call__(self, event_name):\n",
    "        \n",
    "        event = getattr(self, event_name, None)\n",
    "        \n",
    "        if event is not None:\n",
    "            output = event()\n",
    "        else:\n",
    "            output = None\n",
    "            \n",
    "        if not event_name=='compute_reward':\n",
    "            _ = self.base_reward(event_name)\n",
    "            \n",
    "        return output\n",
    "        \n",
    "    def compute_and_clean(self, samples):\n",
    "        rewards = self.base_reward.reward(samples)\n",
    "        if isinstance(rewards, torch.Tensor):\n",
    "            rewards = rewards.detach().cpu()\n",
    "            \n",
    "        rewards = np.array(rewards)\n",
    "        return rewards\n",
    "        \n",
    "    def _compute_reward(self, samples):\n",
    "        source_samples = [i[0] for i in samples]\n",
    "        target_samples = [i[1] for i in samples]\n",
    "        \n",
    "        source_rewards = self.compute_and_clean(source_samples)\n",
    "        target_rewards = self.compute_and_clean(target_samples)\n",
    "        \n",
    "        rewards = target_rewards - source_rewards\n",
    "        if self.max_score is not None:\n",
    "            rewards = rewards / (self.max_score-source_rewards)\n",
    "            \n",
    "        rewards = to_device(torch.from_numpy(rewards).float())\n",
    "            \n",
    "        return rewards\n",
    "    \n",
    "    def compute_reward(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        samples = batch_state[self.sample_name]\n",
    "        \n",
    "        rewards = self._compute_reward(samples)\n",
    "        \n",
    "        batch_state.rewards += rewards\n",
    "        batch_state[self.name] = rewards\n",
    "        \n",
    "        if self.track:\n",
    "            env.log.update_metric(self.name, rewards.mean().detach().cpu().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mrl)",
   "language": "python",
   "name": "mrl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
