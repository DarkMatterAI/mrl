{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward\n",
    "\n",
    "> Rewards - non-differentiable scores for samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from mrl.imports import *\n",
    "from mrl.core import *\n",
    "from mrl.callbacks import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Reward(Callback):\n",
    "    def __init__(self, name, sample_name='samples', \n",
    "                 weight=1., bs=None,\n",
    "                 order=10, track=True, log=True):\n",
    "        super().__init__(name=name, order=order)\n",
    "        \n",
    "        self.sample_name = sample_name\n",
    "        self.weight = weight\n",
    "        self.bs = bs\n",
    "        self.track = track\n",
    "        self.log = log\n",
    "        self.score_log = {}\n",
    "        \n",
    "    def load_data(self, samples, values):\n",
    "        for i in range(len(samples)):\n",
    "            self.score_log[samples[i]] = values[i]\n",
    "        \n",
    "    def setup(self):\n",
    "        log = self.environment.log\n",
    "        log.add_log(self.name)\n",
    "        if self.track:\n",
    "            log.add_metric(self.name)\n",
    "            \n",
    "    def _compute_reward(self, samples):\n",
    "        return [0. for i in samples]\n",
    "    \n",
    "    def compute_batched_reward(self, samples):\n",
    "        if self.bs is not None:\n",
    "            sample_chunks = chunk_list(samples, self.bs)\n",
    "            rewards = []\n",
    "            for chunk in sample_chunks:\n",
    "                rewards_iter = self._compute_reward(chunk)\n",
    "                if isinstance(rewards_iter, torch.Tensor):\n",
    "                    rewards_iter = rewards_iter.detach().cpu()\n",
    "                    \n",
    "                rewards += list(rewards_iter)\n",
    "            \n",
    "        else:\n",
    "            rewards = self._compute_reward(samples)\n",
    "            if isinstance(rewards, torch.Tensor):\n",
    "                rewards = rewards.detach().cpu()\n",
    "            \n",
    "        return rewards\n",
    "    \n",
    "    def compute_reward(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        samples = batch_state[self.sample_name]\n",
    "#         prescored = batch_state.prescored\n",
    "        \n",
    "        rewards = np.array([0. for i in samples])\n",
    "        \n",
    "        to_score = []\n",
    "        to_score_idxs = []\n",
    "        \n",
    "        for i in range(len(samples)):\n",
    "#             if not prescored[i]:\n",
    "                \n",
    "            if self.log:\n",
    "                if samples[i] in self.score_log:\n",
    "                    rewards[i] = self.score_log[samples[i]]\n",
    "                else:\n",
    "                    to_score.append(samples[i])\n",
    "                    to_score_idxs.append(i)\n",
    "\n",
    "            else:\n",
    "                to_score.append(samples[i])\n",
    "                to_score_idxs.append(i)\n",
    "                    \n",
    "        new_rewards = self.compute_batched_reward(to_score)\n",
    "        \n",
    "        for i in range(len(to_score)):\n",
    "            batch_idx = to_score_idxs[i]\n",
    "            reward = new_rewards[i]\n",
    "            rewards[batch_idx] = reward\n",
    "            \n",
    "            if self.log:\n",
    "                self.score_log[to_score[i]] = reward\n",
    "                \n",
    "        rewards = to_device(torch.tensor(rewards).float()).squeeze()\n",
    "        rewards = rewards * self.weight\n",
    "        \n",
    "        batch_state.rewards += rewards\n",
    "        batch_state[self.name] = rewards\n",
    "        \n",
    "        if self.track:\n",
    "            env.log.update_metric(self.name, rewards.mean().detach().cpu().numpy())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class RewardModification(Callback):\n",
    "    def __init__(self, name, sample_name='samples', \n",
    "                 weight=1., bs=None,\n",
    "                 order=10, track=True):\n",
    "        super().__init__(name=name, order=order)\n",
    "        \n",
    "        self.sample_name = sample_name\n",
    "        self.weight = weight\n",
    "        self.bs = bs\n",
    "        self.track = track\n",
    "        \n",
    "    def setup(self):\n",
    "        log = self.environment.log\n",
    "        log.add_log(self.name)\n",
    "        if self.track:\n",
    "            log.add_metric(self.name)\n",
    "            \n",
    "    def _compute_reward_modification(self, samples):\n",
    "        return [0. for i in samples]\n",
    "    \n",
    "    def compute_batched_reward_modification(self, samples):\n",
    "        if self.bs is not None:\n",
    "            sample_chunks = chunk_list(samples, self.bs)\n",
    "            rewards = []\n",
    "            for chunk in sample_chunks:\n",
    "                rewards_iter = self._compute_reward_modification(chunk)\n",
    "                if isinstance(rewards_iter, torch.Tensor):\n",
    "                    rewards_iter = rewards_iter.detach().cpu()\n",
    "                    \n",
    "                rewards += list(rewards_iter)\n",
    "            \n",
    "        else:\n",
    "            rewards = self._compute_reward_modification(samples)\n",
    "            if isinstance(rewards, torch.Tensor):\n",
    "                rewards = rewards.detach().cpu()\n",
    "            \n",
    "        return rewards\n",
    "    \n",
    "    def reward_modification(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        samples = batch_state[self.sample_name]\n",
    "        \n",
    "        rewards = self.compute_batched_reward_modification(samples)\n",
    "                \n",
    "        rewards = to_device(torch.tensor(rewards).float()).squeeze()\n",
    "        rewards = rewards * self.weight\n",
    "        \n",
    "        batch_state.rewards_final += rewards\n",
    "        batch_state[self.name] = rewards\n",
    "        \n",
    "        if self.track:\n",
    "            env.log.update_metric(self.name, rewards.mean().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class ContrastiveReward(Reward):\n",
    "    def __init__(self, base_reward, max_score=None):\n",
    "        super().__init__(name = base_reward.name,\n",
    "                         sample_name = base_reward.sample_name,\n",
    "                         weight = base_reward.weight,\n",
    "                         bs = base_reward.bs,\n",
    "                         order = base_reward.order,\n",
    "                         track = base_reward.track,\n",
    "                         log = base_reward.log)\n",
    "        \n",
    "        self.base_reward = base_reward\n",
    "        self.max_score = max_score\n",
    "        \n",
    "        \n",
    "    def __call__(self, event_name):\n",
    "        \n",
    "        event = getattr(self, event_name, None)\n",
    "        \n",
    "        if event is not None:\n",
    "            output = event()\n",
    "        else:\n",
    "            output = None\n",
    "            \n",
    "        if not event_name=='compute_reward':\n",
    "            _ = self.base_reward(event_name)\n",
    "            \n",
    "        return output\n",
    "        \n",
    "    def compute_and_clean(self, samples):\n",
    "        rewards = self.base_reward._compute_reward(samples)\n",
    "        if isinstance(rewards, torch.Tensor):\n",
    "            rewards = rewards.detach().cpu()\n",
    "            \n",
    "        rewards = np.array(rewards)\n",
    "        return rewards\n",
    "        \n",
    "    def _compute_reward(self, samples):\n",
    "        source_samples = [i[0] for i in samples]\n",
    "        target_samples = [i[1] for i in samples]\n",
    "        \n",
    "        source_rewards = self.compute_and_clean(source_samples)\n",
    "        target_rewards = self.compute_and_clean(target_samples)\n",
    "        \n",
    "        rewards = target_rewards - source_rewards\n",
    "        if self.max_score is not None:\n",
    "            rewards = rewards / (self.max_score-source_rewards)\n",
    "            \n",
    "        return list(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class FunctionReward(Reward):\n",
    "    def __init__(self, reward_function, name, sample_name='samples', \n",
    "                 weight=1., bs=None,\n",
    "                 order=10, track=True, log=True):\n",
    "        super().__init__(name=name, \n",
    "                         sample_name=sample_name, \n",
    "                         weight=weight,\n",
    "                         bs=bs,\n",
    "                         order=order, \n",
    "                         track=track, \n",
    "                         log=log)\n",
    "        \n",
    "        self.reward_function = reward_function\n",
    "        \n",
    "        \n",
    "    def _compute_reward(self, samples):\n",
    "        rewards = []\n",
    "        if samples:\n",
    "            rewards = self.reward_function(samples)\n",
    "        return rewards\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class NoveltyReward(RewardModification):\n",
    "    def __init__(self, weight=1., track=True):\n",
    "        super().__init__(name='novel', \n",
    "                         sample_name='samples',\n",
    "                         weight=weight, \n",
    "                         order=10, \n",
    "                         track=track)\n",
    "        \n",
    "        \n",
    "    def _compute_reward_modification(self, samples):\n",
    "        \n",
    "        df = self.environment.log.df\n",
    "        \n",
    "        new = (~pd.Series(samples).isin(df.samples)).values\n",
    "\n",
    "        new = [float(i) for i in new]\n",
    "        return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mrl)",
   "language": "python",
   "name": "mrl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
