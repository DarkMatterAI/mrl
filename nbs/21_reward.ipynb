{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward\n",
    "\n",
    "> Rewards - non-differentiable scores for samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from mrl.imports import *\n",
    "from mrl.core import *\n",
    "from mrl.callbacks import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Reward():\n",
    "    def __init__(self, reward_function, weight=1, bs=None, log=True):\n",
    "        \n",
    "        self.reward_function = reward_function\n",
    "        self.weight = weight\n",
    "        self.bs = bs\n",
    "        self.score_log = {}\n",
    "        self.log = log\n",
    "        \n",
    "    def load_data(self, samples, values):\n",
    "        for i in range(len(samples)):\n",
    "            self.score_log[samples[i]] = values[i]\n",
    "            \n",
    "    def __call__(self, samples):\n",
    "        \n",
    "        rewards = np.array([0. for i in samples])\n",
    "        \n",
    "        to_score = []\n",
    "        to_score_idxs = []\n",
    "        \n",
    "        for i in range(len(samples)):\n",
    "                \n",
    "            if self.log:\n",
    "                if samples[i] in self.score_log:\n",
    "                    rewards[i] = self.score_log[samples[i]]\n",
    "                else:\n",
    "                    to_score.append(samples[i])\n",
    "                    to_score_idxs.append(i)\n",
    "\n",
    "            else:\n",
    "                to_score.append(samples[i])\n",
    "                to_score_idxs.append(i)\n",
    "                    \n",
    "        if to_score:\n",
    "            new_rewards = self.compute_batched_reward(to_score)\n",
    "\n",
    "            for i in range(len(to_score)):\n",
    "                batch_idx = to_score_idxs[i]\n",
    "                reward = new_rewards[i]\n",
    "                rewards[batch_idx] = reward\n",
    "\n",
    "                if self.log:\n",
    "                    self.score_log[to_score[i]] = reward\n",
    "                \n",
    "        rewards = to_device(torch.tensor(rewards).float()).squeeze()\n",
    "        rewards = rewards * self.weight\n",
    "\n",
    "        return rewards\n",
    "            \n",
    "    def _compute_reward(self, samples):\n",
    "        return self.reward_function(samples)\n",
    "    \n",
    "    def compute_batched_reward(self, samples):\n",
    "        if self.bs is not None:\n",
    "            sample_chunks = chunk_list(samples, self.bs)\n",
    "            rewards = []\n",
    "            for chunk in sample_chunks:\n",
    "                rewards_iter = self._compute_reward(chunk)\n",
    "                if isinstance(rewards_iter, torch.Tensor):\n",
    "                    rewards_iter = rewards_iter.detach().cpu()\n",
    "                    \n",
    "                rewards += list(rewards_iter)\n",
    "            \n",
    "        else:\n",
    "            rewards = self._compute_reward(samples)\n",
    "            if isinstance(rewards, torch.Tensor):\n",
    "                rewards = rewards.detach().cpu()\n",
    "            \n",
    "        return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class RewardCallback(Callback):\n",
    "    def __init__(self, reward, name, sample_name='samples',\n",
    "                order=10, track=True):\n",
    "        super().__init__(name=name, order=order)\n",
    "        \n",
    "        self.reward = reward\n",
    "        self.sample_name = sample_name\n",
    "        self.track = track\n",
    "        \n",
    "    def setup(self):\n",
    "        log = self.environment.log\n",
    "        log.add_log(self.name)\n",
    "        if self.track:\n",
    "            log.add_metric(self.name)\n",
    "            \n",
    "    def compute_reward(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        samples = batch_state[self.sample_name]\n",
    "        \n",
    "        if samples:\n",
    "            rewards = self.reward(samples)\n",
    "        else:\n",
    "            rewards = 0.\n",
    "\n",
    "        batch_state.rewards += rewards\n",
    "        batch_state[self.name] = rewards\n",
    "        \n",
    "        if self.track:\n",
    "            env.log.update_metric(self.name, rewards.mean().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class RewardModification(Callback):\n",
    "    def __init__(self, reward, name, sample_name='samples',\n",
    "                order=10, track=True):\n",
    "        super().__init__(name=name, order=order)\n",
    "        \n",
    "        self.reward = reward\n",
    "        self.sample_name = sample_name\n",
    "        self.track = track\n",
    "        \n",
    "    def setup(self):\n",
    "        log = self.environment.log\n",
    "        log.add_log(self.name)\n",
    "        if self.track:\n",
    "            log.add_metric(self.name)\n",
    "            \n",
    "    def reward_modification(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        samples = batch_state[self.sample_name]\n",
    "        \n",
    "        if samples:\n",
    "            rewards = self.reward(samples)\n",
    "        else:\n",
    "            rewards = 0.\n",
    "\n",
    "        batch_state.rewards += rewards\n",
    "        batch_state[self.name] = rewards\n",
    "        \n",
    "        if self.track:\n",
    "            env.log.update_metric(self.name, rewards.mean().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class NoveltyReward(Callback):\n",
    "    def __init__(self, weight=1., track=True):\n",
    "        super().__init__(name='novel')\n",
    "        \n",
    "        self.weight = weight\n",
    "        self.track = track\n",
    "        \n",
    "    def setup(self):\n",
    "        log = self.environment.log\n",
    "        log.add_log(self.name)\n",
    "        if self.track:\n",
    "            log.add_metric(self.name)\n",
    "            \n",
    "    def reward_modification(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        samples = batch_state.samples\n",
    "        \n",
    "        df = env.log.df\n",
    "        new = (~pd.Series(samples).isin(df.samples)).values\n",
    "        \n",
    "        rewards = np.array([float(i) for i in new])*self.weight\n",
    "        rewards = to_device(torch.from_numpy(rewards).float())\n",
    "\n",
    "        batch_state.rewards += rewards\n",
    "        batch_state[self.name] = rewards\n",
    "        \n",
    "        if self.track:\n",
    "            env.log.update_metric(self.name, rewards.mean().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class ContrastiveReward(RewardCallback):\n",
    "    def __init__(self, base_reward, max_score=None):\n",
    "        super().__init__(reward = base_reward.reward,\n",
    "                         name = base_reward.name,\n",
    "                         sample_name = base_reward.sample_name,\n",
    "                         order = base_reward.order,\n",
    "                         track = base_reward.track)\n",
    "        \n",
    "        self.base_reward = base_reward\n",
    "        self.max_score = max_score\n",
    "    \n",
    "    def setup(self):\n",
    "        self.base_reward.environment = self.environment\n",
    "        \n",
    "    def __call__(self, event_name):\n",
    "        \n",
    "        event = getattr(self, event_name, None)\n",
    "        \n",
    "        if event is not None:\n",
    "            output = event()\n",
    "        else:\n",
    "            output = None\n",
    "            \n",
    "        if not event_name=='compute_reward':\n",
    "            _ = self.base_reward(event_name)\n",
    "            \n",
    "        return output\n",
    "        \n",
    "    def compute_and_clean(self, samples):\n",
    "        rewards = self.base_reward.reward(samples)\n",
    "        if isinstance(rewards, torch.Tensor):\n",
    "            rewards = rewards.detach().cpu()\n",
    "            \n",
    "        rewards = np.array(rewards)\n",
    "        return rewards\n",
    "        \n",
    "    def _compute_reward(self, samples):\n",
    "        source_samples = [i[0] for i in samples]\n",
    "        target_samples = [i[1] for i in samples]\n",
    "        \n",
    "        source_rewards = self.compute_and_clean(source_samples)\n",
    "        target_rewards = self.compute_and_clean(target_samples)\n",
    "        \n",
    "        rewards = target_rewards - source_rewards\n",
    "        if self.max_score is not None:\n",
    "            rewards = rewards / (self.max_score-source_rewards)\n",
    "            \n",
    "        rewards = to_device(torch.from_numpy(rewards).float())\n",
    "            \n",
    "        return rewards\n",
    "    \n",
    "    def compute_reward(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        samples = batch_state[self.sample_name]\n",
    "        \n",
    "        rewards = self._compute_reward(samples)\n",
    "        \n",
    "        batch_state.rewards += rewards\n",
    "        batch_state[self.name] = rewards\n",
    "        \n",
    "        if self.track:\n",
    "            env.log.update_metric(self.name, rewards.mean().detach().cpu().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mrl)",
   "language": "python",
   "name": "mrl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
