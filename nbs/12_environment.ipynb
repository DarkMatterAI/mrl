{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c92780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a774717",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "> Environment/Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0482fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d9b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/envs/mrl/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for boost::shared_ptr<RDKit::FilterCatalogEntry const> already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "from mrl.imports import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *\n",
    "from mrl.chem import *\n",
    "from mrl.templates import *\n",
    "from mrl.agent import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae552783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Callback():\n",
    "    def __init__(self, name='callback', order=1000):\n",
    "        self.order=order\n",
    "        self.name = name\n",
    "    \n",
    "    def __call__(self, event_name):\n",
    "        \n",
    "        event = getattr(self, event_name, None)\n",
    "        if event is not None:\n",
    "            output = event()\n",
    "        else:\n",
    "            output = None\n",
    "            \n",
    "        return output\n",
    "    \n",
    "class BatchStats(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__(name='stats', order=0)\n",
    "        \n",
    "        self.pbar = None\n",
    "        self.iterations = 0\n",
    "        self.diversity = []\n",
    "        self.valid = []\n",
    "        self.rewards = []\n",
    "        self.metric_vals = ['iterations', 'diversity',\n",
    "                            'valid', 'rewards']\n",
    "        \n",
    "    def before_train(self):\n",
    "        if self.pbar is None:\n",
    "            print('\\t'.join([key for key in self.metric_vals]))\n",
    "        else:\n",
    "            self.pbar.write(self.metric_vals, table=True)\n",
    "        \n",
    "    def add_metric(self, name):\n",
    "        setattr(self, name, [])\n",
    "        self.metric_vals.append(name)\n",
    "        \n",
    "    def after_batch(self):\n",
    "        outputs = []\n",
    "        for metric in self.metric_vals:\n",
    "            v = getattr(self, metric)\n",
    "            if type(v)==list:\n",
    "                val = v[-1]\n",
    "            else:\n",
    "                val = v\n",
    "                \n",
    "            if type(val)==int:\n",
    "                val = f'{val}'\n",
    "            else:\n",
    "                val = f'{val:.2f}'\n",
    "                \n",
    "            outputs.append(val)\n",
    "            \n",
    "        if self.pbar is None:\n",
    "            print('\\t'.join(outputs))\n",
    "        else:\n",
    "            self.pbar.write(outputs, table=True)\n",
    "            \n",
    "        self.iterations += 1\n",
    "    \n",
    "class Buffer(Callback):\n",
    "    def __init__(self, p_total, max_size=1000000):\n",
    "        super().__init__(name='buffer', order=0)\n",
    "        \n",
    "        self.buffer = []\n",
    "        self.used_buffer = []\n",
    "        self.max_size = max_size\n",
    "        self.p_total = p_total\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def add(self, item):\n",
    "        \n",
    "        if is_container(item):\n",
    "            for i in item:\n",
    "                self.add(i)\n",
    "        else:\n",
    "            place_idx = (len(self.buffer)%self.max_size)+1\n",
    "\n",
    "            if place_idx>=len(self.buffer):\n",
    "                self.buffer.append(item)\n",
    "            else:\n",
    "                self.buffer[place_idx-1] = item\n",
    "            \n",
    "    def sample(self, n):\n",
    "        \n",
    "        idxs = np.random.choice(np.arange(len(self.buffer)), n, replace=False)\n",
    "        batch = [self.buffer[i] for i in idxs]\n",
    "        for idx in sorted(idxs, reverse=True):\n",
    "            self.buffer.pop(idx)\n",
    "            \n",
    "        self.used_buffer += batch\n",
    "        self.used_buffer = self.used_buffer[-self.max_size:]\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    def after_build_buffer(self):\n",
    "        self.buffer = self.environment.template_cb.filter_sequences(self.buffer)\n",
    "    \n",
    "    def sample_batch(self):\n",
    "        bs = int(self.environment.bs * self.p_total)\n",
    "        sample = self.sample(bs)\n",
    "        self.batch_state.samples += sample\n",
    "        self.batch_state.sources += ['buffer']*len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69c98ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class SettrDict(dict):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def __setitem__(self, key, item):\n",
    "        super().__setitem__(key, item)\n",
    "        super().__setattr__(key, item)\n",
    "    \n",
    "    def __setattr__(self, key, item):\n",
    "        super().__setitem__(key, item)\n",
    "        super().__setattr__(key, item)\n",
    "        \n",
    "    def update_from_dict(self, update_dict):\n",
    "        for k,v in update_dict.items():\n",
    "            self[k] = v\n",
    "        \n",
    "class BatchState(SettrDict):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.samples = []\n",
    "        self.sources = []\n",
    "        self.rewards = to_device(torch.tensor(0.))\n",
    "        self.rewards_scaled = to_device(torch.tensor(0.))\n",
    "        self.trajectory_rewards = to_device(torch.tensor(0.))\n",
    "        self.loss = to_device(torch.tensor(0.))\n",
    "        self.latent_data = []\n",
    "        \n",
    "#         self.sequence_trajectories = []\n",
    "#         self.x = None\n",
    "#         self.y = None\n",
    "#         self.mask = None\n",
    "#         self.sl = None\n",
    "#         self.model_output = None\n",
    "#         self.model_encoded = None\n",
    "#         self.model_logprobs = None\n",
    "#         self.model_gathered_logprobs = None\n",
    "#         self.y_gumbel = None\n",
    "#         self.vhead_values = None\n",
    "#         self.old_vhead_values = None\n",
    "#         self.ref_output = None\n",
    "#         self.ref_encoded = None\n",
    "#         self.ref_logprobs = None\n",
    "#         self.ref_gathered_logprobs = None\n",
    "#         self.trajectory_rewards = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb6842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Event():\n",
    "    def __init__(self):\n",
    "        self.setup = 'setup'\n",
    "        self.before_train = 'before_train'\n",
    "        self.build_buffer = 'build_buffer'\n",
    "        self.after_build_buffer = 'after_build_buffer'\n",
    "        self.before_batch = 'before_batch'\n",
    "        self.sample_batch = 'sample_batch'\n",
    "        self.after_sample = 'after_sample'\n",
    "        self.get_model_outputs = 'get_model_outputs'\n",
    "        self.compute_reward = 'compute_reward'\n",
    "        self.after_compute_reward = 'after_compute_reward'\n",
    "        self.compute_loss = 'compute_loss'\n",
    "        self.zero_grad = 'zero_grad'\n",
    "        self.step = 'step'\n",
    "        self.after_batch = 'after_batch'\n",
    "        self.after_train = 'after_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b05ca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Environment():\n",
    "    def __init__(self, agent_cb, template=None, samplers=[], reward_cbs=[], loss_cbs=[], cbs=[],\n",
    "                buffer_p_batch=None, reward_decay=0.9):\n",
    "        self.agent_cb = agent_cb\n",
    "        self.template_cb = TemplateCallback(template)\n",
    "        self.samplers = samplers\n",
    "        self.reward_cbs = reward_cbs\n",
    "        self.loss_cbs = loss_cbs\n",
    "        self.cbs = []\n",
    "        if buffer_p_batch is None:\n",
    "            buffer_p_batch = 1.\n",
    "            for samp in samplers:\n",
    "                buffer_p_batch -= samp.p_batch\n",
    "        self.buffer = Buffer(buffer_p_batch)\n",
    "        self.batch_state = BatchState()\n",
    "        self.batch_stats = BatchStats()\n",
    "        self.mean_reward = None\n",
    "        self.reward_decay = reward_decay\n",
    "        \n",
    "        all_cbs = [self.agent_cb] + [self.template_cb] + self.samplers + self.reward_cbs\n",
    "        all_cbs += self.loss_cbs + cbs + [self.buffer] + [self.batch_stats]\n",
    "        \n",
    "        self.register_cbs(all_cbs)\n",
    "        self('setup')\n",
    "        \n",
    "    def __call__(self, event):\n",
    "        for cb in self.cbs:\n",
    "            if hasattr(cb, event):\n",
    "                cb(event)\n",
    "        \n",
    "    def register_cb(self, cb):\n",
    "        if isinstance(cb, type): \n",
    "            cb = cb()\n",
    "        cb.environment = self\n",
    "        setattr(self, cb.name, cb)\n",
    "        self.cbs.append(cb)\n",
    "        \n",
    "    def register_cbs(self, cbs):\n",
    "        for cb in cbs:\n",
    "            self.register_cb(cb)\n",
    "            \n",
    "    def build_buffer(self):\n",
    "        if len(self.buffer) < self.bs:\n",
    "            self('build_buffer')\n",
    "            self('after_build_buffer')\n",
    "            \n",
    "    def sample_batch(self):\n",
    "        self.batch_state = BatchState()\n",
    "        for cb in self.cbs:\n",
    "            cb.batch_state = self.batch_state\n",
    "        self('before_batch') \n",
    "        self('sample_batch') \n",
    "        sequences = self.batch_state.samples\n",
    "        self.batch_stats.diversity.append(len(set(sequences))/len(sequences))\n",
    "        self.batch_stats.valid.append(\n",
    "            len([i for i in sequences if to_mol(i) is not None])/len(sequences))\n",
    "        self('after_sample') \n",
    "        \n",
    "    def compute_reward(self):\n",
    "        self('compute_reward')\n",
    "        rewards = self.batch_state.rewards\n",
    "        \n",
    "        if self.mean_reward is None:\n",
    "            self.mean_reward = rewards.mean()\n",
    "        else:\n",
    "            self.mean_reward = (1-self.reward_decay)*rewards.mean() + self.reward_decay*self.mean_reward\n",
    "            \n",
    "        rewards_scaled = rewards - self.mean_reward\n",
    "        self.batch_state.rewards_scaled = rewards_scaled\n",
    "        self.batch_stats.rewards.append(rewards.mean().detach().cpu().numpy())\n",
    "        self('after_compute_reward')\n",
    "        \n",
    "    def compute_loss(self):\n",
    "        self('compute_loss')\n",
    "        loss = self.batch_state.loss\n",
    "        self('zero_grad')\n",
    "        loss.backward()\n",
    "        self('step')\n",
    "            \n",
    "    def fit(self, bs, sl, iters, buffer_size):\n",
    "        self.bs = bs\n",
    "        self.sl = sl\n",
    "        self.buffer_size = buffer_size\n",
    "        mb = master_bar(range(iters))\n",
    "        self.batch_stats.pbar = mb\n",
    "        self('before_train')\n",
    "        for step in mb:\n",
    "            self.build_buffer()\n",
    "            self.sample_batch()\n",
    "            self('get_model_outputs')\n",
    "            self.compute_reward()\n",
    "            self.compute_loss()\n",
    "            self('after_batch')\n",
    "        self('after_train')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f67a193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ad67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Sampler(Callback):\n",
    "    def __init__(self, name, p_buffer=0., p_batch=0.):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.p_buffer = p_buffer\n",
    "        self.p_batch = p_batch\n",
    "        \n",
    "    def setup(self):\n",
    "        if self.p_batch>0.:\n",
    "            bs = self.environment.batch_stats\n",
    "            bs.add_metric(f'{self.name}_diversity')\n",
    "            bs.add_metric(f'{self.name}_valid')\n",
    "            bs.add_metric(f'{self.name}_rewards')\n",
    "#             setattr(bs, f'{self.name}_diversity', [])\n",
    "#             setattr(bs, f'{self.name}_valid', [])\n",
    "#             setattr(bs, f'{self.name}_rewards', [])\n",
    "#             setattr(bs, f'{self.name}_new', [])\n",
    "        \n",
    "    def build_buffer(self):\n",
    "        pass\n",
    "    \n",
    "    def sample_batch(self):\n",
    "        pass\n",
    "    \n",
    "    def after_compute_reward(self):\n",
    "        if self.p_batch>0:\n",
    "            state = self.environment.batch_state\n",
    "            stats = self.environment.batch_stats\n",
    "            rewards = state.rewards.detach().cpu().numpy()\n",
    "            sources = np.array(state.sources)\n",
    "            if self.name in sources:\n",
    "                getattr(stats, f'{self.name}_rewards').append(rewards[sources==self.name].mean())\n",
    "            else:\n",
    "                getattr(stats, f'{self.name}_rewards').append(0.)\n",
    "                  \n",
    "    \n",
    "class ModelSampler(Sampler):\n",
    "    def __init__(self, agent, model, name, p_buffer, p_batch, genbatch, latent=False):\n",
    "        super().__init__(name, p_buffer, p_batch)\n",
    "        self.agent = agent\n",
    "        self.model = model\n",
    "        self.genbatch = genbatch\n",
    "        self.latent = latent if self.agent.latents is not None else False\n",
    "        \n",
    "    def build_buffer(self):\n",
    "        env = self.environment\n",
    "        bs = int(env.buffer_size * self.p_buffer)\n",
    "        outputs = []\n",
    "        to_generate = bs\n",
    "        \n",
    "        if bs > 0:\n",
    "            for batch in range(int(np.ceil(bs/self.genbatch))):\n",
    "                current_bs = min(self.genbatch, to_generate)\n",
    "                \n",
    "                preds, _ = self.model.sample_no_grad(current_bs, env.sl, multinomial=True)\n",
    "                sequences = self.agent.reconstruct(preds)\n",
    "                sequences = list(set(sequences))\n",
    "                sequences = [i for i in sequences if to_mol(i) is not None]\n",
    "                outputs += sequences\n",
    "                outputs = list(set(outputs))\n",
    "                to_generate = bs - len(outputs)\n",
    "                \n",
    "            env.buffer.add(outputs)\n",
    "            \n",
    "            \n",
    "    def sample_batch(self):\n",
    "        env = self.environment\n",
    "        bs = int(env.bs * self.p_batch)\n",
    "        \n",
    "        if bs > 0:\n",
    "            \n",
    "            if self.latent:\n",
    "                latents = self.agent.latents\n",
    "                latent_idxs = torch.randint(0, latents.shape[0]-1, bs)\n",
    "                sample_latents = latents[latent_idxs]\n",
    "                self.batch_state.latent_data.append([self.name, latent_idxs])\n",
    "            else:\n",
    "                sample_latents=None\n",
    "            \n",
    "            \n",
    "            preds, _ = self.model.sample_no_grad(bs, env.sl, z=sample_latents, multinomial=True)\n",
    "            sequences = self.agent.reconstruct(preds)\n",
    "            diversity = len(set(sequences))/len(sequences)\n",
    "            valid = np.array([to_mol(i) is not None for i in sequences])\n",
    "            getattr(env.batch_stats, f\"{self.name}_diversity\").append(diversity)\n",
    "            getattr(env.batch_stats, f\"{self.name}_valid\").append(valid.mean())\n",
    "            self.batch_state.samples += sequences\n",
    "            self.batch_state.sources += [self.name]*len(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a232c5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d5a9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class TemplateCallback(Callback):\n",
    "    def __init__(self, template=None):\n",
    "        super().__init__(order=-1)\n",
    "        self.template = template\n",
    "        self.name = 'template'\n",
    "        \n",
    "    def compute_reward(self):\n",
    "        env = self.environment\n",
    "        state = env.batch_state\n",
    "        \n",
    "        if self.template is not None:\n",
    "            rewards = np.array(self.template.eval_mols(state.samples))\n",
    "            hps = np.array(self.template(state.samples))\n",
    "        else:\n",
    "            rewards = np.array([0.]*len(state.samples))\n",
    "            hps = np.array([0.]*len(state.samples))\n",
    "        \n",
    "        state.template_rewards = rewards\n",
    "        state.template_passes = hps\n",
    "        state.rewards += to_device(torch.from_numpy(rewards).float())\n",
    "        \n",
    "    def filter_sequences(self, sequences):\n",
    "        if self.template is not None:\n",
    "            hp = np.array(self.template(sequences))\n",
    "            sequences = np.array(sequences)[hp]\n",
    "            sequences = list(sequences)\n",
    "            \n",
    "        return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115f91a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9cc324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class AgentCallback(Callback):\n",
    "    def __init__(self, agent, name):\n",
    "        super().__init__()\n",
    "        self.agent = agent\n",
    "        self.name = name\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.agent.zero_grad()\n",
    "        \n",
    "    def step(self):\n",
    "        self.agent.step()\n",
    "        \n",
    "    def after_sample(self):\n",
    "        # convert samples to tensors\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def get_model_outputs(self):\n",
    "        # get relevant model outputs\n",
    "        raise NotImplementedError\n",
    "        \n",
    "class GenAgentCallback(AgentCallback):\n",
    "    def __init__(self, agent, name, contrastive=False):\n",
    "        super().__init__(agent, name)\n",
    "        self.contrastive = contrastive\n",
    "    \n",
    "    def after_sample(self):\n",
    "        \n",
    "        batch_ds = self.agent.dataset.new(self.batch_state.samples)\n",
    "        batch = batch_ds.collate_function([batch_ds[i] for i in range(len(batch_ds))])\n",
    "        x,y = batch\n",
    "        \n",
    "        self.batch_state.x = x\n",
    "        self.batch_state.y = y\n",
    "        mask = ~(y==self.agent.vocab.stoi['pad'])\n",
    "        self.batch_state.mask = mask\n",
    "        self.batch_state.lengths = mask.sum(-1)\n",
    "        self.batch_state.sl = y.shape[-1]\n",
    "        self.batch_state.sequence_trajectories = self.agent.reconstruct_trajectory(y)\n",
    "        self.batch_state.rewards = to_device(torch.zeros(x.shape[0]))\n",
    "        self.batch_state.rewards_scaled = to_device(torch.zeros(x.shape[0]))\n",
    "        self.batch_state.trajectory_rewards = to_device(torch.zeros(y.shape))\n",
    "        \n",
    "    def get_model_outputs(self):\n",
    "            \n",
    "        x = self.batch_state.x\n",
    "        y = self.batch_state.y\n",
    "        sources = self.batch_state.sources\n",
    "        latent_info = self.batch_state.latent_data\n",
    "        \n",
    "        if latent_info:\n",
    "            latent_sources = []\n",
    "            output_tensors = []\n",
    "            for (latent_source, latent_idxs) in latent_info:\n",
    "                latent_sources.append(latent_source)\n",
    "                latent_mask = torch.tensor([i==latent_source for i in sources]).bool()\n",
    "                latents = self.agent.latents[latent_idxs]\n",
    "                out = self.agent.model.get_rl_tensors(x[latent_mask], y[latent_mask],\n",
    "                                                      latents=latents)\n",
    "                output_tensors.append(out)\n",
    "                \n",
    "            non_latent_mask = torch.tensor([not i in latent_sources for i in sources]).bool()\n",
    "            out = self.agent.model.get_rl_tensors(x[non_latent_mask], y[non_latent_mask])\n",
    "            output_tensors.append(out)\n",
    "            \n",
    "            mo = torch.cat([i[0] for i in output_tensors], 0)\n",
    "            mlp = torch.cat([i[1] for i in output_tensors], 0)\n",
    "            mglp = torch.cat([i[2] for i in output_tensors], 0)\n",
    "            me = torch.cat([i[3] for i in output_tensors], 0)\n",
    "            \n",
    "        else:\n",
    "            mo, mlp, mglp, me = self.agent.model.get_rl_tensors(x,y)\n",
    "            \n",
    "        mprob = mlp.exp()\n",
    "        \n",
    "        self.batch_state.model_output = mo\n",
    "        self.batch_state.model_logprobs = mlp\n",
    "        self.batch_state.model_gathered_logprobs = mglp\n",
    "        self.batch_state.model_encoded = me\n",
    "        self.batch_state.y_gumbel = F.one_hot(y, len(self.agent.vocab.itos)) + mprob - mprob.detach()\n",
    "        \n",
    "        if self.agent.value_head is not None:\n",
    "            value_predictions = self.agent.value_head(me)\n",
    "            with torch.no_grad():\n",
    "                base_value_predictions = self.agent.base_value_head(me)\n",
    "        else:\n",
    "            value_predictions = None\n",
    "            base_value_predictions = None\n",
    "            \n",
    "        self.batch_state.state_values = value_predictions\n",
    "        self.batch_state.ref_state_values = base_value_predictions\n",
    "        \n",
    "        if self.agent.base_model is not None:\n",
    "            with torch.no_grad():\n",
    "                bo, blp, bglp, be = self.agent.base_model.get_rl_tensors(x,y)\n",
    "        else:\n",
    "            bo, blp, bglp, be = None, None, None, None\n",
    "            \n",
    "        self.batch_state.reference_output = bo\n",
    "        self.batch_state.reference_logprobs = blp\n",
    "        self.batch_state.reference_gathered_logprobs = bglp\n",
    "        self.batch_state.reference_encoded = be\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87e61f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class RewardCallback(Callback):\n",
    "    def __init__(self, reward_function, name, weight=1.):\n",
    "        super().__init__(order=1)\n",
    "        self.name = name\n",
    "        self.reward_function = reward_function\n",
    "        self.weight = weight\n",
    "        \n",
    "    def setup(self):\n",
    "        bs = self.environment.batch_stats\n",
    "        bs.add_metric(self.name)\n",
    "#         setattr(bs, self.name, [])\n",
    "        \n",
    "    def compute_reward(self):\n",
    "        rewards, reward_dict = self.reward_function.from_batch_state(self.batch_state)\n",
    "        getattr(self.environment.batch_stats, self.name).append(rewards.mean().detach().cpu().numpy())\n",
    "        rewards = rewards * self.weight\n",
    "        self.batch_state.rewards += rewards\n",
    "        self.batch_state[self.name] = reward_dict\n",
    "\n",
    "class LossCallback(Callback):\n",
    "    def __init__(self, loss_function, name, weight=1.):\n",
    "        super().__init__(order=1)\n",
    "        self.name = name\n",
    "        self.loss_function = loss_function\n",
    "        self.weight = weight\n",
    "        \n",
    "    def setup(self):\n",
    "        bs = self.environment.batch_stats\n",
    "        bs.add_metric(self.name)\n",
    "#         setattr(bs, self.name, [])\n",
    "        \n",
    "    def compute_loss(self):\n",
    "        loss, loss_dict = self.loss_function.from_batch_state(self.batch_state)\n",
    "        getattr(self.environment.batch_stats, self.name).append(loss.detach().cpu().numpy())\n",
    "        loss = loss * self.weight\n",
    "        self.batch_state.loss += loss\n",
    "        self.batch_state[self.name] = loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0f1702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7fd62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrl.dataloaders import *\n",
    "from mrl.layers import *\n",
    "from mrl.g_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68e8a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrl.policy_gradient import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4db900a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = CharacterVocab(SMILES_CHAR_VOCAB)\n",
    "\n",
    "ds = TextDataset(['CCC'], vocab)\n",
    "\n",
    "d_vocab = len(vocab.itos)\n",
    "d_embedding = 256\n",
    "d_hidden = 1024\n",
    "n_layers = 3\n",
    "lstm_drop = 0.\n",
    "lin_drop = 0.\n",
    "bos_idx = vocab.stoi['bos']\n",
    "bidir = False\n",
    "tie_weights = True\n",
    "\n",
    "lm_model = LSTM_LM(d_vocab, d_embedding, d_hidden, n_layers,\n",
    "                lstm_drop, lin_drop, bos_idx, bidir, tie_weights)\n",
    "\n",
    "lm_model.load_state_dict(torch.load('untracked_files/lstm_lm_small.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e64f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = GenerativeAgent(lm_model, vocab, CrossEntropy(), ds, value_head=None, opt_kwargs={'lr':1e-4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfbbe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_sa(sa):\n",
    "    return (10-sa)/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcd2be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = Template([ValidityFilter(), SingleCompoundFilter()],\n",
    "                    [QEDFilter(None, None, score=PassThroughScore()),\n",
    "                     SAFilter(None, None, \n",
    "                              score=PropertyFunctionScore(scale_sa))], fail_score=-1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebc88e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a39777",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_cb = GenAgentCallback(agent, 'generative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb58e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler1 = ModelSampler(agent, agent.model, 'live', 0.5, 0.5, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd83bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler2 = ModelSampler(agent, agent.base_model, 'base', 0.5, 0., 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2658114",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg = PolicyGradient()\n",
    "# pg = TRPO(0.97, 2)\n",
    "# pg = PPO(0.97, 0.2)\n",
    "\n",
    "loss1 = LossCallback(pg, 'pg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc3a889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46906df",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(agent_cb, template, samplers=[sampler1, sampler2], loss_cbs=[loss1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89979ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.bs = 32\n",
    "env.sl = 90\n",
    "env.buffer_size=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36877931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations\tdiversity\tvalid\trewards\tlive_diversity\tlive_valid\tlive_rewards\tpg\n"
     ]
    }
   ],
   "source": [
    "env('before_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d804bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.build_buffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e5b620",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.sample_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793dd4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env('get_model_outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc45d4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n"
     ]
    }
   ],
   "source": [
    "env.compute_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b08460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3644245]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.batch_stats.live_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56511f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.compute_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5345f471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00\t1.00\t0.97\t1.41\t1.00\t0.94\t1.36\t-0.10\n"
     ]
    }
   ],
   "source": [
    "env('after_batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02dd1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env('after_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7407dffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations  diversity  valid     rewards   live_diversity  live_valid  live_rewards  pg      \n",
      "0           1.00       1.00      1.53      1.00            1.00        1.55          0.01      \n",
      "1           1.00       1.00      1.50      1.00            1.00        1.52          0.04      \n",
      "2           1.00       1.00      1.51      1.00            1.00        1.55          0.01      \n",
      "3           1.00       1.00      1.51      1.00            1.00        1.55          0.04      \n"
     ]
    }
   ],
   "source": [
    "env.fit(32, 90, 4, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3f4864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastprogress.fastprogress.ConsoleMasterBar at 0x1377145f8>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.batch_stats.pbar.write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8a350c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5         1.00      1.00      1.49      1.00      1.00      1.47      0.04      \n"
     ]
    }
   ],
   "source": [
    "env.batch_stats('after_batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b187ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16b017c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c74182d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
