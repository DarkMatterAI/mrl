{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "> Environment/Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/envs/mrl/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for boost::shared_ptr<RDKit::FilterCatalogEntry const> already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "from mrl.imports import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *\n",
    "from mrl.chem import *\n",
    "from mrl.templates import *\n",
    "from mrl.agent import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Callback():\n",
    "    def __init__(self, name='callback', order=10):\n",
    "        self.order=order\n",
    "        self.name = name\n",
    "    \n",
    "    def __call__(self, event_name):\n",
    "        \n",
    "        event = getattr(self, event_name, None)\n",
    "        if event is not None:\n",
    "            output = event()\n",
    "        else:\n",
    "            output = None\n",
    "            \n",
    "        return output\n",
    "    \n",
    "class Log(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__(name='log', order=100)\n",
    "        \n",
    "        self.pbar = None\n",
    "        self.iterations = 0\n",
    "        self.metrics = defaultdict(list)\n",
    "        self.metrics['rewards']\n",
    "#         self.metrics['mean_reward']\n",
    "        self.metrics['valid']\n",
    "        self.metrics['diversity']\n",
    "        \n",
    "        self.log = defaultdict(list)\n",
    "        self.log['samples']\n",
    "        self.log['sources']\n",
    "        self.log['rewards']\n",
    "#         self.log['rewards_scaled']\n",
    "        \n",
    "        self.report = 1\n",
    "        self.do_log = True\n",
    "        self.unique_samples = set()\n",
    "        \n",
    "    def before_train(self):\n",
    "        cols = ['iterations'] + list(self.metrics.keys())\n",
    "        if self.pbar is None:\n",
    "            print('\\t'.join(cols))\n",
    "        else:\n",
    "            self.pbar.write(cols, table=True)\n",
    "            \n",
    "    def add_metric(self, name):\n",
    "        self.metrics[name]\n",
    "        \n",
    "    def add_log(self, name):\n",
    "        self.log[name]\n",
    "        \n",
    "    def update_metric(self, name, value):\n",
    "        if self.do_log:\n",
    "            self.metrics[name].append(value)\n",
    "        \n",
    "    def update_log(self):\n",
    "        if self.do_log:\n",
    "            env = self.environment\n",
    "            batch_state = env.batch_state\n",
    "            samples = batch_state.samples\n",
    "            self.unique_samples.update(set(samples))\n",
    "\n",
    "            for key in self.log.keys():\n",
    "                items = batch_state[key]\n",
    "                if isinstance(items, torch.Tensor):\n",
    "                    items = items.detach().cpu().numpy()\n",
    "                self.log[key].append(items)\n",
    "        \n",
    "    def report_batch(self):\n",
    "        outputs = [f'{self.iterations}']\n",
    "        if self.iterations%self.report==0:\n",
    "            \n",
    "            for k,v in self.metrics.items():\n",
    "                val = v[-1]\n",
    "\n",
    "                if type(val)==int:\n",
    "                    val = f'{val}'\n",
    "                else:\n",
    "                    val = f'{val:.3f}'\n",
    "\n",
    "                outputs.append(val)\n",
    "\n",
    "            if self.pbar is None:\n",
    "                print('\\t'.join(outputs))\n",
    "            else:\n",
    "                self.pbar.write(outputs, table=True)\n",
    "            \n",
    "        self.iterations += 1\n",
    "        \n",
    "    def after_batch(self):\n",
    "        self.update_log()\n",
    "        self.report_batch()\n",
    "\n",
    "    \n",
    "class Buffer(Callback):\n",
    "    def __init__(self, p_total, max_size=1000000):\n",
    "        super().__init__(name='buffer', order=0)\n",
    "        \n",
    "        self.buffer = []\n",
    "        self.used_buffer = []\n",
    "        self.max_size = max_size\n",
    "        self.p_total = p_total\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def add(self, item):\n",
    "        \n",
    "        if is_container(item):\n",
    "            for i in item:\n",
    "                self.add(i)\n",
    "        else:\n",
    "            place_idx = (len(self.buffer)%self.max_size)+1\n",
    "\n",
    "            if place_idx>=len(self.buffer):\n",
    "                self.buffer.append(item)\n",
    "            else:\n",
    "                self.buffer[place_idx-1] = item\n",
    "            \n",
    "    def sample(self, n):\n",
    "        \n",
    "        idxs = np.random.choice(np.arange(len(self.buffer)), n, replace=False)\n",
    "        batch = [self.buffer[i] for i in idxs]\n",
    "        for idx in sorted(idxs, reverse=True):\n",
    "            self.buffer.pop(idx)\n",
    "\n",
    "        return batch\n",
    "    \n",
    "    def after_sample(self):\n",
    "        samples = self.batch_state.samples\n",
    "        samples = self.environment.template_cb.standardize(samples)\n",
    "        self.batch_state.samples = samples\n",
    "        self.used_buffer += samples\n",
    "        \n",
    "        if self.environment.log.iterations%40 == 0:\n",
    "            self.used_buffer = list(set(self.used_buffer))\n",
    "            \n",
    "            if len(self.used_buffer)>self.max_size:\n",
    "                self.used_buffer = self.used_buffer[-self.max_size:]\n",
    "        \n",
    "    def after_build_buffer(self):\n",
    "        template = self.environment.template_cb\n",
    "        if self.buffer:\n",
    "            self.buffer = template.standardize(self.buffer)\n",
    "            self.buffer = list(set(self.buffer))\n",
    "            self.buffer = template.filter_sequences(self.buffer)\n",
    "    \n",
    "    def sample_batch(self):\n",
    "        bs = int(self.environment.bs * self.p_total)\n",
    "        if bs>0:\n",
    "            sample = self.sample(bs)\n",
    "            self.batch_state.samples += sample\n",
    "            self.batch_state.sources += ['buffer']*len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class SettrDict(dict):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def __setitem__(self, key, item):\n",
    "        super().__setitem__(key, item)\n",
    "        super().__setattr__(key, item)\n",
    "    \n",
    "    def __setattr__(self, key, item):\n",
    "        super().__setitem__(key, item)\n",
    "        super().__setattr__(key, item)\n",
    "        \n",
    "    def update_from_dict(self, update_dict):\n",
    "        for k,v in update_dict.items():\n",
    "            self[k] = v\n",
    "        \n",
    "class BatchState(SettrDict):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.samples = []\n",
    "        self.sources = []\n",
    "        self.rewards = to_device(torch.tensor(0.))\n",
    "        self.trajectory_rewards = to_device(torch.tensor(0.))\n",
    "        self.loss = to_device(torch.tensor(0.))\n",
    "        self.latent_data = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Event():\n",
    "    def __init__(self):\n",
    "        self.setup = 'setup'\n",
    "        self.before_train = 'before_train'\n",
    "        self.build_buffer = 'build_buffer'\n",
    "        self.after_build_buffer = 'after_build_buffer'\n",
    "        self.before_batch = 'before_batch'\n",
    "        self.sample_batch = 'sample_batch'\n",
    "        self.after_sample = 'after_sample'\n",
    "        self.get_model_outputs = 'get_model_outputs'\n",
    "        self.compute_reward = 'compute_reward'\n",
    "        self.after_compute_reward = 'after_compute_reward'\n",
    "        self.compute_loss = 'compute_loss'\n",
    "        self.zero_grad = 'zero_grad'\n",
    "        self.before_step = 'before_step'\n",
    "        self.step = 'step'\n",
    "        self.after_batch = 'after_batch'\n",
    "        self.after_train = 'after_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Environment():\n",
    "    def __init__(self, agent_cb, template_cb=None, samplers=[], reward_cbs=[], loss_cbs=[], cbs=[],\n",
    "                buffer_p_batch=None, reward_decay=0.9):\n",
    "        self.agent_cb = agent_cb\n",
    "        self.template_cb = template_cb\n",
    "        self.samplers = samplers\n",
    "        self.reward_cbs = reward_cbs\n",
    "        self.loss_cbs = loss_cbs\n",
    "        self.cbs = []\n",
    "        if buffer_p_batch is None:\n",
    "            buffer_p_batch = 1.\n",
    "            for samp in samplers:\n",
    "                buffer_p_batch -= samp.p_batch\n",
    "        self.buffer = Buffer(buffer_p_batch)\n",
    "        self.batch_state = BatchState()\n",
    "        self.log = Log()\n",
    "#         self.mean_reward = None\n",
    "        self.reward_decay = reward_decay\n",
    "        \n",
    "        all_cbs = [self.agent_cb] + [self.template_cb] + self.samplers + self.reward_cbs\n",
    "        all_cbs += self.loss_cbs + cbs + [self.buffer] + [self.log]\n",
    "        \n",
    "        self.register_cbs(all_cbs)\n",
    "        self('setup')\n",
    "        \n",
    "    def __call__(self, event):\n",
    "        for cb in self.cbs:\n",
    "            if hasattr(cb, event):\n",
    "                cb(event)\n",
    "        \n",
    "    def register_cb(self, cb):\n",
    "        if isinstance(cb, type): \n",
    "            cb = cb()\n",
    "        cb.environment = self\n",
    "        setattr(self, cb.name, cb)\n",
    "        self.cbs.append(cb)\n",
    "        \n",
    "    def register_cbs(self, cbs):\n",
    "        for cb in cbs:\n",
    "            self.register_cb(cb)\n",
    "            \n",
    "    def remove_cb(self, cb):\n",
    "        cb.environment = None\n",
    "        cb.batch_state = None\n",
    "        if hasattr(self, cb.name):\n",
    "            delattr(self, cb.name)\n",
    "            \n",
    "        if cb in self.cbs:\n",
    "            self.cbs.remove(cb)\n",
    "        \n",
    "    def remove_cbs(self, cbs):\n",
    "        for cb in cbs:\n",
    "            self.remove_cb(ccb)\n",
    "        \n",
    "    def build_buffer(self):\n",
    "        if (len(self.buffer) < self.bs) and (self.buffer_size>0):\n",
    "            self('build_buffer')\n",
    "            self('after_build_buffer')\n",
    "            \n",
    "    def sample_batch(self):\n",
    "        self.batch_state = BatchState()\n",
    "        for cb in self.cbs:\n",
    "            cb.batch_state = self.batch_state\n",
    "        self('before_batch') \n",
    "        self('sample_batch') \n",
    "\n",
    "        self('after_sample') \n",
    "        \n",
    "    def compute_reward(self):\n",
    "        self('compute_reward')\n",
    "        rewards = self.batch_state.rewards\n",
    "        \n",
    "        self.log.update_metric('rewards', rewards.mean().detach().cpu().numpy())\n",
    "        \n",
    "        self('after_compute_reward')\n",
    "        \n",
    "    def compute_loss(self):\n",
    "        self('compute_loss')\n",
    "        loss = self.batch_state.loss\n",
    "        self('zero_grad')\n",
    "        try:\n",
    "            loss.backward()\n",
    "        except:\n",
    "            # possibly no loss\n",
    "            pass\n",
    "        self('before_step')\n",
    "        self('step')\n",
    "            \n",
    "    def fit(self, bs, sl, iters, buffer_size, report, cbs=[]):\n",
    "        self.register_cbs(cbs)\n",
    "        self.bs = bs\n",
    "        self.sl = sl\n",
    "        self.buffer_size = buffer_size\n",
    "        self.report = report\n",
    "        mb = master_bar(range(1))\n",
    "        self.log.pbar = mb\n",
    "        self.log.report = report\n",
    "        self('before_train')\n",
    "        for _ in mb:\n",
    "            for step in progress_bar(range(iters), parent=mb):\n",
    "                self.build_buffer()\n",
    "                self.sample_batch()\n",
    "                self('get_model_outputs')\n",
    "                self.compute_reward()\n",
    "                self.compute_loss()\n",
    "                self('after_batch')\n",
    "                \n",
    "        self('after_train')\n",
    "        self.remove_cbs(cbs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Sampler(Callback):\n",
    "    def __init__(self, name, p_buffer=0., p_batch=0., track=True):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.p_buffer = p_buffer\n",
    "        self.p_batch = p_batch\n",
    "        self.track = track\n",
    "        \n",
    "    def setup(self):\n",
    "        if self.p_batch>0. and self.track:\n",
    "            bs = self.environment.log\n",
    "            bs.add_metric(f'{self.name}_diversity')\n",
    "            bs.add_metric(f'{self.name}_valid')\n",
    "            bs.add_metric(f'{self.name}_rewards')\n",
    "            bs.add_metric(f'{self.name}_new')\n",
    "        \n",
    "    def build_buffer(self):\n",
    "        pass\n",
    "    \n",
    "    def sample_batch(self):\n",
    "        pass\n",
    "    \n",
    "    def after_compute_reward(self):\n",
    "        if self.p_batch>0. and self.track:\n",
    "            state = self.environment.batch_state\n",
    "            log = self.environment.log\n",
    "            rewards = state.rewards.detach().cpu().numpy()\n",
    "            sources = np.array(state.sources)\n",
    "            if self.name in sources:\n",
    "                log.update_metric(f'{self.name}_rewards', rewards[sources==self.name].mean())\n",
    "            else:\n",
    "                log.update_metric(f'{self.name}_rewards', 0.)\n",
    "                \n",
    "    def after_sample(self):\n",
    "        if self.p_batch > 0. and self.track:\n",
    "            log = self.environment.log\n",
    "            state = self.environment.batch_state\n",
    "            samples = state.samples\n",
    "            sources = np.array(state.sources)==self.name\n",
    "\n",
    "            samples = [samples[i] for i in range(len(samples)) if sources[i]]\n",
    "#             samples = samples[sources==self.name]\n",
    "            used = log.unique_samples\n",
    "            novel = [i for i in samples if not i in used]\n",
    "            percent_novel = len(novel)/len(samples)\n",
    "\n",
    "            log.update_metric(f'{self.name}_new', percent_novel)\n",
    "                  \n",
    "class ModelSampler(Sampler):\n",
    "    def __init__(self, agent, model, name, p_buffer, p_batch, genbatch, latent=False, track=True,\n",
    "                temperature=1., contrastive=False):\n",
    "        super().__init__(name, p_buffer, p_batch, track)\n",
    "        self.agent = agent\n",
    "        self.model = model\n",
    "        self.genbatch = genbatch\n",
    "        self.latent = latent if self.agent.latents is not None else False\n",
    "        self.temperature = temperature\n",
    "        self.contrastive = contrastive\n",
    "        \n",
    "    def build_buffer(self):\n",
    "        env = self.environment\n",
    "        bs = int(env.buffer_size * self.p_buffer)\n",
    "        outputs = []\n",
    "        to_generate = bs\n",
    "        \n",
    "        if bs > 0:\n",
    "            for batch in range(int(np.ceil(bs/self.genbatch))):\n",
    "                current_bs = min(self.genbatch, to_generate)\n",
    "                \n",
    "                preds, _ = self.model.sample_no_grad(current_bs, env.sl, multinomial=True,\n",
    "                                                     temperature=self.temperature)\n",
    "                sequences = self.agent.reconstruct(preds)\n",
    "                sequences = list(set(sequences))\n",
    "                sequences = [i for i in sequences if to_mol(i) is not None]\n",
    "                outputs += sequences\n",
    "                outputs = list(set(outputs))\n",
    "                to_generate = bs - len(outputs)\n",
    "                \n",
    "            env.buffer.add(outputs)\n",
    "            \n",
    "            \n",
    "    def sample_batch(self):\n",
    "        env = self.environment\n",
    "        bs = int(env.bs * self.p_batch)\n",
    "        \n",
    "        if bs > 0:\n",
    "            \n",
    "            if self.latent:\n",
    "                latents = self.agent.latents\n",
    "                latent_idxs = torch.randint(0, latents.shape[0]-1, (bs,))\n",
    "                sample_latents = latents[latent_idxs]\n",
    "            else:\n",
    "                sample_latents=None\n",
    "            \n",
    "            \n",
    "            preds, _ = self.model.sample_no_grad(bs, env.sl, z=sample_latents, multinomial=True,\n",
    "                                                temperature=self.temperature)\n",
    "            sequences = self.agent.reconstruct(preds)\n",
    "            sequences = env.template_cb.standardize(sequences)\n",
    "            diversity = len(set(sequences))/len(sequences)\n",
    "            valid = np.array([to_mol(i) is not None for i in sequences])\n",
    "            \n",
    "            if self.track:\n",
    "                env.log.update_metric(f\"{self.name}_diversity\", diversity)\n",
    "                env.log.update_metric(f\"{self.name}_valid\", valid.mean())\n",
    "            \n",
    "            hps = env.template_cb.get_hps(sequences)\n",
    "            sequences = list(np.array(sequences)[hps])\n",
    "            \n",
    "            if sample_latents is not None:\n",
    "                latent_idxs = latent_idxs[hps]\n",
    "                self.batch_state.latent_data.append([self.name, latent_idxs])\n",
    "            \n",
    "            self.batch_state.samples += sequences\n",
    "            self.batch_state.sources += [self.name]*len(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "        \n",
    "class TemplateCallback(Callback):\n",
    "    def __init__(self, template=None, weight=1., track=True, prefilter=True):\n",
    "        super().__init__(order=-1)\n",
    "        self.template = template\n",
    "        self.track = track\n",
    "        self.name = 'template'\n",
    "        self.prefilter = prefilter\n",
    "        self.weight = weight\n",
    "        \n",
    "    def setup(self):\n",
    "        if self.track:\n",
    "            log = self.environment.log\n",
    "            log.add_metric(self.name)\n",
    "            log.add_log(self.name)\n",
    "        \n",
    "    def compute_reward(self):\n",
    "        env = self.environment\n",
    "        state = env.batch_state\n",
    "        \n",
    "        if self.template is not None:\n",
    "            rewards = np.array(self.template.eval_mols(state.samples))\n",
    "            hps = np.array(self.template(state.samples))\n",
    "        else:\n",
    "            rewards = np.array([0.]*len(state.samples))\n",
    "            hps = np.array([0.]*len(state.samples))\n",
    "        \n",
    "        state[self.name] = rewards\n",
    "        \n",
    "        if self.track:\n",
    "            env.log.update_metric(self.name, rewards.mean())\n",
    "            \n",
    "        state.template_passes = hps\n",
    "        state.rewards += to_device(torch.from_numpy(self.weight*rewards).float())\n",
    "        \n",
    "    def get_hps(self, sequences):\n",
    "        if self.template is not None:\n",
    "            hps = np.array(self.template(sequences))\n",
    "        else:\n",
    "            hps = np.array([True]*len(sequences))\n",
    "            \n",
    "        return hps\n",
    "        \n",
    "    def filter_sequences(self, sequences):\n",
    "        \n",
    "        if self.prefilter:\n",
    "            hps = self.get_hps(sequences)\n",
    "            sequences = list(np.array(sequences)[hps])\n",
    "        return sequences\n",
    "    \n",
    "    def standardize(self, sequences):\n",
    "        if self.template is not None:\n",
    "            sequences = self.template.standardize(sequences)\n",
    "        \n",
    "        return sequences\n",
    "\n",
    "    \n",
    "class ContrastiveTemplate(TemplateCallback):\n",
    "    def __init__(self, similarity_function, template=None, weight=1., track=True, prefilter=True):\n",
    "        super().__init__(template=template, weight=weight, track=track, prefilter=prefilter)\n",
    "        self.similarity_function = similarity_function\n",
    "        \n",
    "    def setup(self):\n",
    "        if self.track:\n",
    "            log = self.environment.log\n",
    "            log.add_metric(self.name)\n",
    "            log.add_metric(self.name+'_temp')\n",
    "            log.add_metric(self.name+'_sim')\n",
    "            log.add_log(self.name)\n",
    "            log.add_log(self.name+'_temp')\n",
    "            log.add_log(self.name+'_sim')\n",
    "        \n",
    "    def compute_reward(self):\n",
    "        env = self.environment\n",
    "        state = env.batch_state\n",
    "        source_samples = state.source_samples\n",
    "        samples = state.target_samples\n",
    "        \n",
    "        if template is not None:\n",
    "            source_rewards = np.array(self.template.eval_mols(source_samples))\n",
    "            target_rewards = np.array(self.template.eval_mols(samples))\n",
    "            \n",
    "            source_hps = np.array(self.template(source_samples))\n",
    "            target_hps = np.array(self.template(samples))\n",
    "            hps = source_hps * target_hps\n",
    "            rewards = target_rewards - source_rewards\n",
    "            sims = self.similarity_function(source_samples, samples)\n",
    "            rewards = rewards\n",
    "            \n",
    "        else:\n",
    "            rewards = np.array([0.]*len(state.samples))\n",
    "            sims = np.array([0.]*len(state.samples))\n",
    "            hps = np.array([0.]*len(state.samples))\n",
    "        \n",
    "        state.template = rewards\n",
    "        \n",
    "        full_rewards = rewards + sims\n",
    "        \n",
    "        if self.track:\n",
    "            env.log.update_metric(self.name, full_rewards.mean())\n",
    "            env.log.update_metric(self.name+'_temp', rewards.mean())\n",
    "            env.log.update_metric(self.name+'_sim', sims.mean())\n",
    "            \n",
    "        state[self.name] = full_rewards\n",
    "        state[self.name+'_temp'] = rewards\n",
    "        state[self.name+'_sim'] = sims\n",
    "            \n",
    "        state.template_passes = hps\n",
    "        state.rewards += to_device(torch.from_numpy(self.weight*full_rewards).float())\n",
    "        \n",
    "    def get_hps(self, sequences):\n",
    "        if self.template is not None:\n",
    "            s_hps = np.array(self.template([i[0] for i in sequences]))\n",
    "            t_hps = np.array(self.template([i[1] for i in sequences]))\n",
    "            hps = s_hps * t_hps\n",
    "            hps = np.array(self.template(sequences))\n",
    "        else:\n",
    "            hps = np.array([True]*len(sequences))\n",
    "            \n",
    "        return hps\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "move contrastive tuple sampling into sampler (remove from agent)\n",
    "update agent to change `batch_ds = self.agent.dataset.new(sequences)` if contrastive and remove new sequence generation\n",
    "update hard passes for contrastive template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class AgentCallback(Callback):\n",
    "    def __init__(self, agent, name, clip=1.):\n",
    "        super().__init__(order=20)\n",
    "        self.agent = agent\n",
    "        self.name = name\n",
    "        self.clip = clip\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.agent.zero_grad()\n",
    "    \n",
    "    def before_step(self):\n",
    "        nn.utils.clip_grad_norm_(self.agent.model.parameters(), self.clip)\n",
    "        \n",
    "    def step(self):\n",
    "        self.agent.step()\n",
    "        \n",
    "    def after_sample(self):\n",
    "        env = self.environment\n",
    "        sequences = self.batch_state.samples\n",
    "        bs = len(sequences)\n",
    "        self.batch_state.rewards = to_device(torch.zeros(bs))\n",
    "#         self.batch_state.rewards_scaled = to_device(torch.zeros(bs))\n",
    "                \n",
    "        diversity = len(set(sequences))/len(sequences)\n",
    "        valid = len([i for i in sequences if to_mol(i) is not None])/len(sequences)\n",
    "        \n",
    "        env.log.update_metric('diversity', diversity)\n",
    "        env.log.update_metric('valid', valid)\n",
    "        \n",
    "    def get_model_outputs(self):\n",
    "        # get relevant model outputs\n",
    "        pass\n",
    "        \n",
    "class GenAgentCallback(AgentCallback):\n",
    "    def __init__(self, agent, name, contrastive=False):\n",
    "        super().__init__(agent, name)\n",
    "        self.contrastive = contrastive\n",
    "    \n",
    "    def after_sample(self):\n",
    "        \n",
    "        env = self.environment\n",
    "        sequences = self.batch_state.samples\n",
    "        batch_ds = self.agent.dataset.new(sequences)\n",
    "        batch = batch_ds.collate_function([batch_ds[i] for i in range(len(batch_ds))])\n",
    "        batch = to_device(batch)\n",
    "        bs = len(batch_ds)\n",
    "        x,y = batch\n",
    "        \n",
    "        if self.contrastive:\n",
    "            z = self.agent.model.x_to_latent(x)\n",
    "            preds, _ = self.agent.model.sample_no_grad(z.shape[0], env.sl, z=z)\n",
    "            new_sequences = self.agent.reconstruct(preds)\n",
    "            batch_ds = self.agent.dataset.new(new_sequences)\n",
    "            batch = batch_ds.collate_function([batch_ds[i] for i in range(len(batch_ds))])\n",
    "            batch = to_device(batch)\n",
    "            \n",
    "            old_x = x\n",
    "            old_y = x\n",
    "            \n",
    "            x,y = batch\n",
    "            x = [x[0], old_x[1]]\n",
    "            self.batch_state.source_samples = sequences\n",
    "            self.batch_state.target_samples = new_sequences\n",
    "            self.batch_state.samples = [(sequences[i], new_sequences[i]) for i in range(len(sequences))]\n",
    "            \n",
    "        self.batch_state.x = x\n",
    "        self.batch_state.y = y\n",
    "        self.batch_state.bs = bs\n",
    "        mask = ~(y==self.agent.vocab.stoi['pad'])\n",
    "        self.batch_state.mask = mask\n",
    "        self.batch_state.lengths = mask.sum(-1)\n",
    "        self.batch_state.sl = y.shape[-1]\n",
    "        self.batch_state.sequence_trajectories = self.agent.reconstruct_trajectory(y)\n",
    "        self.batch_state.rewards = to_device(torch.zeros(bs))\n",
    "        self.batch_state.trajectory_rewards = to_device(torch.zeros(y.shape))\n",
    "                \n",
    "        diversity = len(set(sequences))/len(sequences)\n",
    "        valid = len([i for i in sequences if to_mol(i) is not None])/len(sequences)\n",
    "        \n",
    "        env.log.update_metric('diversity', diversity)\n",
    "        env.log.update_metric('valid', valid)\n",
    "        \n",
    "    def subset_tensor(self, x, mask):\n",
    "        if type(x)==list:\n",
    "            x = [i[mask] for i in x]\n",
    "        else:\n",
    "            x = x[mask]\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_rl_tensors(self, model, x, y, latent_info, sources):\n",
    "        if latent_info:\n",
    "            latent_sources = []\n",
    "            output_tensors = []\n",
    "            for (latent_source, latent_idxs) in latent_info:\n",
    "                latent_sources.append(latent_source)\n",
    "                latent_mask = torch.tensor([i==latent_source for i in sources]).bool()\n",
    "                latents = self.agent.latents[latent_idxs]\n",
    "                out = self.agent.model.get_rl_tensors(self.subset_tensor(x, latent_mask), \n",
    "                                                      self.subset_tensor(y, latent_mask),\n",
    "                                                      latent=latents)\n",
    "                output_tensors.append(out)\n",
    "                \n",
    "            non_latent_mask = torch.tensor([not i in latent_sources for i in sources]).bool()\n",
    "            if non_latent_mask.sum()>0:\n",
    "                out = model.get_rl_tensors(self.subset_tensor(x, non_latent_mask), \n",
    "                                                      self.subset_tensor(y, non_latent_mask))\n",
    "                output_tensors.append(out)\n",
    "            \n",
    "            mo = torch.cat([i[0] for i in output_tensors], 0)\n",
    "            mlp = torch.cat([i[1] for i in output_tensors], 0)\n",
    "            mglp = torch.cat([i[2] for i in output_tensors], 0)\n",
    "            me = torch.cat([i[3] for i in output_tensors], 0)\n",
    "            \n",
    "        else:\n",
    "            mo, mlp, mglp, me = model.get_rl_tensors(x,y)\n",
    "            \n",
    "        return mo, mlp, mglp, me \n",
    "        \n",
    "    def get_model_outputs(self):\n",
    "            \n",
    "        x = self.batch_state.x\n",
    "        y = self.batch_state.y\n",
    "        sources = self.batch_state.sources\n",
    "        latent_info = self.batch_state.latent_data\n",
    "            \n",
    "        mo, mlp, mglp, me = self.get_rl_tensors(self.agent.model, x, y, latent_info, sources)\n",
    "        mprob = mlp.exp()\n",
    "        \n",
    "        self.batch_state.model_output = mo\n",
    "        self.batch_state.model_logprobs = mlp\n",
    "        self.batch_state.model_gathered_logprobs = mglp\n",
    "        self.batch_state.model_encoded = me\n",
    "        self.batch_state.y_gumbel = F.one_hot(y, len(self.agent.vocab.itos)) + mprob - mprob.detach()\n",
    "        \n",
    "        if self.agent.value_head is not None:\n",
    "            value_predictions = self.agent.value_head(me)\n",
    "            with torch.no_grad():\n",
    "                base_value_predictions = self.agent.base_value_head(me)\n",
    "        else:\n",
    "            value_predictions = None\n",
    "            base_value_predictions = None\n",
    "            \n",
    "        self.batch_state.state_values = value_predictions\n",
    "        self.batch_state.ref_state_values = base_value_predictions\n",
    "        \n",
    "        if self.agent.base_model is not None:\n",
    "            with torch.no_grad():\n",
    "#                 bo, blp, bglp, be = self.agent.base_model.get_rl_tensors(x,y)\n",
    "                bo, blp, bglp, be = self.get_rl_tensors(self.agent.base_model, x, y, latent_info, sources)    \n",
    "        else:\n",
    "            bo, blp, bglp, be = None, None, None, None\n",
    "            \n",
    "        self.batch_state.reference_output = bo\n",
    "        self.batch_state.reference_logprobs = blp\n",
    "        self.batch_state.reference_gathered_logprobs = bglp\n",
    "        self.batch_state.reference_encoded = be\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class RewardCallback(Callback):\n",
    "    def __init__(self, reward_function, name, weight=1., track=True):\n",
    "        super().__init__(order=1)\n",
    "        self.name = name\n",
    "        self.reward_function = reward_function\n",
    "        self.weight = weight\n",
    "        self.track = track\n",
    "        \n",
    "    def setup(self):\n",
    "        if self.track:\n",
    "            log = self.environment.log\n",
    "            log.add_metric(self.name)\n",
    "            log.add_log(self.name)\n",
    "        \n",
    "    def compute_reward(self):\n",
    "        rewards, reward_dict = self.reward_function.from_batch_state(self.batch_state)\n",
    "        \n",
    "        if self.track:\n",
    "            self.environment.log.update_metric(self.name, rewards.mean().detach().cpu().numpy())\n",
    "            \n",
    "        rewards = rewards * self.weight\n",
    "        self.batch_state.rewards += rewards\n",
    "        self.batch_state[self.name] = reward_dict\n",
    "\n",
    "class LossCallback(Callback):\n",
    "    def __init__(self, loss_function, name, weight=1., track=True):\n",
    "        super().__init__(order=1)\n",
    "        self.name = name\n",
    "        self.loss_function = loss_function\n",
    "        self.weight = weight\n",
    "        self.track = track\n",
    "        \n",
    "    def setup(self):\n",
    "        if self.track:\n",
    "            log = self.environment.log\n",
    "            log.add_metric(self.name)\n",
    "            log.add_log(self.name)\n",
    "        \n",
    "    def compute_loss(self):\n",
    "        loss, loss_dict = self.loss_function.from_batch_state(self.batch_state)\n",
    "        \n",
    "        if self.track:\n",
    "            self.environment.log.update_metric(self.name, loss.mean().detach().cpu().numpy())\n",
    "            \n",
    "        loss = loss * self.weight\n",
    "        self.batch_state.loss += loss\n",
    "        self.batch_state[self.name] = loss_dict\n",
    "        \n",
    "        \n",
    "class PriorLoss():\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "        \n",
    "    def pg_loss(self, x, rewards):\n",
    "        model = self.agent.model\n",
    "        prior = self.agent.model.prior\n",
    "\n",
    "        old_prior = self.agent.base_model.prior\n",
    "        \n",
    "        z = model.x_to_latent(x)\n",
    "        prior_lps = prior.log_prob(z)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            old_lps = old_prior.log_prob(z)\n",
    "\n",
    "        ratios = prior_lps - old_lps.detach()\n",
    "\n",
    "        rewards = rewards - rewards.mean()\n",
    "\n",
    "        prior_loss = (-ratios.sum(-1)*rewards)\n",
    "        prior_loss = torch.clip(prior_loss, -256, 256)\n",
    "        prior_loss = prior_loss.mean()\n",
    "        \n",
    "        return prior_loss\n",
    "    \n",
    "    def from_batch_state(self, batch_state):\n",
    "        x = batch_state.x\n",
    "        rewards = batch_state.rewards\n",
    "        prior_loss = self.pg_loss(x, rewards)\n",
    "        return prior_loss, {}\n",
    "    \n",
    "class HistoricPriorLoss(Callback):\n",
    "    def __init__(self, agent, percentile, n, start_iter):\n",
    "        super().__init__(name='prior_his', order=10)\n",
    "        self.agent = agent\n",
    "        self.loss = PriorLoss(agent)\n",
    "        self.percentile = percentile\n",
    "        self.n = n\n",
    "        self.start_iter = start_iter\n",
    "        \n",
    "    def setup(self):\n",
    "        log = self.environment.log\n",
    "        log.add_metric(self.name)\n",
    "        \n",
    "    def compute_loss(self):\n",
    "        loss = self.historic_loss()\n",
    "        \n",
    "        self.batch_state.loss += loss\n",
    "        self.environment.log.update_metric(self.name, loss.detach().cpu().numpy())\n",
    "        \n",
    "    def historic_loss(self):\n",
    "        env = self.environment\n",
    "        \n",
    "        iterations = self.environment.log.iterations\n",
    "\n",
    "        if iterations > self.start_iter:\n",
    "            df = log_to_df(env.log.log, ['samples', 'rewards'])\n",
    "            df.drop_duplicates(subset='samples', inplace=True)\n",
    "            \n",
    "            df1 = df[df.rewards>np.percentile(df.rewards.values, self.percentile)]\n",
    "            n_samp = min(self.n, df1.shape[0])\n",
    "            samples1 = df1.sample(n=n_samp)\n",
    "            \n",
    "            df2 = df[df.rewards<np.percentile(df.rewards.values, self.percentile)]\n",
    "            n_samp = min(self.n, df2.shape[0])\n",
    "            samples2 = df2.sample(n=n_samp)\n",
    "            \n",
    "            df = pd.concat([samples1, samples2])\n",
    "            \n",
    "            rewards = to_device(torch.tensor(df.rewards.values).float())\n",
    "            \n",
    "            \n",
    "            batch_ds = self.agent.dataset.new(df.samples.values)\n",
    "            batch = batch_ds.collate_function([batch_ds[i] for i in range(len(batch_ds))])\n",
    "            batch = to_device(batch)\n",
    "            x,y = batch\n",
    "            \n",
    "            prior_loss = self.loss.pg_loss(x, rewards)\n",
    "\n",
    "        else:\n",
    "            prior_loss = torch.tensor(0.)\n",
    "        \n",
    "        return prior_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class UpdateBaselineCB(Callback):\n",
    "    def __init__(self, agent, iters, name):\n",
    "        super().__init__(order=10, name=name)\n",
    "        self.agent = agent\n",
    "        self.iters = iters\n",
    "        self.num_updates = 0\n",
    "        \n",
    "    def after_batch(self):\n",
    "        log = self.environment.log\n",
    "        iterations = log.iterations\n",
    "        if iterations%self.iters == 0 and iterations>0:\n",
    "            self.agent.update_base_models()\n",
    "            self.num_updates += 1\n",
    "            \n",
    "            \n",
    "class StatsCallback(Callback):\n",
    "    def __init__(self, grabname, name, order):\n",
    "        self.grabname = grabname\n",
    "        self.name = name\n",
    "        self.order = order\n",
    "        \n",
    "    def setup(self):\n",
    "        log = self.environment.log\n",
    "        log.add_metric(f'{self.grabname}_p90')\n",
    "        log.add_metric(f'{self.grabname}_max')\n",
    "        \n",
    "    def after_compute_reward(self):\n",
    "        log = self.environment.log\n",
    "        state = self.environment.batch_state\n",
    "        rewards = state.rewards.detach().cpu().numpy()\n",
    "        sources = np.array(state.sources)\n",
    "        \n",
    "        rewards = rewards[sources==self.grabname]\n",
    "        \n",
    "        log.update_metric(f'{self.grabname}_p90', np.percentile(rewards, 90))\n",
    "        log.update_metric(f'{self.grabname}_max', rewards.max())\n",
    "\n",
    "class Rollback(Callback):\n",
    "    def __init__(self, model1, model2, metric, lookback, target, alpha, name):\n",
    "        super().__init__(order=10, name=name)\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.metric = metric\n",
    "        self.lookback = lookback\n",
    "        self.target = target\n",
    "        self.alpha = alpha\n",
    "        self.last_rollback = 0\n",
    "        \n",
    "    def after_batch(self):\n",
    "        env = self.environment\n",
    "        log = env.log\n",
    "        current_val = np.array(log.metrics[self.metric][-self.lookback:]).mean()\n",
    "    \n",
    "        if current_val < self.target and self.last_rollback <= 0:\n",
    "            print('rollback')\n",
    "            merge_models(self.model1, self.model2, alpha=self.alpha)\n",
    "            self.last_rollback = self.lookback\n",
    "            \n",
    "        self.last_rollback -= 1\n",
    "        \n",
    "class RetrainRollback(Callback):\n",
    "    def __init__(self, agent, percentile, base_update, lr, bs, metric, lookback, target, name):\n",
    "        super().__init__(order=1000, name=name)\n",
    "        self.agent = agent\n",
    "        self.percentile = percentile\n",
    "        self.base_update = base_update\n",
    "        self.lr = lr\n",
    "        self.bs = bs\n",
    "        self.metric = metric\n",
    "        self.lookback = lookback\n",
    "        self.target = target\n",
    "        self.last_rollback = 0\n",
    "        \n",
    "    def after_batch(self):\n",
    "        env = self.environment\n",
    "        log = env.log\n",
    "        current_val = np.array(log.metrics[self.metric][-self.lookback:]).mean()\n",
    "    \n",
    "        if current_val < self.target and self.last_rollback <= 0:\n",
    "            self.train_model()\n",
    "            self.last_rollback = self.lookback\n",
    "            \n",
    "        self.last_rollback -= 1\n",
    "        \n",
    "    def train_model(self):\n",
    "        env = self.environment\n",
    "        df = log_to_df(env.log.log, ['samples', 'rewards'])\n",
    "        df.drop_duplicates(subset='samples', inplace=True)\n",
    "        df = df[df.rewards>np.percentile(df.rewards.values, self.percentile)]\n",
    "\n",
    "        self.agent.update_dataset_from_inputs(df.samples.values)\n",
    "        self.agent.train_supervised(self.bs, 1, self.lr)\n",
    "\n",
    "        merge_models(self.agent.base_model, self.agent.model, alpha=self.base_update)     \n",
    "        \n",
    "class SupevisedCB(Callback):\n",
    "    def __init__(self, agent, frequency, base_update, percentile, lr, bs):\n",
    "        super().__init__('supervised', order=1000)\n",
    "        self.agent = agent\n",
    "        self.frequency = frequency\n",
    "        self.base_update = base_update\n",
    "        self.percentile = percentile\n",
    "        self.lr = lr\n",
    "        self.bs = bs\n",
    "        \n",
    "    def after_batch(self):\n",
    "        env = self.environment\n",
    "        iterations = self.environment.log.iterations\n",
    "        \n",
    "        if iterations>0 and iterations%self.frequency==0:\n",
    "            self.train_model()\n",
    "            \n",
    "            \n",
    "    def train_model(self):\n",
    "        env = self.environment\n",
    "        df = log_to_df(env.log.log, ['samples', 'rewards'])\n",
    "        df.drop_duplicates(subset='samples', inplace=True)\n",
    "        df = df[df.rewards>np.percentile(df.rewards.values, self.percentile)]\n",
    "\n",
    "        self.agent.update_dataset_from_inputs(df.samples.values)\n",
    "        self.agent.train_supervised(self.bs, 1, self.lr)\n",
    "\n",
    "        merge_models(self.agent.base_model, self.agent.model, alpha=self.base_update)      \n",
    "\n",
    "\n",
    "class LogSampler(Sampler):\n",
    "    def __init__(self, sample_name, start_iter, percentile, p_buffer=0.):\n",
    "        super().__init__(sample_name+'_sample', p_buffer, p_batch=0.)\n",
    "        self.start_iter = start_iter\n",
    "        self.percentile = percentile\n",
    "        self.sample_name = sample_name\n",
    "        \n",
    "    def build_buffer(self):\n",
    "        env = self.environment\n",
    "        \n",
    "        iterations = self.environment.log.iterations\n",
    "\n",
    "        if iterations > self.start_iter:\n",
    "            df = log_to_df(env.log.log, ['samples', self.sample_name])\n",
    "            df.drop_duplicates(subset='samples', inplace=True)\n",
    "            bs = int(env.buffer_size * self.p_buffer)\n",
    "            if bs > 0:\n",
    "                \n",
    "                subset = df[df[self.sample_name]>np.percentile(df[self.sample_name].values, \n",
    "                                                               self.percentile)]\n",
    "                outputs = list(subset.sample(n=min(bs, subset.shape[0])).samples.values)\n",
    "                env.buffer.add(outputs)\n",
    "                    \n",
    "class LogEnumerator(Sampler):\n",
    "    def __init__(self, sample_name, start_iter, n_samp, percentile):\n",
    "        super().__init__(sample_name+'_enum', p_buffer=0., p_batch=0.)\n",
    "        self.start_iter = start_iter\n",
    "        self.n_samp = n_samp\n",
    "        self.atom_types = ['C', 'N', 'O', 'F', 'S', 'Br', 'Cl', -1]\n",
    "        self.sample_name = sample_name\n",
    "        self.percentile = percentile\n",
    "        \n",
    "    def build_buffer(self):\n",
    "        \n",
    "        env = self.environment\n",
    "        \n",
    "        iterations = self.environment.log.iterations\n",
    "\n",
    "        if iterations > self.start_iter:\n",
    "            df = log_to_df(env.log.log, ['samples', self.sample_name])\n",
    "            df.drop_duplicates(subset='samples', inplace=True)\n",
    "            bs = int(env.buffer_size * self.p_buffer)\n",
    "                \n",
    "            subset = df[df[self.sample_name]>np.percentile(df[self.sample_name].values, \n",
    "                                                           self.percentile)]\n",
    "            samples = list(subset.sample(n=min(bs, subset.shape[0])).samples.values)\n",
    "            outputs = []\n",
    "\n",
    "            for s in samples:\n",
    "                new_smiles = add_atom_combi(s, self.atom_types) + add_bond_combi(s)\n",
    "                new_smiles = [i for i in new_smiles if i is not None]\n",
    "                new_smiles = [i for i in smiles if not '.' in i]\n",
    "                outputs += new_smiles\n",
    "\n",
    "            env.buffer.add(outputs)\n",
    "                    \n",
    "class DatasetSampler(Sampler):\n",
    "    def __init__(self, n_samples, samples, name):\n",
    "        super().__init__(name, 0., 0.)\n",
    "        self.n_samples = n_samples\n",
    "        self.samples = samples\n",
    "        \n",
    "    def build_buffer(self):\n",
    "        idxs = np.random.randint(0, len(self.samples), self.n_samples)\n",
    "        samples = [self.samples[i] for i in idxs]\n",
    "        self.environment.buffer.add(samples)\n",
    "        \n",
    "class NoveltyBonus(Callback):\n",
    "    def __init__(self, name, reward):\n",
    "        super().__init__(name=name, order=1)\n",
    "        self.reward = reward\n",
    "        \n",
    "    def setup(self):\n",
    "        log = self.environment.log\n",
    "        log.add_metric(self.name)\n",
    "        log.add_log(self.name)\n",
    "        \n",
    "    def compute_reward(self):\n",
    "        \n",
    "        log = self.environment.log\n",
    "        state = self.environment.batch_state\n",
    "        samples = np.array(state.samples)\n",
    "        \n",
    "        used = log.unique_samples\n",
    "        \n",
    "        new = [not i in used for i in samples]\n",
    "        reward = np.array([self.reward*i for i in new])\n",
    "        self.batch_state.rewards += to_device(torch.tensor(reward).float())\n",
    "        self.batch_state[self.name] = reward\n",
    "\n",
    "        log.update_metric(self.name, reward.mean())\n",
    "\n",
    "def log_to_df(log, keys=None):\n",
    "    batch = 0\n",
    "    output_dict = defaultdict(list)\n",
    "    \n",
    "    if keys is None:\n",
    "        keys = list(log.keys())\n",
    "    \n",
    "    items = log[keys[0]]\n",
    "    for item in items:\n",
    "        output_dict['batch'] += [batch]*len(item)\n",
    "        batch += 1\n",
    "        \n",
    "    for key in keys:\n",
    "        output_dict[key] = flatten_list_of_lists(log[key])\n",
    "        \n",
    "    return pd.DataFrame(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
