{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "! [ -e /content ] && pip install -Uqq mrl-pypi  # upgrade mrl on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab\n",
    "\n",
    "> Functions related to converting strings into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from mrl.imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenzation defines how we break text strings (ie SMILES strings) down into subunits that are fed to the model. The standard process goes as follows:\n",
    "1. A tokenization process breaks a string down into tokens\n",
    "2. Tokens are mapped to integers\n",
    "3. The token integers are sent to the model\n",
    "\n",
    "This brings up the problem of how best to tokenize smiles. The following methods are implemented out of the box:\n",
    "\n",
    "### Character Tokenization\n",
    "\n",
    "Character Tokenization is when we break down SMILES by character. This is implemented with the `tokenize_by_character` function.\n",
    "\n",
    "```\n",
    "tokenize_by_character('CC[NH]CC')\n",
    ">> ['C', 'C', '[', 'N', 'H', ']', 'C', 'C']\n",
    "```\n",
    "\n",
    "This form of tokenization is quick and simple. One drawback of this approach is some characters might be overloaded. For example, `Br` is tokenized to `['B', 'r']`, leading to the `B` token meaning both boron (in the standard context) and Bromine (in the `Br` context). In practice, this isn't much of an issue. Language models are particularly adept at learning co-location of tokens.\n",
    "\n",
    "### Character Tokenization with Replacement\n",
    "\n",
    "Character tokenization with replacement is the same as character tokenization except we add a dictionary of multi-character tokens to be replaced with single-character tokens. This dictionary has the form `{multi_character_token : single_character_token}`. Before tokenizing by character, all instances of `multi_character_token` are replaced with `single_character_token`. Character Tokenization with Replacement is implemented with the `tokenize_with_replacements` function.\n",
    "\n",
    "```\n",
    "replacement_dict = {'Br' : 'R', 'Cl' : 'L'}\n",
    "tokenize_with_replacements('[Cl]CC[Br]', replacement_dict)\n",
    ">> ['[', 'L', ']', 'C', 'C', '[', 'R', ']']\n",
    "```\n",
    "\n",
    "### Regex Tokenization\n",
    "\n",
    "Regex tokenization uses a regex string to decompose SMILES. This is mainly used to keep bracketed terms (ie `[O-]`) as single tokens. This method avoids character overloading by keeping all bracketed terms as individual tokens, but has issues with generating a large number of low frequency tokens. Regex tokenization is implemented with the `regex_tokenize` function\n",
    "\n",
    "```\n",
    "SMILE_REGEX = \"\"\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|H|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\"\"\n",
    "regex_tokenize('CCC[Br]', re.compile(SMILE_REGEX))\n",
    ">>['C', 'C', 'C', '[Br]']\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "SMILES_CHAR_VOCAB = ['#', '(', ')', '+', '-', '/', '0',\n",
    "                 '1', '2', '3', '4', '5', '6', '7',\n",
    "                 '8', '9', '=', '@', 'B', 'C', 'F', 'H',\n",
    "                 'I', 'N', 'O', 'P', 'S', '[', '\\\\',\n",
    "                 ']', 'c', 'i', 'l', 'n', 'o', 'r', 's',\n",
    "                 '*', ':', '.', 'a', 'K', 'e']\n",
    "\n",
    "\n",
    "SPECIAL_TOKENS = ['bos', 'eos', 'pad', 'unk']\n",
    "\n",
    "MAPPING_TOKENS = ['[1*:1]', '[2*:1]', '[1*:2]', '[2*:2]', '[1*:3]',\n",
    "                  '[2*:3]', '[1*:4]', '[2*:4]', '[1*:5]', '[2*:5]']\n",
    "\n",
    "HALOGEN_REPLACE = {'Br':'R',\n",
    "                   'Cl':'L'}\n",
    "\n",
    "MAPPING_REPLACE = {'[1*:1]':'A',\n",
    "                   '[2*:1]':'D',\n",
    "                   '[1*:2]':'E',\n",
    "                   '[2*:2]':'G',\n",
    "                   '[1*:3]':'J',\n",
    "                   '[2*:3]':'M',\n",
    "                   '[1*:4]':'Q',\n",
    "                   '[2*:4]':'T',\n",
    "                   '[1*:5]':'U', \n",
    "                   '[2*:5]':'V'}\n",
    "\n",
    "AMINO_ACID_VOCAB = ['A', 'C', 'D', 'E', 'F',\n",
    "                     'G', 'H', 'I', 'K', 'L',\n",
    "                     'M', 'N', 'P', 'Q', 'R',\n",
    "                     'S', 'T', 'V', 'W', 'Y']\n",
    "\n",
    "NUCLEIC_ACID_VOCAB = ['A', 'C', 'G', 'T', 'U', 'N']\n",
    "\n",
    "NUCLEIC_ACID_EXPANDED_VOCAB = ['A', 'C', 'G', 'T', \n",
    "                              'U', 'M', 'R', 'W', \n",
    "                              'S', 'Y', 'K', 'V', \n",
    "                              'H', 'D', 'B', 'N']\n",
    "\n",
    "NUCLEIC_ACID_DIMERS = ['AA', 'AC', 'AG', 'AT', 'AU',\n",
    "                     'AN', 'CA', 'CC', 'CG', 'CT', 'CU',\n",
    "                     'CN', 'GA', 'GC', 'GG', 'GT', 'GU',\n",
    "                     'GN', 'TA', 'TC', 'TG', 'TT', 'TU',\n",
    "                     'TN', 'UA', 'UC', 'UG', 'UT', 'UU',\n",
    "                     'UN', 'NA', 'NC', 'NG', 'NT', 'NU',\n",
    "                     'NN']\n",
    "\n",
    "NUCLEIC_ACID_TRIMERS = ['ACG', 'ACT', 'ACU', 'ACN', 'AGC', 'AGT',\n",
    "                     'AGU', 'AGN', 'ATC', 'ATG', 'ATN', 'AUC', 'AUG',\n",
    "                     'AUN', 'ANC', 'ANG', 'ANT', 'ANU', 'CAG', 'CAT',\n",
    "                     'CAU', 'CAN', 'CGA', 'CGT', 'CGU', 'CGN', 'CTA',\n",
    "                     'CTG', 'CTN', 'CUA', 'CUG', 'CUN', 'CNA', 'CNG',\n",
    "                     'CNT', 'CNU', 'GAC', 'GAT', 'GAU', 'GAN', 'GCA',\n",
    "                     'GCT', 'GCU', 'GCN', 'GTA', 'GTC', 'GTN', 'GUA',\n",
    "                     'GUC', 'GUN', 'GNA', 'GNC', 'GNT', 'GNU', 'TAC',\n",
    "                     'TAG', 'TAN', 'TCA', 'TCG', 'TCN', 'TGA', 'TGC',\n",
    "                     'TGN', 'TNA', 'TNC', 'TNG', 'UAC', 'UAG', 'UAN',\n",
    "                     'UCA', 'UCG', 'UCN', 'UGA', 'UGC', 'UGN', 'UNA',\n",
    "                     'UNC', 'UNG', 'NAC', 'NAG', 'NAT', 'NAU', 'NCA',\n",
    "                     'NCG', 'NCT', 'NCU', 'NGA', 'NGC', 'NGT', 'NGU',\n",
    "                     'NTA', 'NTC', 'NTG', 'NUA', 'NUC', 'NUG']\n",
    "\n",
    "NUCLEIC_ACID_TRIMERS = ['AAA', 'AAC', 'AAG', 'AAT', 'AAU', 'AAN',\n",
    "                     'ACA', 'ACC', 'ACG', 'ACT', 'ACU', 'ACN',\n",
    "                     'AGA', 'AGC', 'AGG', 'AGT', 'AGU', 'AGN',\n",
    "                     'ATA', 'ATC', 'ATG', 'ATT', 'ATU', 'ATN',\n",
    "                     'AUA', 'AUC', 'AUG', 'AUT', 'AUU', 'AUN',\n",
    "                     'ANA', 'ANC', 'ANG', 'ANT', 'ANU', 'ANN',\n",
    "                     'CAA', 'CAC', 'CAG', 'CAT', 'CAU', 'CAN',\n",
    "                     'CCA', 'CCC', 'CCG', 'CCT', 'CCU', 'CCN',\n",
    "                     'CGA', 'CGC', 'CGG', 'CGT', 'CGU', 'CGN',\n",
    "                     'CTA', 'CTC', 'CTG', 'CTT', 'CTU', 'CTN',\n",
    "                     'CUA', 'CUC', 'CUG', 'CUT', 'CUU', 'CUN',\n",
    "                     'CNA', 'CNC', 'CNG', 'CNT', 'CNU', 'CNN',\n",
    "                     'GAA', 'GAC', 'GAG', 'GAT', 'GAU', 'GAN',\n",
    "                     'GCA', 'GCC', 'GCG', 'GCT', 'GCU', 'GCN',\n",
    "                     'GGA', 'GGC', 'GGG', 'GGT', 'GGU', 'GGN',\n",
    "                     'GTA', 'GTC', 'GTG', 'GTT', 'GTU', 'GTN',\n",
    "                     'GUA', 'GUC', 'GUG', 'GUT', 'GUU', 'GUN',\n",
    "                     'GNA', 'GNC', 'GNG', 'GNT', 'GNU', 'GNN',\n",
    "                     'TAA', 'TAC', 'TAG', 'TAT', 'TAU', 'TAN',\n",
    "                     'TCA', 'TCC', 'TCG', 'TCT', 'TCU', 'TCN',\n",
    "                     'TGA', 'TGC', 'TGG', 'TGT', 'TGU', 'TGN',\n",
    "                     'TTA', 'TTC', 'TTG', 'TTT', 'TTU', 'TTN',\n",
    "                     'TUA', 'TUC', 'TUG', 'TUT', 'TUU', 'TUN',\n",
    "                     'TNA', 'TNC', 'TNG', 'TNT', 'TNU', 'TNN',\n",
    "                     'UAA', 'UAC', 'UAG', 'UAT', 'UAU', 'UAN',\n",
    "                     'UCA', 'UCC', 'UCG', 'UCT', 'UCU', 'UCN',\n",
    "                     'UGA', 'UGC', 'UGG', 'UGT', 'UGU', 'UGN',\n",
    "                     'UTA', 'UTC', 'UTG', 'UTT', 'UTU', 'UTN',\n",
    "                     'UUA', 'UUC', 'UUG', 'UUT', 'UUU', 'UUN',\n",
    "                     'UNA', 'UNC', 'UNG', 'UNT', 'UNU', 'UNN',\n",
    "                     'NAA', 'NAC', 'NAG', 'NAT', 'NAU', 'NAN',\n",
    "                     'NCA', 'NCC', 'NCG', 'NCT', 'NCU', 'NCN',\n",
    "                     'NGA', 'NGC', 'NGG', 'NGT', 'NGU', 'NGN',\n",
    "                     'NTA', 'NTC', 'NTG', 'NTT', 'NTU', 'NTN',\n",
    "                     'NUA', 'NUC', 'NUG', 'NUT', 'NUU', 'NUN',\n",
    "                     'NNA', 'NNC', 'NNG', 'NNT', 'NNU', 'NNN']\n",
    "\n",
    "DNA_VOCAB = ['A', 'C', 'G', 'T']\n",
    "\n",
    "DNA_DIMERS = ['AA', 'AC', 'AG', 'AT',\n",
    "             'CA', 'CC', 'CG', 'CT',\n",
    "             'GA', 'GC', 'GG', 'GT',\n",
    "             'TA', 'TC', 'TG', 'TT']\n",
    "\n",
    "DNA_TRIMERS = ['AAA', 'AAC', 'AAG', 'AAT',\n",
    " 'ACA', 'ACC', 'ACG', 'ACT', 'AGA',\n",
    " 'AGC', 'AGG', 'AGT', 'ATA', 'ATC',\n",
    " 'ATG', 'ATT', 'CAA', 'CAC', 'CAG',\n",
    " 'CAT', 'CCA', 'CCC', 'CCG', 'CCT',\n",
    " 'CGA', 'CGC', 'CGG', 'CGT', 'CTA',\n",
    " 'CTC', 'CTG', 'CTT', 'GAA', 'GAC',\n",
    " 'GAG', 'GAT', 'GCA', 'GCC', 'GCG',\n",
    " 'GCT', 'GGA', 'GGC', 'GGG', 'GGT',\n",
    " 'GTA', 'GTC', 'GTG', 'GTT', 'TAA',\n",
    " 'TAC', 'TAG', 'TAT', 'TCA', 'TCC',\n",
    " 'TCG', 'TCT', 'TGA', 'TGC', 'TGG',\n",
    " 'TGT', 'TTA', 'TTC', 'TTG', 'TTT']\n",
    "\n",
    "SELFIES_VOCAB = ['[C]', '[Ring1]', '[=C]', '[Branch1_1]',\n",
    "             '[N]', '[Branch1_2]', '[=O]', '[O]', '[Branch2_1]',\n",
    "             '[=N]', '[Ring2]', '[C@Hexpl]', '[C@@Hexpl]', '[F]',\n",
    "             '[S]', '[Branch1_3]', '[Branch2_2]', '[Branch2_3]', '[#C]',\n",
    "             '[Expl=Ring1]', '[P]', '[Cl]', '[NHexpl]', '[Br]',\n",
    "             '[/C]', '[C@expl]', '[C@@expl]', '[#N]', '[O-expl]',\n",
    "             '[N+expl]', '[Expl=Ring2]', '[\\\\C]', '[=S]', '[I]',\n",
    "             '[S@expl]', '[S@@expl]', '[=N+expl]', '[/N]', '[/Cl]',\n",
    "             '[\\\\Cl]', '[/O]', '[/S]', '[Siexpl]', '[\\\\S]',\n",
    "             '[=S@expl]', '[=S@@expl]', '[\\\\N]', '[/C@@Hexpl]', '[/C@Hexpl]',\n",
    "             '[\\\\O]', '[\\\\C@Hexpl]', '[\\\\C@@Hexpl]', '[B]', '[/F]',\n",
    "             '[/C@expl]', '[\\\\C@expl]', '[CHexpl]', '[\\\\F]', '[P@expl]',\n",
    "             '[Cexpl]', '[/C@@expl]', '[\\\\C@@expl]', '[=P]', '[P@@expl]',\n",
    "             '[/NH+expl]', '[/S-expl]', '[=NH+expl]', '[N-expl]', '[NH+expl]',\n",
    "             '[NH2+expl]', '[NH3+expl]', '[S-expl]', '[\\\\NHexpl]', '[\\\\O-expl]', \n",
    "             '[\\\\S-expl]']\n",
    "\n",
    "# includes tokens that appear <500 times in a dataset of 79 million compounds\n",
    "SELFIES_EXPANDED_VOCAB = ['[C]', '[Ring1]', '[=C]',\n",
    "             '[Branch1_1]', '[N]', '[Branch1_2]', '[=O]', '[O]', '[Branch2_1]',\n",
    "             '[=N]', '[Ring2]', '[C@Hexpl]', '[C@@Hexpl]', '[F]', '[S]',\n",
    "             '[Branch1_3]', '[Branch2_2]', '[Branch2_3]', '[#C]', '[Expl=Ring1]', '[P]',\n",
    "             '[Cl]', '[NHexpl]', '[Br]', '[/C]', '[C@expl]', '[C@@expl]',\n",
    "             '[#N]', '[O-expl]', '[N+expl]', '[Expl=Ring2]', '[\\\\C]', '[=S]',\n",
    "             '[I]', '[S@expl]', '[S@@expl]', '[=N+expl]', '[/N]', '[/Cl]',\n",
    "             '[\\\\Cl]', '[/O]', '[/S]', '[Siexpl]', '[\\\\S]', '[=S@expl]',\n",
    "             '[=S@@expl]', '[\\\\N]', '[/C@@Hexpl]', '[/C@Hexpl]', '[\\\\O]', '[\\\\C@Hexpl]',\n",
    "             '[\\\\C@@Hexpl]', '[B]', '[/F]', '[/C@expl]', '[\\\\C@expl]', '[CHexpl]',\n",
    "             '[\\\\F]', '[P@expl]', '[Cexpl]', '[/C@@expl]', '[\\\\C@@expl]', '[=P]',\n",
    "             '[P@@expl]', '[/Br]', '[=N-expl]', '[/N+expl]', '[S+expl]', '[\\\\NHexpl]',\n",
    "             '[\\\\Br]', '[/NHexpl]', '[N@+expl]', '[/S@expl]', '[N@@+expl]', '[N-expl]',\n",
    "             '[/S@@expl]', '[CH2expl]', '[=P@expl]', '[Oexpl]', '[Snexpl]', '[\\\\S@expl]',\n",
    "             '[C-expl]', '[/B]', '[\\\\N+expl]', '[#N+expl]', '[=P@@expl]', \n",
    "             '[/NH+expl]', '[/S-expl]', '[=NH+expl]', '[N-expl]', '[NH+expl]',\n",
    "             '[NH2+expl]', '[NH3+expl]', '[S-expl]', '[\\\\NHexpl]', '[\\\\O-expl]', \n",
    "             '[\\\\S-expl]', '[CH-expl]',\n",
    "             '[\\\\O-expl]', '[Expl/Ring2]', '[/Oexpl]', '[B-expl]', '[S@@+expl]', '[=S+expl]',\n",
    "             '[P+expl]', '[/O-expl]', '[PHexpl]', '[=S@+expl]', '[P@@Hexpl]', '[\\\\I]',\n",
    "             '[Expl/Ring1]', '[Expl\\\\Ring2]', '[S@+expl]', '[/I]', '[Nexpl]', '[=B]',\n",
    "             '[=O+expl]', '[O+expl]', '[CH2-expl]', '[B@-expl]', '[=S@@+expl]', '[B@@-expl]',\n",
    "             '[\\\\B]', '[/S+expl]', '[SHexpl]', '[\\\\S@@expl]', '[\\\\P@@expl]', '[/P@expl]',\n",
    "             '[=P@@Hexpl]', '[\\\\P@expl]', '[/P@@expl]', '[/Siexpl]', '[=17Oexpl]', '[=Nexpl]',\n",
    "             '[I+expl]', '[=P@Hexpl]', '[\\\\Snexpl]', '[\\\\C-expl]', '[=SHexpl]', '[\\\\Siexpl]',\n",
    "             '[SnH4+2expl]', '[Sn+expl]', '[=Snexpl]', '[=P+expl]', '[C+expl]', '[N@@H+expl]',\n",
    "             '[Sn+3expl]', '[/C-expl]', '[/Cexpl]', '[BH3-expl]', '[\\\\CH-expl]', '[=Siexpl]',\n",
    "             '[/CHexpl]', '[/Snexpl]', '[BH2-expl]', '[\\\\Cexpl]', '[\\\\P]', '[=PHexpl]',\n",
    "             '[#N+expl]', '[#NH+expl]', '[#PHexpl]', '[#P]', '[#Pexpl]', '[#SHexpl]',\n",
    "             '[#S]', '[#Sexpl]', '[/Br]', '[/CHexpl]', '[/Cexpl]', '[/N+expl]',\n",
    "             '[/NHexpl]', '[/O-expl]', '[/PHexpl]', '[/P]', '[/SHexpl]', '[=CHexpl]', '[=Cexpl]',\n",
    "             '[=N-expl]', '[=P+expl]', '[=P@@expl]', '[=P@expl]', '[=PHexpl]', '[=Pexpl]',\n",
    "             '[=S-expl]', '[=SHexpl]', '[=Sexpl]', '[=Siexpl]', '[Expl#Ring1]', '[Expl#Ring2]',\n",
    "             '[Expl/Ring1]', '[Expl/Ring2]', '[Expl\\\\Ring1]', '[Expl\\\\Ring2]', '[P+expl]', '[PHexpl]',\n",
    "             '[Pexpl]', '[SHexpl]', '[Sexpl]', '[\\\\Br]', '[\\\\CHexpl]', '[\\\\Cexpl]',\n",
    "             '[\\\\I]', '[\\\\N+expl]', '[\\\\SHexpl]', '[\\\\Siexpl]']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def pad_vocab(vocab):\n",
    "    '''\n",
    "    pads `vocab` to have a length divisible by 8 - improves fp16 performance\n",
    "    '''\n",
    "    if not len(vocab)%8==0:\n",
    "        final_length = np.ceil(len(vocab)/8)*8\n",
    "        to_add = len(vocab) - final_length\n",
    "        vocab = vocab + ['extra']*to_add\n",
    "        \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are regex patterns to decompose smiles into tokens\n",
    "\n",
    "`SMILE_REGEX` is based off [this work](https://github.com/pschwllr/MolecularTransformer/blob/master/README.md). The pattern decomposes SMILES into individual characters, but keeps `Cl`, `Br`, and any term in brackets (ie `[O-]`) intact. \n",
    "\n",
    "`MAPPING_REGEX` is a derivative of `SMILE_REGEX` designed to work with the mapping framework used with the `Block` class. `MAPPING_REGEX` keeps `Cl`, `Br`, and any string of the form `[{isotope}*:{map_num}]` intact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "SMILE_REGEX = \"\"\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|H|\\(|\\)|\\.|=|\n",
    "                 #|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|#|\\*|\\$|\\%[0-9]{2}|[0-9])\"\"\"\n",
    "\n",
    "MAPPING_REGEX = \"\"\"(\\[.\\*:.]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|H|\\[|\\]|\\(|\\)|\\.|=|\n",
    "                    #|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|#|\\*|\\$|\\%[0-9]{2}|[0-9])\"\"\"\n",
    "\n",
    "AA_MAPPING_REGEX = \"\"\"(\\[.\\*:.]|A|C|D|E|F|G|H|I|K|L|M|N|P|Q|R|S|T|V|W|Y)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def tokenize_by_character(input):\n",
    "    \"Splits `input` into inividual characters\"\n",
    "    unks = False\n",
    "    if 'unk' in input:\n",
    "        input = input.replace('unk', '_')\n",
    "        unks = True\n",
    "    tokens = [i for i in input]\n",
    "    if unks:\n",
    "        for i, item in enumerate(tokens):\n",
    "            if item=='_':\n",
    "                tokens[i] = 'unk'\n",
    "    return tokens\n",
    "\n",
    "def tokenize_with_replacements(input, replacement_dict):\n",
    "    \"Replaces substrings in `input` using `replacement_dict`, then tokenizes by character\"\n",
    "    for k,v in replacement_dict.items():\n",
    "        input = input.replace(k,v)\n",
    "    return [i for i in input]\n",
    "\n",
    "def regex_tokenize(input, regex):\n",
    "    'Uses `regex` to tokenize `input`'\n",
    "    tokens = [token for token in regex.findall(input)]\n",
    "    return tokens\n",
    "\n",
    "def tokenize_by_kmer(input, kmer, stride):\n",
    "    tokens = [input[i:i+kmer] for i in range(0, len(input), stride)]\n",
    "    if len(tokens[-1]) != kmer:\n",
    "        tokens = tokens[:-1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenize_by_character('CCC[Br]') == ['C', 'C', 'C', '[', 'B', 'r', ']']\n",
    "assert tokenize_with_replacements('CCC[Br]', HALOGEN_REPLACE) == ['C', 'C', 'C', '[', 'R', ']']\n",
    "assert regex_tokenize('CCC[Br]', re.compile(SMILE_REGEX)) == ['C', 'C', 'C', '[Br]']\n",
    "assert regex_tokenize('[1*:1]CCC[Br]', re.compile(MAPPING_REGEX)) == ['[1*:1]', 'C', 'C', 'C', '[', 'Br', ']']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "The `Vocab` class handles tokenization. `Vocab.tokenize` breaks strings down into tokens. `Vocab.numericalize` maps tokens to integers. `Vocab.reconstruct` converts integers back into strings.\n",
    "\n",
    "`Vocab` holds `itos`, a list of tokens, and `stoi`, a dictionary mapping tokens to integers. `Vocab` automatically adds four special tokens `['bos', 'eos', 'pad', 'unk']` indicating beginning of sentence, end of sentence, padding and unknown.\n",
    "\n",
    "### Custom Vocbulary\n",
    "\n",
    "To implement custom tokenization, subclass `Vocab` and update the `tokenize`, `numericalize` and `reconstruct` methods. Use the `test_reconstruction` function to verify your custom vocab can successfully reconstruct sequences.\n",
    "\n",
    "Vocabs also have `prefunc` and `postfunc` hooks for added flexibility. `prefunc` is called on the inputs to `tokenize` before tokenization. `postfunc` is called during `reconstruct` after tokens are joined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Vocab():\n",
    "    '''\n",
    "    Vocab - base vocabulary class\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `itos list`: list of tokens in vocabulary\n",
    "        \n",
    "    - `prefunc Optional[Callable]`: function applied to `input` before tokenization\n",
    "        \n",
    "    - `postfunc Optional[Callable]`: function applied to `input` after reconstruction\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, itos, prefunc=None, postfunc=None):\n",
    "        self.special_tokens = ['bos', 'eos', 'pad', 'unk']\n",
    "        \n",
    "        self.itos = self.special_tokens + [i for i in itos if not i in self.special_tokens]\n",
    "        self.stoi = {self.itos[i]:i for i in range(len(self.itos))}\n",
    "        self.unks = set()\n",
    "        self.prefunc = prefunc\n",
    "        self.postfunc = postfunc\n",
    "        \n",
    "    def _tokenize(self, input):\n",
    "        'Tokenize `input`'\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def tokenize(self, input):\n",
    "        input = self.preprocess(input)\n",
    "        toks = self._tokenize(input)\n",
    "        toks = ['bos'] + toks + ['eos']\n",
    "        return toks\n",
    "    \n",
    "    def join_tokens(self, tokens):\n",
    "        return ''.join(tokens)\n",
    "        \n",
    "    def preprocess(self, input):\n",
    "        if self.prefunc is not None:\n",
    "            input = self.prefunc(input)\n",
    "        return input\n",
    "    \n",
    "    def postprocess(self, input):\n",
    "        if self.postfunc is not None:\n",
    "            input = self.postfunc(input)\n",
    "        return input\n",
    "        \n",
    "    def numericalize(self, input):\n",
    "        'Numericalize `input` into integers'\n",
    "        output = []\n",
    "        for tok in input:\n",
    "            if tok in self.stoi.keys():\n",
    "                output.append(self.stoi[tok])\n",
    "            else:\n",
    "                output.append(self.stoi['unk'])\n",
    "                self.unks.add(tok)\n",
    "        return output\n",
    "    \n",
    "    def _reconstruct(self, input):\n",
    "        'Reconstruct `input` into a string'\n",
    "        output = []\n",
    "        for item in input:\n",
    "            item = self.itos[item]\n",
    "            if item=='eos':\n",
    "                break\n",
    "                \n",
    "            if (not item=='bos') and (not item=='pad'):\n",
    "                output.append(item)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def reconstruct(self, input):\n",
    "        tokens = self._reconstruct(input)\n",
    "        output = self.join_tokens(tokens)\n",
    "        output = self.postprocess(output)\n",
    "        return output\n",
    "    \n",
    "    def reconstruct_trajectory(self, input):\n",
    "        tokens = self._reconstruct(input)\n",
    "        return [self.join_tokens(tokens[:i]) for i in range(1,len(tokens)+1)]\n",
    "                \n",
    "    def update_vocab(self):\n",
    "        'Adds tokens in `self.unks` to vocabulary'\n",
    "        unks = list(self.unks)\n",
    "        self.itos += unks\n",
    "        self.stoi = {self.itos[i]:i for i in range(len(self.itos))}\n",
    "        self.unks = set()\n",
    "        \n",
    "    def update_vocab_from_data(self, inputs):\n",
    "        'Tokenizes `inputs` and updates the vocabulary with any unknown tokens'\n",
    "        _ = [self.numericalize(self.tokenize(i)) for i in inputs]\n",
    "        self.update_vocab()\n",
    "        \n",
    "        \n",
    "class CharacterVocab(Vocab):\n",
    "    '''\n",
    "    CharacterVocab - tokenize by character\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `itos list`: list of tokens in vocabulary\n",
    "        \n",
    "    - `prefunc Optional[Callable]`: function applied to `input` before tokenization\n",
    "        \n",
    "    - `postfunc Optional[Callable]`: function applied to `input` after reconstruction\n",
    "    '''\n",
    "    def _tokenize(self, input):\n",
    "        toks = tokenize_by_character(input)\n",
    "        return toks\n",
    "\n",
    "class FuncVocab(Vocab):\n",
    "    '''\n",
    "    FuncVocab - tokenize by `tok_func`\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `itos list`: list of tokens in vocabulary\n",
    "    \n",
    "    - `tok_func Callable`: tokenization function\n",
    "        \n",
    "    - `prefunc Optional[Callable]`: function applied to `input` before tokenization\n",
    "        \n",
    "    - `postfunc Optional[Callable]`: function applied to `input` after reconstruction\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, itos, tok_func, prefunc=None, postfunc=None):\n",
    "        super().__init__(itos, prefunc, postfunc)\n",
    "        self.tok_func = tok_func\n",
    "    \n",
    "    def _tokenize(self, input):\n",
    "        toks = self.tok_func(input)\n",
    "        return toks\n",
    "    \n",
    "    \n",
    "class SelfiesVocab(FuncVocab):\n",
    "    '''\n",
    "    SelfiesVocab - converts smiles to selfies \n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `itos list`: list of tokens in vocabulary\n",
    "    '''\n",
    "    def __init__(self, itos):\n",
    "        super().__init__(itos, split_selfie, smile_to_selfie, selfie_to_smile)\n",
    "        \n",
    "    \n",
    "class CharacterReplaceVocab(Vocab):\n",
    "    '''\n",
    "    CharacterReplaceVocab - tokenize by character with replacement\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `itos list`: list of tokens in vocabulary\n",
    "        \n",
    "    - `replace_dict dict`: replacement dictionary of the form \n",
    "    {multi_character_token : single_character_token}. \n",
    "    ie replace_dict={'Br':'R', 'Cl':'L'}\n",
    "        \n",
    "    - `prefunc Optional[Callable]`: function applied to `input` before tokenization\n",
    "        \n",
    "    - `postfunc Optional[Callable]`: function applied to `input` after reconstruction\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, itos, replace_dict, prefunc=None, postfunc=None):\n",
    "        itos = list(itos)\n",
    "        self.replace_dict = replace_dict\n",
    "        if not 'unk' in self.replace_dict.keys():\n",
    "            self.replace_dict['unk'] = '_'\n",
    "        \n",
    "        self.reverse_dict = {v:k for k,v in replace_dict.items()}\n",
    "        for rep in self.reverse_dict.keys():\n",
    "            if not rep in itos:\n",
    "                itos.append(rep)\n",
    "        super().__init__(itos, prefunc, postfunc)\n",
    "        \n",
    "    def _tokenize(self, smile):\n",
    "        toks = tokenize_with_replacements(smile, self.replace_dict)\n",
    "        return toks\n",
    "    \n",
    "    def _reconstruct(self, input):\n",
    "        output = []\n",
    "        for item in input:\n",
    "            item = self.itos[item]\n",
    "            if item=='eos':\n",
    "                break\n",
    "            \n",
    "            if (not item=='bos') and (not item=='pad'):\n",
    "                if item in self.reverse_dict.keys():\n",
    "                    item = self.reverse_dict[item]\n",
    "\n",
    "                output.append(item)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "class RegexVocab(Vocab):\n",
    "    '''\n",
    "    RegexVocab - tokenize using `pattern`\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `itos list`: list of tokens in vocabulary\n",
    "        \n",
    "    - `pattern str`: regex string\n",
    "        \n",
    "    - `prefunc Optional[Callable]`: function applied to `input` before tokenization\n",
    "        \n",
    "    - `postfunc Optional[Callable]`: function applied to `input` after reconstruction\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, itos, pattern, prefunc=None, postfunc=None):\n",
    "        super().__init__(itos, prefunc, postfunc)\n",
    "        \n",
    "        self.pattern = pattern\n",
    "        self.regex = re.compile(self.pattern)\n",
    "        \n",
    "    def _tokenize(self, smile):\n",
    "        toks = regex_tokenize(smile, self.regex)\n",
    "        return toks\n",
    "    \n",
    "    \n",
    "class KmerVocab(Vocab):\n",
    "    '''\n",
    "    KmerVocab - Kmer tokenization vocabulary\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `itos list`: list of tokens in vocabulary\n",
    "    \n",
    "    - `kmer int`: kmer size\n",
    "    \n",
    "    - `stride Optional[int]`: kmer stride. If not passed, stride \n",
    "    will be the same as kmer. Using a stride value different from \n",
    "    the kmer value will prevent proper reconstruction\n",
    "        \n",
    "    - `prefunc Optional[Callable]`: function applied to `input` before tokenization\n",
    "        \n",
    "    - `postfunc Optional[Callable]`: function applied to `input` after reconstruction\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, itos, kmer, stride=None, prefunc=None, postfunc=None):\n",
    "        super().__init__(itos, prefunc, postfunc)\n",
    "        self.kmer = kmer\n",
    "        \n",
    "        if stride is None:\n",
    "            stride = kmer\n",
    "            \n",
    "        self.stride = stride\n",
    "        \n",
    "    def _tokenize(self, input):\n",
    "        toks = tokenize_by_kmer(input, self.kmer, self.stride)\n",
    "        return toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def test_reconstruction(vocab, inputs):\n",
    "    \"Returns all items in `inputs` that can't be correctly reconstructed using `vocab`\"\n",
    "    fails = []\n",
    "    for item in inputs:\n",
    "        recon = vocab.reconstruct(vocab.numericalize(vocab.tokenize(item)))\n",
    "        if not item==recon:\n",
    "            fails.append((item, recon))\n",
    "            \n",
    "    return fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('files/smiles.csv')\n",
    "smiles = df.smiles.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = CharacterVocab(SMILES_CHAR_VOCAB)\n",
    "assert test_reconstruction(vocab, smiles)==[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = FuncVocab(SMILES_CHAR_VOCAB, tokenize_by_character)\n",
    "assert test_reconstruction(vocab, smiles)==[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = CharacterReplaceVocab(SMILES_CHAR_VOCAB, HALOGEN_REPLACE)\n",
    "assert vocab.tokenize('CC[Br]') == ['bos', 'C', 'C', '[', 'R', ']', 'eos']\n",
    "assert test_reconstruction(vocab, smiles)==[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = RegexVocab(SMILES_CHAR_VOCAB, SMILE_REGEX)\n",
    "assert vocab.tokenize('CC[Br]') == ['bos', 'C', 'C', '[Br]', 'eos']\n",
    "vocab.update_vocab_from_data(smiles)\n",
    "assert test_reconstruction(vocab, smiles)==[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab Example - Removing Stereochemistry\n",
    "\n",
    "Stereochemistry is represented in SMILES strings using the `@` character. For example, `C[C@H](N)C(=O)O` and `C[C@@H](N)C(=O)O` represent two different stereoisomers for the same compound.\n",
    "\n",
    "Stereochemistry in SMILES strings can lead to interesting outcomes for ML models. Predictive models can overfit to specific stereocenters present in training data, and generative models can fall into a form of mode collapse of predicting different stereoisomers of the same compound, or generating excessive stereocenters.\n",
    "\n",
    "For these reasons, we may wish to deal with generic SMILES strings without stereochemistry information.\n",
    "\n",
    "One way to do this is to bulk preprocess datasets to remove stereochemistry information, but this requires storing a copy of the stereochemistry-free data, which can be prohibitive for large datasets.\n",
    "\n",
    "Another way would be to apply stereochemistry removal as a `prefunc` in our `Vocab`, which will remove stereochemistry on the fly before tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrl.chem import *\n",
    "\n",
    "V1 = CharacterVocab(SMILES_CHAR_VOCAB)\n",
    "V2 = CharacterVocab(SMILES_CHAR_VOCAB, prefunc=remove_stereo)\n",
    "\n",
    "assert '@' in V1.tokenize('C[C@H](N)C(=O)O')\n",
    "assert not '@' in V2.tokenize('C[C@H](N)C(=O)O')\n",
    "assert V1.reconstruct(V1.numericalize(V1.tokenize('C[C@H](N)C(=O)O'))) == 'C[C@H](N)C(=O)O'\n",
    "assert V2.reconstruct(V2.numericalize(V2.tokenize('C[C@H](N)C(=O)O'))) == 'CC(N)C(=O)O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab Example - SELFIES Vocab\n",
    "\n",
    "[SELFIES](https://github.com/aspuru-guzik-group/selfies) is molecule text representation alternative to SMILES strings. One advantage of using SELFIES representations is token swaps in SELFIES strings always result in valid compounds.\n",
    "\n",
    "MRL is standardized around using SMILES representations for working with compounds, so we can't use SELFIES representations for RL training (unless you want to write a parallel SELFIES-compatible version of all the comp chem functions).\n",
    "\n",
    "Luckily, we can make use of `prefunc` and `postfunc` utilities to use SELFIES representations in our generative models. We can use `smile_to_selfie` as a prefunc and `selfie_to_smile` as a postfunc.\n",
    "\n",
    "This means we can keep all our data in SMILES forms for compatibility. When we process SMILES for the model, we first use the `prefunc` to convert SMILES to SELFIES. Then we tokenize, numericalize and train models in selfies space. Then when we reconstruct a sequence, we use the `postfunc` to convert it back into SMILES strings.\n",
    "\n",
    "From the outside perspective, the model takes in and produces SMILES strings. But internally, everything is in SELFIES space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILE: COc1ccc2[nH]cc(CCNC(C)=O)c2c1\n",
      "SELFIE: [C][O][C][C][=C][C][NHexpl][C][=C][Branch1_1][Branch2_2][C][C][N][C][Branch1_1][C][C][=O][C][Expl=Ring1][O][C][Expl=Ring1][#C]\n"
     ]
    }
   ],
   "source": [
    "from mrl.chem import *\n",
    "\n",
    "vocab = FuncVocab(SELFIES_VOCAB, split_selfie, \n",
    "                  prefunc=smile_to_selfie, postfunc=selfie_to_smile)\n",
    "\n",
    "smile = 'COc1ccc2[nH]cc(CCNC(C)=O)c2c1'\n",
    "print(f'SMILE: {smile}')\n",
    "selfie = vocab.prefunc(smile)\n",
    "print(f'SELFIE: {selfie}')\n",
    "assert vocab.postfunc(vocab.prefunc(smile)) == smile # note - only works of smile is canonicalized\n",
    "\n",
    "full_recon = vocab.reconstruct(vocab.numericalize(vocab.tokenize(smile)))\n",
    "partial_recon = vocab.join_tokens(vocab._reconstruct(vocab.numericalize(vocab.tokenize(smile))))\n",
    "\n",
    "assert full_recon == smile\n",
    "assert partial_recon == selfie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
