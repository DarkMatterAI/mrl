{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses\n",
    "\n",
    "> Loss callback functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from mrl.imports import *\n",
    "from mrl.core import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *\n",
    "from mrl.callbacks import *\n",
    "from mrl.policy_gradient import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function Callbacks\n",
    "\n",
    "Loss function callbacks compute some loss value from the current batch state and add the resulting value to `BatchState.loss`.\n",
    "\n",
    "`LossCallback` provides a simple hook for custom loss functions. Any object with a `from_batch_state` method that returns a scalar value can be passed to `LossCallback`. Ex:\n",
    "\n",
    "```\n",
    "class MyLoss():\n",
    "    def from_batch_state(self, batch_state):\n",
    "        loss = self.do_loss_calculation()\n",
    "        return loss\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LossCallback(Callback):\n",
    "    '''\n",
    "    LossCallback - basic loss callback\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `loss_function`: any object with a `from_batch_state` method\n",
    "    \n",
    "    - `name str`: loss name\n",
    "    \n",
    "    - `weight float`: loss scaling weight\n",
    "    \n",
    "    - `track bool`: if values from this loss should be tracked\n",
    "    \n",
    "    - `order int`: callback order\n",
    "    '''\n",
    "    def __init__(self, loss_function, name, weight=1., track=True, order=20):\n",
    "        super().__init__(name=name, order=order)\n",
    "        self.loss_function = loss_function\n",
    "        self.weight = weight\n",
    "        self.track = track\n",
    "        \n",
    "    def setup(self):\n",
    "        if self.track:\n",
    "            log = self.environment.log\n",
    "            log.add_metric(self.name)\n",
    "            log.add_log(self.name)\n",
    "        \n",
    "    def compute_loss(self):\n",
    "        loss = self.loss_function.from_batch_state(self.environment.batch_state)\n",
    "        loss = loss * self.weight\n",
    "        \n",
    "        if self.track:\n",
    "            self.environment.log.update_metric(self.name, loss.mean().detach().cpu().numpy())\n",
    "            \n",
    "        self.environment.batch_state.loss += loss.mean()\n",
    "        self.environment.batch_state[self.name] = loss.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Loss\n",
    "\n",
    "The `PolicyLoss` interfaces with any of the `BasePolicy` subclasses like `PolicyGradient`, `TRPO` or `PPO`.\n",
    "\n",
    "`PolicyLoss` can optionally contain a `value_head`, a model to predict state values. The code will look for a `batch_state.value_input` attribute, which is typically set by `Agent.get_model_outputs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class PolicyLoss(LossCallback):\n",
    "    '''\n",
    "    PolicyLoss - Loss callback for `BasePolicy` subclasses\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `policy_function BasePolicy`: policy\n",
    "    \n",
    "    - `name str`: loss name\n",
    "    \n",
    "    - `value_head Optional[nn.Module]`: state value prediction model\n",
    "    \n",
    "    - `v_update float`: update fraction for the baseline value head\n",
    "    \n",
    "    - `v_update_iter int`: update frequency for baseline value head \n",
    "    \n",
    "    - `vopt_kwargs dict`: dictionary of keyword args passed to `optim.Adam`\n",
    "    \n",
    "    - `track bool`: if values from this loss should be tracked\n",
    "    '''\n",
    "    def __init__(self, policy_function, name, value_head=None, \n",
    "                 v_update=0.95, v_update_iter=10,\n",
    "                 vopt_kwargs={}, track=True):\n",
    "        assert isinstance(policy_function, BasePolicy)\n",
    "        super().__init__(policy_function, name, weight=1., track=track)\n",
    "        \n",
    "        self.set_model(value_head, vopt_kwargs)\n",
    "        self.v_update = v_update\n",
    "        self.v_update_iter = v_update_iter\n",
    "        self.fields = [\n",
    "            'model_gathered_logprobs',\n",
    "            'base_gathered_logprobs',\n",
    "            'mask',\n",
    "            'trajectory_rewards',\n",
    "            'model_logprobs',\n",
    "            'base_logprobs',\n",
    "            'value_input'\n",
    "        ]\n",
    "        \n",
    "    def set_model(self, value_head, vopt_kwargs):\n",
    "        self.value_head = value_head\n",
    "        if self.value_head is not None:\n",
    "            self.base_value_head = copy.deepcopy(self.value_head)\n",
    "            to_device(self.value_head)\n",
    "            to_device(self.base_value_head)\n",
    "            self.opt = optim.Adam(self.value_head.parameters(), **vopt_kwargs)\n",
    "        else:\n",
    "            self.opt = None\n",
    "            \n",
    "    def after_sample(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        for field in self.fields:\n",
    "            if not hasattr(batch_state, field):\n",
    "                batch_state[field] = None\n",
    "                \n",
    "    def get_model_outputs(self):\n",
    "            \n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        value_input = batch_state.value_input\n",
    "        \n",
    "        if (self.value_head is not None) and (value_input is not None):\n",
    "            value_predictions = self.value_head(value_input)\n",
    "            with torch.no_grad():\n",
    "                base_value_predictions = self.base_value_head(value_input)\n",
    "                \n",
    "        else:\n",
    "            value_predictions = None\n",
    "            base_value_predictions = None\n",
    "            \n",
    "        batch_state.state_values = value_predictions\n",
    "        batch_state.ref_state_values = base_value_predictions\n",
    "            \n",
    "    def zero_grad(self):\n",
    "        if self.opt is not None:\n",
    "            self.opt.zero_grad()\n",
    "        \n",
    "    def step(self):\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "        \n",
    "    def after_batch(self):\n",
    "        log = self.environment.log\n",
    "        iterations = log.iterations\n",
    "        if iterations%self.v_update_iter == 0 and iterations>0:\n",
    "            self.update_base_model()\n",
    "            \n",
    "    def update_base_model(self):\n",
    "        if self.value_head is not None:\n",
    "            if self.v_update < 1:\n",
    "                merge_models(self.base_value_head, self.value_head, alpha=self.v_update)\n",
    "                \n",
    "    def save_weights(self, filename):\n",
    "        state_dict = {}\n",
    "            \n",
    "        if isinstance(self.value_head, nn.Module):\n",
    "            state_dict['value_head'] = self.value_head.state_dict()\n",
    "            state_dict['base_value_head'] = self.base_value_head.state_dict()\n",
    "        else:\n",
    "            state_dict['value_head'] = None\n",
    "            state_dict['base_value_head'] = None\n",
    "            \n",
    "        torch.save(state_dict, filename)\n",
    "        \n",
    "    def load_weights(self, filename):\n",
    "        state_dict = torch.load(filename, map_location=get_model_device(self.model))\n",
    "            \n",
    "        if isinstance(self.value_head, nn.Module):\n",
    "            self.value_head.load_state_dict(state_dict['value_head'])\n",
    "            self.base_value_head.load_state_dict(state_dict['base_value_head'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class PriorLoss():\n",
    "    '''\n",
    "    PriorLoss - loss for a trainable prior\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `prior nn.Module`: trainable prior\n",
    "    \n",
    "    - `base_prior Optional[nn.Module]`: optional baseline prior\n",
    "    \n",
    "    - `clip float`: loss clipping value\n",
    "    '''\n",
    "    def __init__(self, prior, base_prior=None, clip=10.):\n",
    "        \n",
    "        self.prior = prior\n",
    "        self.base_prior = base_prior\n",
    "        self.clip = clip\n",
    "        \n",
    "    def loss(self, z, rewards):\n",
    "        rewards = rewards-rewards.mean()\n",
    "        \n",
    "        prior_lps = self.prior.log_prob(z)\n",
    "        \n",
    "        if self.base_prior is not None:\n",
    "            with torch.no_grad():\n",
    "                base_lps = self.base_prior.log_prob(z.detach())\n",
    "            \n",
    "            ratios = prior_lps - base_lps.detach()\n",
    "            ratios = torch.clip(ratios, -self.clip, self.clip)\n",
    "            \n",
    "            prior_loss = (-ratios.mean(-1)*rewards)\n",
    "        else:\n",
    "            prior_lps = torch.clip(prior_lps, -self.clip, self.clip)\n",
    "            prior_loss = (-prior_lps.mean(-1)*rewards)\n",
    "                    \n",
    "        return prior_loss\n",
    "    \n",
    "    def from_batch_state(self, batch_state, subset_name=None):\n",
    "        z = batch_state.model_latent\n",
    "        rewards = batch_state.rewards\n",
    "        \n",
    "        if subset_name is not None:\n",
    "            sources = batch_state.sources\n",
    "            sources = np.array([i.replace('_buffer', '') for i in sources])\n",
    "            source_mask = sources==subset_name\n",
    "            \n",
    "            loss = to_device(torch.zeros(sources.shape))\n",
    "            \n",
    "            z = z[source_mask]\n",
    "            rewards = rewards[source_mask]\n",
    "            \n",
    "            if z.numel()>0:\n",
    "                loss[source_mask] = self.loss(z, rewards)\n",
    "                \n",
    "        else:\n",
    "            loss = self.loss(z, rewards)\n",
    "        \n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class HistoricPriorLoss(Callback):\n",
    "    '''\n",
    "    HistoricPriorLoss - draws samples from batch log \n",
    "    to send to `prior_loss`\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `prior_loss PriorLoss`: prior loss function\n",
    "    \n",
    "    - `model nn.Module`: model used to convert samples to \n",
    "    latent vectors\n",
    "    \n",
    "    - `dataset Base_Dataset`: dataset to convert samples to \n",
    "    tensors\n",
    "    \n",
    "    - `percentile int`: value [1-100] percentile to sample from\n",
    "    \n",
    "    - `n int`: number of samples to draw\n",
    "    \n",
    "    - `above_percent float`: what percentage of samples should \n",
    "    be above `percentile` in score\n",
    "    \n",
    "    - `start_iter int`: what iteration to start using historical \n",
    "    loss\n",
    "    \n",
    "    - `frequency int`: batch frequency of calling historical loss\n",
    "    \n",
    "    - `log_term str`: what term in the batch log to use for \n",
    "    percentile computation\n",
    "    \n",
    "    - `weight float`: loss scaling weight\n",
    "    \n",
    "    - `track bool`: if values from this callback should be tracked\n",
    "    '''\n",
    "    def __init__(self, prior_loss, model, dataset, percentile, \n",
    "                 n, above_percent, start_iter, frequency,\n",
    "                 log_term='rewards', weight=1., track=True):\n",
    "        super().__init__(name='hist_prior', order=20)\n",
    "        \n",
    "        if not is_container(prior_loss):\n",
    "            prior_loss = [prior_loss]\n",
    "        \n",
    "        self.prior_loss = prior_loss\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.percentile = percentile\n",
    "        self.n = n\n",
    "        self.above_percent = above_percent\n",
    "        self.start_iter = start_iter\n",
    "        self.frequency = frequency\n",
    "        self.log_term = log_term\n",
    "        self.weight = weight\n",
    "        self.track = track\n",
    "        \n",
    "    def setup(self):\n",
    "        if self.track:\n",
    "            log = self.environment.log\n",
    "            log.add_metric(self.name)\n",
    "\n",
    "    def compute_loss(self):\n",
    "        loss = self.historic_loss()\n",
    "        loss = loss * self.weight\n",
    "        \n",
    "        if self.track:\n",
    "            self.environment.log.update_metric(self.name, loss.mean().detach().cpu().numpy())\n",
    "        \n",
    "        self.environment.batch_state.loss += loss.mean()\n",
    "        self.environment.batch_state[self.name] = loss.detach().cpu().numpy()\n",
    "        \n",
    "    def select_data(self):\n",
    "        env = self.environment\n",
    "        df = env.log.df\n",
    "\n",
    "        df1 = df[df[self.log_term]>np.percentile(df[self.log_term].values, self.percentile)]\n",
    "        n_samp = min(int(self.n*self.above_percent), df1.shape[0])\n",
    "        samples1 = df1.sample(n=n_samp)\n",
    "\n",
    "        df2 = df[df[self.log_term]<np.percentile(df[self.log_term].values, self.percentile)]\n",
    "        n_samp = min(int(self.n*(1-self.above_percent)), df2.shape[0])\n",
    "        samples2 = df2.sample(n=n_samp)\n",
    "\n",
    "        df = pd.concat([samples1, samples2])\n",
    "        return df\n",
    "        \n",
    "    def historic_loss(self):\n",
    "        env = self.environment\n",
    "        \n",
    "        iterations = self.environment.log.iterations\n",
    "\n",
    "        if (iterations > self.start_iter) and (iterations%self.frequency==0):\n",
    "            \n",
    "            df = self.select_data()\n",
    "            \n",
    "            rewards = to_device(torch.tensor(df.rewards.values).float())\n",
    "            \n",
    "            batch_ds = self.dataset.new(df.samples.values)\n",
    "            batch = batch_ds.collate_function([batch_ds[i] for i in range(len(batch_ds))])\n",
    "            batch = to_device(batch)\n",
    "            x,y = batch\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                z = self.model.x_to_latent(x)\n",
    "            \n",
    "            prior_loss = sum([i.loss(z, rewards) for i in self.prior_loss])\n",
    "\n",
    "        else:\n",
    "            prior_loss = torch.tensor(0.)\n",
    "        \n",
    "        return prior_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mrl)",
   "language": "python",
   "name": "mrl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
