{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "! [ -e /content ] && pip install -Uqq mrl-pypi  # upgrade mrl on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp torch_core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Core\n",
    "\n",
    "> Core pytorch functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from mrl.imports import *\n",
    "from mrl.torch_imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Settings\n",
    "\n",
    "These functions control what device is used for tensors.\n",
    "\n",
    "`to_device` will set a tensor or Pytorch model to the passed device. If no device is passed, the default device is used.\n",
    "\n",
    "If Cuda is available, the device defined by `torch.cuda.current_device()`. The current device can be set using Pytorch:\n",
    "\n",
    "`torch.cuda.set_device(1)`\n",
    "\n",
    "To disable GPU usage, set the `use_cuda` environment variable to `cpu`. `os.environ['use_cuda'] = 'cpu'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "if USE_CUDA:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "if USE_CUDA:\n",
    "    os.environ['use_cuda'] = 'cuda'\n",
    "else:\n",
    "    os.environ['use_cuda'] = 'cpu'\n",
    "    \n",
    "def get_device():\n",
    "    '''\n",
    "    get_device - returns current default device.\n",
    "    \n",
    "    If Cuda is available and `os.environ['use_cuda']=='cuda'`, \n",
    "    `torch.cuda.current_device()` is returned.\n",
    "    \n",
    "    Otherwise, `cpu` is returned\n",
    "    \n",
    "    Returns `torch.device`\n",
    "    '''\n",
    "    if torch.cuda.is_available() and os.environ['use_cuda']=='cuda':\n",
    "        device = torch.cuda.current_device()\n",
    "    else:\n",
    "        device='cpu'\n",
    "            \n",
    "    device = torch.device(device)\n",
    "    return device\n",
    "\n",
    "def to_device(tensor, device=None):\n",
    "    '''\n",
    "    to_device - sets `tensor` to `device` if possible. \n",
    "    If `device=None`, `tensor` is set to the default device \n",
    "    returned by `get_device`\n",
    "    \n",
    "    Inputs\n",
    "    \n",
    "    - `tensor torch.Tensor`: input tensor\n",
    "        \n",
    "    - `device [str, torch.Device]`: device \n",
    "    '''\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "    \n",
    "    if hasattr(tensor, 'to'):\n",
    "        output = tensor.to(device)\n",
    "    else:\n",
    "        output = [to_device(i, device) for i in tensor]\n",
    "    \n",
    "    return output\n",
    "\n",
    "def set_device(device):\n",
    "    'wrapper for torch set_device'\n",
    "    torch.cuda.set_device(device)\n",
    "    \n",
    "def get_model_device(model):\n",
    "    'gets device for model from first parameter'\n",
    "    return next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def freeze(module):\n",
    "    '''\n",
    "    freeze - freezes all parameters in `module` (requires_grad=False)\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `module nn.Module`: Pytorch module\n",
    "    '''\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad_(False)\n",
    "        \n",
    "def unfreeze(module):\n",
    "    '''\n",
    "    unfreeze - unfreezes all parameters in `module` (requires_grad=True)\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `module nn.Module`: Pytorch module\n",
    "    '''\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def x_to_preds(x, multinomial=True):\n",
    "    '''\n",
    "    x_to_preds - helper function for converting `x` \n",
    "    to log probs and taking a hard sample\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `x torch.Tensor`: input tensor\n",
    "    \n",
    "    - `multinomial bool`: if True, use multinomial sampling. \n",
    "    If False, use argmax sampling\n",
    "        \n",
    "    Returns:\n",
    "    \n",
    "    - `idxs torch.LongTensor`: index values of hard sample\n",
    "    \n",
    "    - `lps torch.FloatTensor`: log probabilities for each hard value\n",
    "    '''\n",
    "    log_probs = F.log_softmax(x, -1).squeeze(1)\n",
    "    probs = log_probs.detach().exp()\n",
    "    \n",
    "    if multinomial:\n",
    "        idxs = torch.multinomial(probs, 1)\n",
    "    else:\n",
    "        idxs = x.argmax(-1)\n",
    "\n",
    "    lps = torch.gather(log_probs, 1, idxs)\n",
    "    return idxs, lps\n",
    "\n",
    "def gather_lps(lps, y):\n",
    "    return lps.gather(2, y.unsqueeze(-1)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "def subset_tensor(x, mask):\n",
    "    '''\n",
    "    indexes `x` with `mask`. If `x` is a list or tuple, \n",
    "    function will index all items in `x`\n",
    "    '''\n",
    "    \n",
    "    if isinstance(x, (list, tuple)):\n",
    "        x = [i[mask] for i in x]\n",
    "    else:\n",
    "        x = x[mask]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def merge_weights(sd1, sd2, alpha=0.5):\n",
    "    '''\n",
    "    merges state dicts following `new_weight = alpha*weight_model1 + (1-alpha)*weight_model2`\n",
    "    \n",
    "    New weights are returned as a new state dict\n",
    "    '''\n",
    "    new_dict = {}\n",
    "    for key in sd1.keys():\n",
    "        if key in sd2.keys():\n",
    "            new_dict[key] = sd1[key]*alpha + sd2[key]*(1-alpha)\n",
    "        else:\n",
    "            new_dict[key] = sd1[key]\n",
    "    return new_dict\n",
    "\n",
    "def merge_models(model1, model2, alpha=0.5):\n",
    "    '''\n",
    "    merges weights following `new_weight = alpha*weight_model1 + (1-alpha)*weight_model2`\n",
    "    \n",
    "    New weights are loaded into `modell` inplace\n",
    "    '''\n",
    "    new_weights = merge_weights(model1.state_dict(), model2.state_dict(), alpha)\n",
    "    model1.load_state_dict(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def smooth_batches(batches, beta=0.98):\n",
    "    val = torch.tensor(0.)\n",
    "    count = 0\n",
    "    \n",
    "    for batch in batches:\n",
    "        val = torch.lerp(batch.mean(), val, beta)\n",
    "        count += 1\n",
    "        \n",
    "    return val/(1-beta**count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def discount_rewards(rewards, gamma):\n",
    "    '''\n",
    "    discount_rewards - discounts `rewards` by `gamma`\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `rewards torch.Tensor[bs,sl]`: tensor of undiscounted rewards\n",
    "        \n",
    "    - `gamma float`: discount factor\n",
    "        \n",
    "    Returns:\n",
    "    \n",
    "    - `discounted torch.Tensor[bs, sl]`: tensor of discounted rewards\n",
    "        \n",
    "    Rewards are discounted following \n",
    "    \n",
    "    `discounted[i] = rewards[i] + gamma*discounted[i+1]`\n",
    "    '''\n",
    "    discounted = torch.zeros((rewards.shape[0], rewards.shape[1]+1)).to(rewards.device)\n",
    "    \n",
    "    for i in reversed(range(discounted.shape[1]-1)):\n",
    "        discounted[:,i] = rewards[:,i] + gamma*discounted[:,i+1]\n",
    "        \n",
    "    return discounted[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.6507, 3.7636, 3.8800, 4.0000],\n",
       "        [2.7380, 2.8227, 2.9100, 3.0000],\n",
       "        [1.8253, 1.8818, 1.9400, 2.0000]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards = torch.tensor([[0., 0., 0., 4.],\n",
    "                       [0., 0., 0., 3.],\n",
    "                       [0., 0., 0., 2.],])\n",
    "gamma = 0.97\n",
    "discounted = discount_rewards(rewards, gamma)\n",
    "discounted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def whiten(values, shift_mean=True, mask=None):\n",
    "    '''\n",
    "    whiten - whitens `values` \n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `values torch.FloatTensor`: values to be whitened\n",
    "        \n",
    "    - `shift_mean bool`: if True, outputs will have zero mean. \n",
    "        \n",
    "    - `mask [torch.BoolTensor, torch.LongTensor, None]`: if a mask \n",
    "    is given, masked values will not contribute to calculating the \n",
    "    mean and variance for whitening. Masking is done following \n",
    "    `masked_values = values*mask`. For bool tensors, values where \n",
    "    `mask=True` are kept. For binary float/int tensors, values \n",
    "    where `mask=1` are kept.\n",
    "        \n",
    "    Returns:\n",
    "    \n",
    "    - `whitened torch.FloatTensor`: whitened values\n",
    "    '''\n",
    "    if mask is None:\n",
    "        mean = values.mean()\n",
    "        var = values.var()\n",
    "    else:\n",
    "        mean = (values*mask).sum()/mask.sum()\n",
    "        var = ((values-mean)*mask).pow(2).sum()/(mask.sum()-1)\n",
    "        \n",
    "    whitened = (values - mean) * torch.rsqrt(var + 1e-8)\n",
    "    \n",
    "    if not shift_mean:\n",
    "        whitened += mean\n",
    "        \n",
    "    if mask is not None:\n",
    "        whitened = whitened*mask\n",
    "        \n",
    "    return whitened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def scatter_rewards(rewards, mask):\n",
    "    '''\n",
    "    scatter_rewards - scatter vector of rewards to matrix \n",
    "    based on `mask`\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `rewards torch.FloatTensor[bs]`: vector of rewards\n",
    "        \n",
    "    - `mask torch.Tensor[bs, sl]`: mask tensor\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    - `template torch.FloatTensor`: scattered values\n",
    "        \n",
    "    In molecular RL, we typically have a single reward per molecule \n",
    "    evaluating the entire structure. Before our update, we need to \n",
    "    discount the final reward back to all timesteps. However, we want \n",
    "    to ignore padding when we do this.\n",
    "    \n",
    "    scatter_rewards takes a vector of rewards and a mask where \n",
    "    non-padding tokens are True and padding tokens are False. \n",
    "    Rewards are placed in the last `True` index\n",
    "    \n",
    "    ```\n",
    "    rewards = torch.tensor([4., 5., 6.]).float()\n",
    "    mask = torch.tensor([[True, True, True, False],\n",
    "                         [True, True, False, False],\n",
    "                         [True, True, True, True]])\n",
    "    scattered = scatter_rewards(rewards, mask)\n",
    "    >> torch.tensor([[0., 0., 4., 0.],\n",
    "                     [0., 5., 0., 0.],\n",
    "                     [0., 0., 0., 6.]])\n",
    "    ```\n",
    "    '''\n",
    "    template = torch.zeros(mask.shape).to(mask.device)\n",
    "    lengths = mask.sum(-1)\n",
    "    template[torch.arange(template.shape[0]), lengths-1]=rewards\n",
    "    return template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 4., 0.],\n",
       "        [0., 5., 0., 0.],\n",
       "        [0., 0., 0., 6.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards = torch.tensor([4., 5., 6.]).float()\n",
    "mask = torch.tensor([[True, True, True, False],\n",
    "                     [True, True, False, False],\n",
    "                     [True, True, True, True]])\n",
    "scattered = scatter_rewards(rewards, mask)\n",
    "scattered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def compute_advantages(rewards, values, gamma, lam):\n",
    "    '''\n",
    "    Calculate advantages according to Generalized Advantage Estimation (GAE)\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `rewards torch.Tensor`: reward tensor\n",
    "        \n",
    "    - `values torch.Tensor`: value function predictions\n",
    "        \n",
    "    - 'gamma float`: GAE gamma factor\n",
    "        \n",
    "    - `lam float`: GAE lambda factor\n",
    "        \n",
    "    Returns:\n",
    "    \n",
    "    - `advantages torch.Tensor`: computed advantages\n",
    "        \n",
    "    Advantages are computed according to GAE\n",
    "    \n",
    "    `delta = rewards[i] + gamma*values[i+1] - values[i]`\n",
    "        \n",
    "    `advantages[i] = delta + gamma*lam*glv`\n",
    "    '''\n",
    "\n",
    "    advantages = torch.zeros(rewards.shape).to(rewards.device)\n",
    "\n",
    "    for i in reversed(range(rewards.shape[1])):\n",
    "        if i==rewards.shape[1]-1:\n",
    "            v_t1 = 0.\n",
    "            glv = 0.\n",
    "        else:\n",
    "            v_t1 = values[:,i+1]\n",
    "\n",
    "        delta = rewards[:, i] + gamma*v_t1 - values[:, i]\n",
    "        glv = delta + gamma * lam * glv\n",
    "        advantages[:,i] = glv\n",
    "        \n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "Pytorch losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class CrossEntropy():\n",
    "    '''\n",
    "    CrossEntropy - cross entropy loss for sequence predictions. \n",
    "    Flattens predictions and targets before computing loss\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def __call__(self, output, target):\n",
    "        '''\n",
    "        Inputs:\n",
    "        \n",
    "        - `output torch.FloatTensor[bs, sl, n]`: predictions\n",
    "            \n",
    "        - `target torch.LongTensor[bs, sl]`: target integer values\n",
    "        '''\n",
    "        if USE_CUDA and not os.environ['use_cuda']=='cpu':\n",
    "            output = output.view(-1, output.shape[-1])\n",
    "            target = target.view(-1).long()\n",
    "        else:\n",
    "            output = output.reshape(-1, output.shape[-1])\n",
    "            target = target.reshape(-1).long()\n",
    "        return self.loss(output, target)\n",
    "    \n",
    "class BinaryCrossEntropy():\n",
    "    def __init__(self):\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def __call__(self, output, target):\n",
    "        if USE_CUDA and not os.environ['use_cuda']=='cpu':\n",
    "            output = output.view(-1)\n",
    "            target = target.view(-1)\n",
    "        else:\n",
    "            output = output.reshape(-1)\n",
    "            target = target.reshape(-1)\n",
    "        return self.loss(output, target)\n",
    "    \n",
    "    \n",
    "class HuberLoss():\n",
    "    def __init__(self, beta=1.):\n",
    "        self.loss = nn.SmoothL1Loss(beta=beta)\n",
    "        \n",
    "    def __call__(self, output, target):\n",
    "        output = output.squeeze(-1)\n",
    "        return self.loss(output, target)\n",
    "    \n",
    "    \n",
    "class MSELoss():\n",
    "    def __init__(self):\n",
    "        self.loss = F.mse_loss\n",
    "        \n",
    "    def __call__(self, output, target):\n",
    "        output = output.squeeze(-1)\n",
    "        return self.loss(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "def pca(x, k=2):\n",
    "    '''\n",
    "    pca of `x` to `k` dimensions\n",
    "    '''\n",
    "    x = x-torch.mean(x,0)\n",
    "    U,S,V = torch.svd(x.t())\n",
    "    return torch.mm(x,U[:,:k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
