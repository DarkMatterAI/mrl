{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp torch_core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Core\n",
    "\n",
    "> Core pytorch functins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from mrl.imports import *\n",
    "from mrl.torch_imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "if USE_CUDA:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "if USE_CUDA:\n",
    "    os.environ['use_cuda'] = 'cuda'\n",
    "else:\n",
    "    os.environ['use_cuda'] = 'cpu'\n",
    "    \n",
    "def get_device():\n",
    "    if torch.cuda.is_available() and os.environ['use_cuda']=='cuda':\n",
    "        device = torch.cuda.current_device()\n",
    "    else:\n",
    "        device='cpu'\n",
    "            \n",
    "    device = torch.device(device)\n",
    "    return device\n",
    "\n",
    "def to_device(tensor, device=None):\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "    \n",
    "    if hasattr(tensor, 'to'):\n",
    "        output = tensor.to(device)\n",
    "    else:\n",
    "        output = [to_device(i, device) for i in tensor]\n",
    "    \n",
    "    return output\n",
    "\n",
    "def set_device(device):\n",
    "    torch.cuda.set_device(device)\n",
    "    \n",
    "def get_model_device(model):\n",
    "    return next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def freeze(module):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad_(False)\n",
    "        \n",
    "def unfreeze(module):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def x_to_preds(x, multinomial=True):\n",
    "    log_probs = F.log_softmax(x, -1).squeeze(1)\n",
    "    probs = log_probs.detach().exp()\n",
    "    \n",
    "    if multinomial:\n",
    "        idxs = torch.multinomial(probs, 1)\n",
    "    else:\n",
    "        idxs = x.argmax(-1)\n",
    "\n",
    "    lps = torch.gather(log_probs, 1, idxs)\n",
    "    return idxs, lps\n",
    "\n",
    "def gather_lps(lps, y):\n",
    "    return lps.gather(2, y.unsqueeze(-1)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "def subset_tensor(x, mask):\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        x = [i[mask] for i in x]\n",
    "    else:\n",
    "        x = x[mask]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def merge_weights(sd1, sd2, alpha=0.5):\n",
    "    new_dict = {}\n",
    "    for key in sd1.keys():\n",
    "        if key in sd2.keys():\n",
    "            new_dict[key] = sd1[key]*alpha + sd2[key]*(1-alpha)\n",
    "        else:\n",
    "            new_dict[key] = sd1[key]\n",
    "    return new_dict\n",
    "\n",
    "def merge_models(model1, model2, alpha=0.5):\n",
    "    new_weights = merge_weights(model1.state_dict(), model2.state_dict(), alpha)\n",
    "    model1.load_state_dict(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def smooth_batches(batches, beta=0.98):\n",
    "    \n",
    "    val = torch.tensor(0.)\n",
    "    count = 0\n",
    "    \n",
    "    for batch in batches:\n",
    "        val = torch.lerp(batch.mean(), val, beta)\n",
    "        count += 1\n",
    "        \n",
    "    return val/(1-beta**count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def discount_rewards(rewards, gamma):\n",
    "    discounted = torch.zeros((rewards.shape[0], rewards.shape[1]+1)).to(rewards.device)\n",
    "    \n",
    "    for i in reversed(range(discounted.shape[1]-1)):\n",
    "        discounted[:,i] = rewards[:,i] + gamma*discounted[:,i+1]\n",
    "        \n",
    "    return discounted[:,:-1]\n",
    "\n",
    "def whiten(values, shift_mean=True, mask=None):\n",
    "    if mask is None:\n",
    "        mean = values.mean()\n",
    "        var = values.var()\n",
    "    else:\n",
    "        mean = (values*mask).sum()/mask.sum()\n",
    "        var = ((values-mean)*mask).pow(2).sum()/(mask.sum()-1)\n",
    "        \n",
    "    whitened = (values - mean) * torch.rsqrt(var + 1e-8)\n",
    "    \n",
    "    if not shift_mean:\n",
    "        whitened += mean\n",
    "        \n",
    "    if mask is not None:\n",
    "        whitened = whitened*mask\n",
    "        \n",
    "    return whitened\n",
    "\n",
    "def scatter_rewards(rewards, mask):\n",
    "    template = torch.zeros(mask.shape).to(mask.device)\n",
    "    lengths = mask.sum(-1)\n",
    "    template[torch.arange(template.shape[0]), lengths-1]=rewards\n",
    "    return template\n",
    "\n",
    "def compute_advantages(rewards, values, gamma, lam):\n",
    "\n",
    "    advantages = torch.zeros(rewards.shape).to(rewards.device)\n",
    "\n",
    "    for i in reversed(range(rewards.shape[1])):\n",
    "        if i==rewards.shape[1]-1:\n",
    "            v_t1 = 0.\n",
    "            glv = 0.\n",
    "        else:\n",
    "            v_t1 = values[:,i+1]\n",
    "\n",
    "        delta = rewards[:, i] + gamma*v_t1 - values[:, i]\n",
    "        glv = delta + gamma * lam * glv\n",
    "        advantages[:,i] = glv\n",
    "        \n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = torch.zeros((2, 9)).float()\n",
    "rewards[0, 8] = 2\n",
    "rewards[1,7] = 1\n",
    "dr = discount_rewards(rewards, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class CrossEntropy():\n",
    "    def __init__(self):\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def __call__(self, output, target):\n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        target = target.view(-1).long()\n",
    "        return self.loss(output, target)\n",
    "    \n",
    "class BinaryCrossEntropy():\n",
    "    def __init__(self):\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def __call__(self, output, target):\n",
    "        output = output.view(-1)\n",
    "        target = target.view(-1)\n",
    "        return self.loss(output, target)\n",
    "    \n",
    "    \n",
    "class HuberLoss():\n",
    "    def __init__(self, beta=1.):\n",
    "        self.loss = nn.SmoothL1Loss(beta=beta)\n",
    "        \n",
    "    def __call__(self, output, target):\n",
    "        output = output.squeeze(-1)\n",
    "        return self.loss(output, target)\n",
    "    \n",
    "    \n",
    "class MSELoss():\n",
    "    def __init__(self):\n",
    "        self.loss = F.mse_loss\n",
    "        \n",
    "    def __call__(self, output, target):\n",
    "        output = output.squeeze(-1)\n",
    "        return self.loss(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "def pca(x, k=2):\n",
    "    x = x-torch.mean(x,0)\n",
    "    U,S,V = torch.svd(x.t())\n",
    "    return torch.mm(x,U[:,:k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mrl)",
   "language": "python",
   "name": "mrl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
