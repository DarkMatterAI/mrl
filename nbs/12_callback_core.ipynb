{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp callbacks.core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callback Core\n",
    "\n",
    "> Base callback class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from mrl.imports import *\n",
    "from mrl.core import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "\n",
    "The training cycle in MRL is built around the Callback system. Rather than trying to explicitly define every training cycle variant, Callbacks define a series of events (see `Events`) that occur during training and allow users to esaily hook into those events. The result is an extremely flexible framework that can adapt to most generative design challenges.\n",
    "\n",
    "Callbacks use the `__call__` function to organize events. The call function will be passed an event name, like `compute_reward`. If the Callback function has an attribute that matches the event name, the attribute is called.\n",
    "\n",
    "Callbacks have access to the training environment (see `Environment`) and can access the training environment, the model/agent, the training buffer, training log, other callbacks and all other aspects of the training state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Callback():\n",
    "    def __init__(self, name='base_callback', order=10):\n",
    "        self.order=order\n",
    "        self.name = name\n",
    "        self.event_timelog = defaultdict(list)\n",
    "    \n",
    "    def __call__(self, event_name):\n",
    "        \n",
    "        start = time.time()\n",
    "        event = getattr(self, event_name, None)\n",
    "        if event is not None:\n",
    "            output = event()\n",
    "        else:\n",
    "            output = None\n",
    "            \n",
    "        end = time.time() - start\n",
    "        self.event_timelog[event_name].append(end)\n",
    "        return output\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.name\n",
    "    \n",
    "    def _filter_batch(self, valids):\n",
    "        valids = np.array(valids)\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        \n",
    "        samples = batch_state.samples\n",
    "        sources = np.array(batch_state.sources)\n",
    "        \n",
    "        if valids.mean()<1.:\n",
    "            filtered_samples = [samples[i] for i in range(len(samples)) if valids[i]]\n",
    "            filtered_sources = [sources[i] for i in range(len(sources)) if valids[i]]\n",
    "            filtered_latent_data = {}\n",
    "\n",
    "            for source,latents in batch_state.latent_data.items():\n",
    "                valid_subset = valids[sources==source]\n",
    "                latent_filtered = latents[valid_subset]\n",
    "                filtered_latent_data[source] = latent_filtered\n",
    "\n",
    "            batch_state.samples = filtered_samples\n",
    "            batch_state.sources = filtered_sources\n",
    "            batch_state.latent_data = filtered_latent_data\n",
    "    \n",
    "    def plot_dict(self, data_dict, cols=4, smooth=True):\n",
    "        num_metrics = len(data_dict.keys())\n",
    "        \n",
    "        rows = int(np.ceil(num_metrics/cols))\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))\n",
    "                \n",
    "        metrics = list(data_dict.keys())\n",
    "        \n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            if i <len(metrics):\n",
    "                ax.plot(np.stack(data_dict[metrics[i]]),)\n",
    "                ax.set_title(metrics[i])\n",
    "            else:\n",
    "                ax.axis('off')\n",
    "    \n",
    "    def plot_time(self, cols=4, smooth=True):\n",
    "        self.plot_dict(self.event_timelog, cols=cols, smooth=smooth)\n",
    "        \n",
    "    def save(self, filename):\n",
    "        torch.save(self, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Event():\n",
    "    '''\n",
    "    Event\n",
    "    \n",
    "    Base class for events\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.event_name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Setup(Event):\n",
    "    '''\n",
    "    Setup\n",
    "    \n",
    "    Setup is called after an `Environment` is created. The setup \n",
    "    step is used to do things like set attributes or add logging terms\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.event_name = 'setup'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class BeforeTrain(Event):\n",
    "    '''\n",
    "    BeforeTrain\n",
    "    \n",
    "    This event is called by `Environment.fit` before the first batch is run\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.event_name = 'before_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class BuildBuffer(Event):\n",
    "    '''\n",
    "    BuildBuffer\n",
    "    \n",
    "    The build buffer event is used to add samples to the Buffer\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.event_name = 'build_buffer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class FilterBuffer(Event):\n",
    "    '''\n",
    "    FilterBuffer\n",
    "    \n",
    "    The filter buffer event is used to screen items added to the \n",
    "    buffer during `build_buffer` and remove ones that do not \n",
    "    match the filter criteria\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.event_name = 'filter_buffer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class AfterBuildBuffer(Event):\n",
    "    '''\n",
    "    AfterBuildBuffer\n",
    "    \n",
    "    This event is called after the buffer has been filtered and \n",
    "    before the next batch starts. This event can be used to \n",
    "    evaluate metrics and statistics related to the buffer creation\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.event_name = 'after_build_buffer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class BeforeBatch(Event):\n",
    "    '''\n",
    "    BeforeBatch\n",
    "    \n",
    "    This event is called before the next batch is sampled\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.event_name = 'before_batch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class SampleBatch(Event):\n",
    "    '''\n",
    "    SampleBatch\n",
    "    \n",
    "    This event produces a series of samples that are added \n",
    "    to the next batch\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.event_name = 'sample_batch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class BeforeFilterBatch(Event):\n",
    "    '''\n",
    "    BeforeFilterBatch\n",
    "    \n",
    "    This event is called before the current batch is filtered\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.event_name = 'before_filter_batch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class FilterBatch(Event):\n",
    "    '''\n",
    "    FilterBatch\n",
    "    \n",
    "    This event is used to screen items in the current batch \n",
    "    and remove items that do not match the filter criteria\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.event_name = 'filter_batch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class AfterSample(Event):\n",
    "    '''\n",
    "    AfterSample\n",
    "    \n",
    "    This event is called after a batch is sampled and filtered. \n",
    "    This event can be used to log stats about the last batch\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.event_name = 'after_sample'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class BeforeComputeReward(Event):\n",
    "    '''\n",
    "    BeforeComputeReward\n",
    "    \n",
    "    This event is called prior to computing rewards \n",
    "    on the current batch. This event can be used to generate \n",
    "    any inputs required for computing rewards\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.event_name = 'before_compute_reward'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class ComputeReward(Event):\n",
    "    '''\n",
    "    ComputeReward\n",
    "    \n",
    "    This event is used to compute rewards for \n",
    "    the current batch\n",
    "    \n",
    "    All rewards should be added to `self.environmemnt.batch_state.rewards`\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.event_name = 'compute_reward'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class AfterComputeReward(Event):\n",
    "    '''\n",
    "    AfterComputeReward\n",
    "    \n",
    "    This event is called after all rewards \n",
    "    have been computed. This event can be used \n",
    "    to log stats and metrics related to the \n",
    "    rewards for the current batch\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.event_name = 'after_compute_reward'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class RewardModification(Event):\n",
    "    '''\n",
    "    RewardModification\n",
    "    \n",
    "    This event is used to modify rewards before they \n",
    "    are used to compute the model's loss. Reward modifications \n",
    "    encompass changes to rewards in the context of the current \n",
    "    training cycle. These are things like \"give a score bonus \n",
    "    to new samples that havent't been seen before\" or \"penalize \n",
    "    the score of samples that have occurred in the last 5 batches\".\n",
    "    \n",
    "    These types of modifications are kept separate from the core \n",
    "    reward for logging purposes. Samples are logged with their \n",
    "    respective rewards. These logged scores are referenced later \n",
    "    when samples are drawn from the log. This means we need the \n",
    "    logged score to be independent from \"batch context\" type scores\n",
    "    \n",
    "    All reward modifications should be \n",
    "    applied to `self.environmemnt.batch_state.rewards`\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.event_name = 'reward_modification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class GetModelOutputs(Event):\n",
    "    '''\n",
    "    GetModelOutputs\n",
    "    \n",
    "    This event is used to generate any model-derived outputs \n",
    "    relevant to loss computation\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.event_name = 'get_model_outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class AfterGetModelOutputs(Event):\n",
    "    '''\n",
    "    AfterGetModelOutputs\n",
    "    \n",
    "    This event is called after `get_model_outputs`. \n",
    "    This event can be used for any processing \n",
    "    required prior to loss computation\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.event_name = 'after_get_model_outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class ComputeLoss(Event):\n",
    "    '''\n",
    "    ComputeLoss\n",
    "    \n",
    "    This event is used to compute loss values\n",
    "    \n",
    "    All loss values should be added to\n",
    "    `self.environment.batch_state.loss`\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.event_name = 'compute_loss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class ZeroGrad(Event):\n",
    "    '''\n",
    "    ZeroGrad\n",
    "    \n",
    "    This event is used to zero gradients \n",
    "    in any optimizers relevant to the fit cycle\n",
    "    \n",
    "    `loss.backward()` is called after zero grad\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.event_name = 'zero_grad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class BeforeStep(Event):\n",
    "    '''\n",
    "    BeforeStep\n",
    "    \n",
    "    This event is used for any processed \n",
    "    needed after `loss.backward()` but \n",
    "    before `opt.step()`, ie gradient clipping\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.event_name = 'before_step'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Step(Event):\n",
    "    '''\n",
    "    Step\n",
    "    \n",
    "    This event is used to step all optimizers\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.event_name = 'step'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class AfterBatch(Event):\n",
    "    '''\n",
    "    AfterBatch\n",
    "    \n",
    "    This event is called after `step`. This \n",
    "    event can be used to compute batch stats \n",
    "    and clean up values before the next batch\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.event_name = 'after_batch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class AfterTrain(Event):\n",
    "    '''\n",
    "    AfterTrain\n",
    "    \n",
    "    This event is called after all \n",
    "    batch steps have been completed \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.event_name = 'after_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export        \n",
    "        \n",
    "class Events():\n",
    "    def __init__(self):\n",
    "        self.setup = Setup()\n",
    "        self.before_train = BeforeTrain()\n",
    "        self.build_buffer = BuildBuffer()\n",
    "        self.filter_buffer = FilterBuffer()\n",
    "        self.after_build_buffer = AfterBuildBuffer()\n",
    "        self.before_batch = BeforeBatch()\n",
    "        self.sample_batch = SampleBatch()\n",
    "        self.before_filter_batch = BeforeFilterBatch()\n",
    "        self.filter_batch = FilterBatch()\n",
    "        self.after_sample = AfterSample()\n",
    "        self.before_compute_reward = BeforeComputeReward()\n",
    "        self.compute_reward = ComputeReward()\n",
    "        self.after_compute_reward = AfterComputeReward()\n",
    "        self.reward_modification = RewardModification()\n",
    "        self.get_model_outputs = GetModelOutputs()\n",
    "        self.after_get_model_outputs = AfterGetModelOutputs()\n",
    "        self.compute_loss = ComputeLoss()\n",
    "        self.zero_grad = ZeroGrad()\n",
    "        self.before_step = BeforeStep()\n",
    "        self.step = Step()\n",
    "        self.after_batch = AfterBatch()\n",
    "        self.after_train = AfterTrain()\n",
    "        \n",
    "        self.event_names = [\n",
    "            'setup',\n",
    "            'before_train',\n",
    "            'build_buffer',\n",
    "            'filter_buffer',\n",
    "            'after_build_buffer',\n",
    "            'before_batch',\n",
    "            'sample_batch',\n",
    "            'before_filter_batch',\n",
    "            'filter_batch',\n",
    "            'after_sample',\n",
    "            'before_compute_reward',\n",
    "            'compute_reward',\n",
    "            'after_compute_reward',\n",
    "            'reward_modification',\n",
    "            'get_model_outputs',\n",
    "            'after_get_model_outputs',\n",
    "            'compute_loss',\n",
    "            'zero_grad',\n",
    "            'before_step',\n",
    "            'step',\n",
    "            'after_batch',\n",
    "            'after_train'\n",
    "        ]\n",
    "        \n",
    "    def __call__(self, event_name):\n",
    "        \n",
    "        event = getattr(self, event_name, None)\n",
    "        if event is not None:\n",
    "            print(event.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class SettrDict(dict):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def __setitem__(self, key, item):\n",
    "        super().__setitem__(key, item)\n",
    "        super().__setattr__(key, item)\n",
    "    \n",
    "    def __setattr__(self, key, item):\n",
    "        super().__setitem__(key, item)\n",
    "        super().__setattr__(key, item)\n",
    "        \n",
    "    def update_from_dict(self, update_dict):\n",
    "        for k,v in update_dict.items():\n",
    "            self[k] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch State\n",
    "\n",
    "The `BatchState` class is used by an `Environment` to track values generated or computed during a batch. Every batch, the old `BatchState` is deleted and a new `BatchState` is created.\n",
    "\n",
    "Attributes in `BatchState` can be set or accessed with a key like a dictionary or as an attribute. `BatchState` can hold any arbitrary value during a batch. However, it was designed for the use case where every attribute is either a single value or a list/container with length equal to the current batch size.\n",
    "\n",
    "### Rewards\n",
    "\n",
    "`BatchState` holds the `rewards` value for a batch. All reward functions should ultimately add their reward value to `BatchState.rewards`. See `Reward` for more information.\n",
    "\n",
    "### Loss\n",
    "\n",
    "`BatchState` holds the `loss` value for a batch. This is the value that will be backpropagated during the optimizer update. All loss functions should ultimately add their value to `BatchState.loss`. See `Loss` for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "        \n",
    "class BatchState(SettrDict):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.samples = []\n",
    "        self.sources = []\n",
    "        self.rewards = to_device(torch.tensor(0.))\n",
    "        self.loss = to_device(torch.tensor(0., requires_grad=True))\n",
    "        self.latent_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mrl)",
   "language": "python",
   "name": "mrl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
