{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "! [ -e /content ] && pip install -Uqq mrl-pypi  # upgrade mrl on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp train.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging\n",
    "\n",
    "> Callbacks for logging data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from mrl.imports import *\n",
    "from mrl.core import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *\n",
    "from mrl.train.callback import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log\n",
    "\n",
    "The `Log` Callback serves the purpose of logging data generated during a training run. The log holds the following objects of interest:\n",
    "\n",
    "- `batch_log` - a dictionary of batch-wise logged data. Each key is a string denoting the name of a logged attribute. The values are lists of lists, where each sub-list is the value of the given attribute for each item in a batch. For example `log.batch_log['rewards'][5]` would yield an array of rewards for each item in batch 5\n",
    "\n",
    "- `timelog` - a dictionary of lists. The keys denote different training steps (`build_buffer`, `sample_batch`, etc) with how long the step took for each batch. To view the times for all training stages, use the `Log.plot_timelog` function.\n",
    "\n",
    "- `metrics` - a dictionary of lists. Each key is the name of a tracked metric. Each value is a list containing the value of that metric for each batch. Metrics are single scalar values, one value per batch. Metrics can be plotted with the `Log.plot_metrics` function\n",
    "\n",
    "- `unique_samples` - a set containing every unique sample seen during training. This can be used to quickly reference if a sample has been seen before\n",
    "\n",
    "- `df` - a dataframe containing every unique sample and every `batch_log` value associated with that sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Log(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__(name='log', order=100)\n",
    "        \n",
    "        self.pbar = None\n",
    "        self.iterations = 0\n",
    "        self.metrics = {}\n",
    "        \n",
    "        self.batch_log = {}\n",
    "        self.timelog = defaultdict(list)\n",
    "        \n",
    "        self.report = 1\n",
    "        self.unique_samples = {}\n",
    "                \n",
    "        self.add_metric('rewards')\n",
    "        self.add_metric('rewards_final')\n",
    "        self.add_metric('new')\n",
    "        self.add_metric('diversity')\n",
    "        self.add_metric('bs')\n",
    "        \n",
    "        self.add_log('samples')\n",
    "        self.add_log('sources')\n",
    "        self.add_log('rewards')\n",
    "        self.add_log('rewards_final')\n",
    "        \n",
    "    def setup(self):\n",
    "        self.df = pd.DataFrame(self.batch_log)\n",
    "        \n",
    "    def before_train(self):\n",
    "        cols = ['iterations'] + list(self.metrics.keys())\n",
    "        if self.pbar is None:\n",
    "            print('\\t'.join(cols))\n",
    "        else:\n",
    "            self.pbar.write(cols, table=True)\n",
    "            \n",
    "    def add_metric(self, name):\n",
    "        if not name in self.metrics.keys():\n",
    "            self.metrics[name] = []\n",
    "        \n",
    "    def add_log(self, name):\n",
    "        if not name in self.batch_log.keys():\n",
    "            self.batch_log[name] = []\n",
    "            \n",
    "    def update_metric(self, name, value):\n",
    "        self.metrics[name].append(value)\n",
    "\n",
    "    def after_sample(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        samples = batch_state.samples\n",
    "        batch_state.rewards = to_device(torch.zeros(len(samples)))\n",
    "        \n",
    "        new = np.array([not i in self.unique_samples for i in samples])\n",
    "        \n",
    "        self.update_metric('new', new.mean())\n",
    "            \n",
    "        if len(samples)>0:\n",
    "            diversity = len(set(samples))/len(samples)\n",
    "        else:\n",
    "            diversity = 0.\n",
    "        self.environment.log.update_metric('diversity', diversity)\n",
    "        \n",
    "        self.environment.log.update_metric('bs', len(batch_state.samples))\n",
    "            \n",
    "    def after_compute_reward(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        samples = batch_state.samples\n",
    "        rewards = batch_state.rewards\n",
    "        batch_state.rewards_final = rewards.clone().detach()\n",
    "        \n",
    "        rewards = rewards.detach().cpu().numpy()\n",
    "        \n",
    "        self.update_metric('rewards', rewards.mean())\n",
    "        \n",
    "        for i in range(len(samples)):\n",
    "            if not samples[i] in self.unique_samples:\n",
    "                self.unique_samples[samples[i]] = rewards[i]\n",
    "                \n",
    "    def after_reward_modification(self):\n",
    "        env = self.environment\n",
    "        rewards = env.batch_state.rewards_final.detach().cpu().numpy()\n",
    "        self.update_metric('rewards_final', rewards.mean())\n",
    "        \n",
    "                \n",
    "    def update_log(self):\n",
    "        env = self.environment\n",
    "        batch_state = env.batch_state\n",
    "        samples = batch_state.samples\n",
    "        update_dict = {}\n",
    "\n",
    "        for key in self.batch_log.keys():\n",
    "            items = batch_state[key]\n",
    "            if isinstance(items, torch.Tensor):\n",
    "                items = items.detach().cpu().numpy()\n",
    "            self.batch_log[key].append(items)\n",
    "            update_dict[key] = items\n",
    "            \n",
    "        new_df = pd.DataFrame(update_dict)\n",
    "        repeats = new_df.samples.isin(self.df.samples)\n",
    "        new_df = new_df[~repeats]\n",
    "            \n",
    "        self.df = self.df.append(new_df, ignore_index=True)\n",
    "        \n",
    "        if self.iterations%10==0 and self.iterations>0:\n",
    "            self.df.drop_duplicates(subset='samples', inplace=True)\n",
    "            \n",
    "    def report_batch(self):\n",
    "        outputs = [f'{self.iterations}']\n",
    "        if self.iterations%self.report==0:\n",
    "            \n",
    "            for k,v in self.metrics.items():\n",
    "                val = v[-1]\n",
    "\n",
    "                if type(val)==int:\n",
    "                    val = f'{val}'\n",
    "                elif type(val)==str:\n",
    "                    val = val\n",
    "                else:\n",
    "                    val = f'{val:.3f}'\n",
    "\n",
    "                outputs.append(val)\n",
    "\n",
    "            if self.pbar is None:\n",
    "                print('\\t'.join(outputs))\n",
    "            else:\n",
    "                self.pbar.write(outputs, table=True)\n",
    "            \n",
    "        self.iterations += 1\n",
    "        \n",
    "    def after_batch(self):\n",
    "        self.update_log()\n",
    "        self.report_batch()\n",
    "        \n",
    "    def get_df(self):\n",
    "        return log_to_df(self.batch_log)\n",
    "    \n",
    "    def plot_metrics(self, cols=4, smooth=True):\n",
    "        self.plot_dict(self.metrics, cols=cols, smooth=smooth)\n",
    "            \n",
    "    def plot_timelog(self, cols=4, smooth=True):\n",
    "        self.plot_dict(self.timelog, cols=cols, smooth=smooth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def log_to_df(log, keys=None):\n",
    "    batch = 0\n",
    "    output_dict = defaultdict(list)\n",
    "    \n",
    "    if keys is None:\n",
    "        keys = list(log.keys())\n",
    "    \n",
    "    items = log[keys[0]]\n",
    "    for item in items:\n",
    "        output_dict['batch'] += [batch]*len(item)\n",
    "        batch += 1\n",
    "        \n",
    "    for key in keys:\n",
    "        output_dict[key] = flatten_list_of_lists(log[key])\n",
    "\n",
    "    return pd.DataFrame(output_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Metrics Logging\n",
    "\n",
    "Adding new items to metric tracking and batch logging is easy.\n",
    "\n",
    "Use the `add_log` to add a new term to the batch log. At some point during the batch, add the values to be logged to the current `BatchState` with an attribute name that matches the name added to the log. The batch log will automatically add the values to the batch log.\n",
    "\n",
    "Use `add_metric` to add a new term to the metric log. At some point during the batch, compute the metric you wish to log. Then use `Log.update_metric` to add the value to the metric log.\n",
    "\n",
    "Here's an outline implementation:\n",
    "\n",
    "```\n",
    "class MyCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__(name='my_callback')\n",
    "        \n",
    "    def setup(self):\n",
    "        log = self.environment.log\n",
    "        log.add_log(self.name) # adding new term to batch log\n",
    "        log.add_metric(self.name) # adding new term to metrics\n",
    "        \n",
    "    def compute_reward(self):\n",
    "        # make tensor of dummy rewards\n",
    "        batch_state = self.environment.batch_state\n",
    "        bs = len(batch_state.samples)\n",
    "        rewards = to_device(torch.ones(bs).float())\n",
    "        \n",
    "        batch_state.rewards += rewards\n",
    "        batch_state[self.name] = rewards # this is the value the batch log will pick up\n",
    "        \n",
    "    def after_compute_reward(self):\n",
    "        log = self.environment.log\n",
    "        batch_state = self.environment.batch_state\n",
    "        \n",
    "        my_callback_rewards = batch_state[self.name]\n",
    "        my_metric = my_callback_rewards.mean()\n",
    "        \n",
    "        log.update_metric(self.name, my_metric.detach().cpu().numpy()) # update metric value\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Callbacks\n",
    "\n",
    "Several logging related callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class StatsCallback(Callback):\n",
    "    '''\n",
    "    StatsCallback - base class for callbacks related to calculating \n",
    "    stats from batches\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `batch_attribute str`: attribute to grab from the log\n",
    "    \n",
    "    - `grabname Optional[str]`: if passed, the `batch_attribute` values \n",
    "    will be subset for those where `source==grabname`\n",
    "    \n",
    "    - `include_buffer bool`: if True, values sourced from the buffer \n",
    "    that match `grabname` will be included\n",
    "    \n",
    "    - `name str`: callback name\n",
    "    \n",
    "    - `order int`: callback order\n",
    "    '''\n",
    "    def __init__(self, batch_attribute, grabname=None, include_buffer=True, \n",
    "                     name='stats', order=20):\n",
    "        super().__init__(name=name, order=order)\n",
    "        \n",
    "        self.grabname = grabname\n",
    "        self.batch_attribute = batch_attribute\n",
    "        self.include_buffer = include_buffer\n",
    "\n",
    "    def get_values(self):\n",
    "        batch_state = self.environment.batch_state\n",
    "        sources = np.array(batch_state.sources)\n",
    "        \n",
    "        if self.include_buffer:\n",
    "            sources = np.array([i.replace('_buffer', '') for i in sources])\n",
    "        \n",
    "        values = batch_state[self.batch_attribute]\n",
    "        \n",
    "        if self.grabname is not None:\n",
    "            source_mask = sources==self.grabname\n",
    "            if source_mask.sum()>0:\n",
    "                values = values[source_mask]\n",
    "            else:\n",
    "                values = np.array([0.])\n",
    "            \n",
    "        if isinstance(values, torch.Tensor):\n",
    "            values = values.detach().cpu().numpy()\n",
    "            \n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class MaxCallback(StatsCallback):\n",
    "    '''\n",
    "    MaxCallback - adds a metric tracking the maximum of \n",
    "    `batch_attribute`, subset by `grabname`, printed every \n",
    "    batch report\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `batch_attribute str`: attribute to grab from the log\n",
    "    \n",
    "    - `grabname Optional[str]`: if passed, the `batch_attribute` values \n",
    "    will be subset for those where `source==grabname`\n",
    "    \n",
    "    - `include_buffer bool`: if True, values sourced from the buffer \n",
    "    that match `grabname` will be included\n",
    "    '''\n",
    "    def __init__(self, batch_attribute, grabname, include_buffer=True):\n",
    "        \n",
    "        if grabname is None:\n",
    "            name = f'{batch_attribute}_max'\n",
    "        else:\n",
    "            name = f'{batch_attribute}_{grabname}_max'\n",
    "        \n",
    "        super().__init__(batch_attribute, grabname, \n",
    "                         include_buffer=include_buffer, name=name)\n",
    "        \n",
    "        \n",
    "    def setup(self):\n",
    "        log = self.environment.log\n",
    "        log.add_metric(self.name)\n",
    "        \n",
    "    def after_compute_reward(self):\n",
    "        \n",
    "        values = self.get_values()\n",
    "        self.environment.log.update_metric(self.name, values.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class MinCallback(StatsCallback):\n",
    "    '''\n",
    "    MinCallback - adds a metric tracking the minimum of \n",
    "    `batch_attribute`, subset by `grabname`, printed every \n",
    "    batch report\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `batch_attribute str`: attribute to grab from the log\n",
    "    \n",
    "    - `grabname Optional[str]`: if passed, the `batch_attribute` values \n",
    "    will be subset for those where `source==grabname`\n",
    "    \n",
    "    - `include_buffer bool`: if True, values sourced from the buffer \n",
    "    that match `grabname` will be included\n",
    "    '''\n",
    "    def __init__(self, batch_attribute, grabname, include_buffer=True):\n",
    "        \n",
    "        if grabname is None:\n",
    "            name = f'{batch_attribute}_min'\n",
    "        else:\n",
    "            name = f'{batch_attribute}_{grabname}_min'\n",
    "        \n",
    "        super().__init__(batch_attribute, grabname, \n",
    "                         include_buffer=include_buffer, name=name)\n",
    "        \n",
    "        \n",
    "    def setup(self):\n",
    "        log = self.environment.log\n",
    "        log.add_metric(self.name)\n",
    "        \n",
    "    def after_compute_reward(self):\n",
    "        \n",
    "        values = self.get_values()\n",
    "        self.environment.log.update_metric(self.name, values.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class MeanCallback(StatsCallback):\n",
    "    '''\n",
    "    MeanCallback - adds a metric tracking the mean of \n",
    "    `batch_attribute`, subset by `grabname`, printed every \n",
    "    batch report\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `batch_attribute str`: attribute to grab from the log\n",
    "    \n",
    "    - `grabname Optional[str]`: if passed, the `batch_attribute` values \n",
    "    will be subset for those where `source==grabname`\n",
    "    \n",
    "    - `include_buffer bool`: if True, values sourced from the buffer \n",
    "    that match `grabname` will be included\n",
    "    '''\n",
    "    def __init__(self, batch_attribute, grabname, include_buffer=True):\n",
    "        \n",
    "        if grabname is None:\n",
    "            name = f'{batch_attribute}_max'\n",
    "        else:\n",
    "            name = f'{batch_attribute}_{grabname}_max'\n",
    "        \n",
    "        super().__init__(batch_attribute, grabname, \n",
    "                         include_buffer=include_buffer, name=name)\n",
    "        \n",
    "        \n",
    "    def setup(self):\n",
    "        log = self.environment.log\n",
    "        log.add_metric(self.name)\n",
    "        \n",
    "    def after_compute_reward(self):\n",
    "        \n",
    "        values = self.get_values()\n",
    "        self.environment.log.update_metric(self.name, values.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class PercentileCallback(StatsCallback):\n",
    "    '''\n",
    "    PercentileCallback - adds a metric tracking the `percentile` \n",
    "    percentile value of `batch_attribute`, subset by `grabname`, \n",
    "    printed every batch report\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `batch_attribute str`: attribute to grab from the log\n",
    "    \n",
    "    - `grabname Optional[str]`: if passed, the `batch_attribute` values \n",
    "    will be subset for those where `source==grabname`\n",
    "    \n",
    "    - `percentile str`: what percentile value to use\n",
    "    \n",
    "    - `include_buffer bool`: if True, values sourced from the buffer \n",
    "    that match `grabname` will be included\n",
    "    '''\n",
    "    def __init__(self, batch_attribute, grabname, percentile, include_buffer=True):\n",
    "        \n",
    "        if grabname is None:\n",
    "            name = f'{batch_attribute}_p{percentile}'\n",
    "        else:\n",
    "            name = f'{batch_attribute}_{grabname}_p{percentile}'\n",
    "        \n",
    "        super().__init__(batch_attribute, grabname, \n",
    "                         include_buffer=include_buffer, name=name)\n",
    "        self.percentile = percentile\n",
    "        \n",
    "    def setup(self):\n",
    "        log = self.environment.log\n",
    "        log.add_metric(self.name)\n",
    "        \n",
    "    def after_compute_reward(self):\n",
    "        \n",
    "        values = self.get_values()\n",
    "        self.environment.log.update_metric(self.name, np.percentile(values, self.percentile))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class SaveLogDF(Callback):\n",
    "    '''\n",
    "    SaveLogDF - periodically saves the Log\n",
    "    dataframe during training\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `frequency int`: how often to save\n",
    "    \n",
    "    - `save_path str`: directory to save \n",
    "    files to\n",
    "    '''\n",
    "    def __init__(self, frequency, save_path):\n",
    "        super().__init__(name='save_log')\n",
    "        self.frequency = frequency\n",
    "        self.save_path = save_path\n",
    "        \n",
    "    def after_batch(self):\n",
    "        log = self.environment.log\n",
    "        if (log.iterations%self.frequency)==0 and log.iterations>0:\n",
    "            log.df.to_csv(f'{self.save_path}/log_df_{log.iterations}.csv', index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
