{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp policy_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient\n",
    "\n",
    "> Policy gradient modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from mrl.imports import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *\n",
    "from mrl.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class BasePolicy():\n",
    "    def __init__(self, gamma=1.):\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def discount_rewards(self, rewards, mask, traj_rewards):\n",
    "        rewards = scatter_rewards(rewards, mask)\n",
    "\n",
    "        if traj_rewards is not None:\n",
    "            rewards += traj_rewards\n",
    "\n",
    "        discounted = discount_rewards(rewards, self.gamma)\n",
    "\n",
    "        return discounted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class PolicyGradient(BasePolicy):\n",
    "    def __init__(self, discount=True, gamma=0.97, ratio=False, scale_rewards=True):\n",
    "        super().__init__(gamma)\n",
    "        self.discount = discount\n",
    "        self.ratio = ratio\n",
    "        self.scale_rewards = scale_rewards\n",
    "        self.mean_reward = None\n",
    "        \n",
    "    def __call__(self, lps, mask, rewards, base_lps=None, traj_rewards=None):\n",
    "        if self.ratio:\n",
    "            lps = (lps - base_lps.detach()).exp()\n",
    "            \n",
    "        if not self.discount:\n",
    "            pg_loss = -((lps*mask).sum(-1)*rewards)/mask.sum(-1)\n",
    "            \n",
    "        else:\n",
    "            rewards = self.discount_rewards(rewards, mask, traj_rewards)\n",
    "            rewards = whiten(rewards, mask=mask)\n",
    "            pg_loss = -(lps*rewards*mask).sum(-1)/mask.sum(-1)\n",
    "            \n",
    "        pg_dict = {'loss':pg_loss.detach().cpu(), \n",
    "                   'rewards':rewards.detach().cpu()}\n",
    "            \n",
    "        return pg_loss, pg_dict\n",
    "    \n",
    "    def from_batch_state(self, batch_state):\n",
    "        lps = batch_state.model_gathered_logprobs\n",
    "        base_lps = batch_state.base_gathered_logprobs\n",
    "        mask = batch_state.mask\n",
    "        \n",
    "        rewards = batch_state.rewards\n",
    "        \n",
    "        if self.scale_rewards:\n",
    "            if self.mean_reward is None:\n",
    "                self.mean_reward = rewards.mean()\n",
    "            else:\n",
    "                self.mean_reward = (1-self.gamma)*rewards.mean() + self.gamma*self.mean_reward\n",
    "                \n",
    "            rewards = rewards - self.mean_reward\n",
    "        \n",
    "        traj_rewards = batch_state.trajectory_rewards\n",
    "        \n",
    "        loss, pg_dict = self(lps, mask, rewards, base_lps, traj_rewards)\n",
    "        return loss, pg_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class TRPO(BasePolicy):\n",
    "    def __init__(self, gamma, kl_target, beta=1., eta=50, lam=0.95, \n",
    "                 v_coef=0.5, scale_rewards=True):\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.eta = eta\n",
    "        self.lam = lam\n",
    "        self.kl_target = kl_target\n",
    "        self.v_coef = v_coef\n",
    "        self.scale_rewards = scale_rewards\n",
    "        self.mean_reward = None\n",
    "        \n",
    "    def __call__(self, lps_g, base_lps_g, lps, base_lps, mask, \n",
    "                 rewards, values, traj_rewards=None):\n",
    "    \n",
    "        discounted_rewards = self.discount_rewards(rewards, mask, traj_rewards)\n",
    "        advantages = self.compute_advantages(discounted_rewards, values)\n",
    "        advantages = whiten(advantages, mask=mask)\n",
    "        \n",
    "        v_loss = self.value_loss(values, discounted_rewards)\n",
    "        \n",
    "        ratios = (lps_g - base_lps_g.detach()).exp()\n",
    "        \n",
    "        loss1 = -(ratios*advantages*mask).sum(-1)/mask.sum(-1)\n",
    "        \n",
    "        kl = torch.distributions.kl.kl_divergence(\n",
    "                    Categorical(logits=base_lps),\n",
    "                    Categorical(logits=lps))\n",
    "        \n",
    "        kl = (kl*mask).sum(-1)/mask.sum(-1)\n",
    "        loss2 = self.beta*kl\n",
    "        \n",
    "        loss3 = self.eta * torch.maximum(to_device(torch.tensor(0.)), \n",
    "                                         kl - 2.0*self.kl_target)\n",
    "\n",
    "        pg_loss = loss1 + loss2 + loss3 + v_loss.mean(-1)\n",
    "        \n",
    "        pg_dict = { 'loss' : pg_loss.detach().cpu(),\n",
    "                    'pg_discounted' : discounted_rewards.detach().cpu(),\n",
    "                    'pg_advantage' : advantages.detach().cpu(),\n",
    "                    'ratios' : ratios.detach().cpu(),\n",
    "                    'kl' : kl.detach().cpu(),\n",
    "                    'loss1' : loss1.detach().cpu(),\n",
    "                    'loss2' : loss2.detach().cpu(),\n",
    "                    'loss3' : loss3.detach().cpu(),\n",
    "                    'v_loss' : v_loss.detach().cpu()}\n",
    "        \n",
    "        return pg_loss, pg_dict\n",
    "    \n",
    "    def from_batch_state(self, batch_state):\n",
    "        lps_g = batch_state.model_gathered_logprobs\n",
    "        base_lps_g = batch_state.base_gathered_logprobs\n",
    "        \n",
    "        lps = batch_state.model_logprobs\n",
    "        base_lps = batch_state.base_logprobs\n",
    "        \n",
    "        mask = batch_state.mask\n",
    "        rewards = batch_state.rewards\n",
    "        \n",
    "        if self.scale_rewards:\n",
    "            if self.mean_reward is None:\n",
    "                self.mean_reward = rewards.mean()\n",
    "            else:\n",
    "                self.mean_reward = (1-self.gamma)*rewards.mean() + self.gamma*self.mean_reward\n",
    "                \n",
    "            rewards = rewards - self.mean_reward\n",
    "        \n",
    "        traj_rewards = batch_state.trajectory_rewards\n",
    "        \n",
    "        values = batch_state.state_values\n",
    "        \n",
    "        loss, pg_dict = self(lps_g, base_lps_g, lps, base_lps, mask, \n",
    "                             rewards, values, traj_rewards)\n",
    "        return loss, pg_dict\n",
    "\n",
    "    def compute_advantages(self, rewards, values):\n",
    "\n",
    "        if values is None:\n",
    "            advantages = rewards\n",
    "        else:\n",
    "            advantages = compute_advantages(rewards, values.detach(), self.gamma, self.lam)\n",
    "\n",
    "        return advantages\n",
    "\n",
    "    def value_loss(self, values, rewards):\n",
    "        if values is None:\n",
    "            v_loss = to_device(torch.tensor(0.))\n",
    "        else:\n",
    "            v_loss = self.v_coef*F.mse_loss(values, rewards, reduction='none')\n",
    "\n",
    "        return v_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export   \n",
    "\n",
    "class PPO(BasePolicy):\n",
    "    def __init__(self, gamma, kl_coef, lam=0.95, v_coef=0.5, cliprange=0.2, \n",
    "                 v_cliprange=0.2, ent_coef=0.01, kl_target=None, \n",
    "                 kl_horizon=None, scale_rewards=True):\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.ent_coef = ent_coef\n",
    "        self.kl_coef = kl_coef\n",
    "        self.kl_target = kl_target\n",
    "        self.kl_horizon = kl_horizon\n",
    "        self.v_coef = v_coef\n",
    "        self.cliprange = cliprange\n",
    "        self.v_cliprange = v_cliprange\n",
    "        self.scale_rewards = scale_rewards\n",
    "        self.mean_reward = None\n",
    "        \n",
    "    def __call__(self, lps, base_lps, mask, \n",
    "                 rewards, values, ref_values, traj_rewards=None):\n",
    "        \n",
    "        discounted_rewards = self.discount_rewards(rewards, mask, traj_rewards)\n",
    "        kl_reward = self.compute_kl_reward(lps, base_lps)\n",
    "        \n",
    "        discounted_rewards = discounted_rewards + kl_reward\n",
    "        advantages = self.compute_advantages(discounted_rewards, values)\n",
    "        advantages = whiten(advantages, mask=mask)\n",
    "        \n",
    "        v_loss = self.value_loss(values, ref_values, discounted_rewards)\n",
    "        \n",
    "        ratios = (lps - base_lps).exp()\n",
    "        ratios_clipped = torch.clamp(ratios, 1.0-self.cliprange, 1.0+self.cliprange)\n",
    "        \n",
    "        loss1 = -(ratios*advantages)\n",
    "        loss2 = -(ratios_clipped*advantages)\n",
    "        \n",
    "        loss = torch.maximum(loss1, loss2)\n",
    "        loss = (loss*mask).sum(-1)/mask.sum(-1)\n",
    "        \n",
    "        entropy = Categorical(lps).entropy()\n",
    "                \n",
    "        pg_loss = loss + v_loss.mean(-1) - self.ent_coef*entropy\n",
    "        \n",
    "        self.update_kl(lps, base_lps, mask)\n",
    "         \n",
    "        pg_dict = { 'loss' : pg_loss.detach().cpu(),\n",
    "                    'pg_discounted' : discounted_rewards,\n",
    "                    'pg_advantage' : advantages,\n",
    "                    'ratios' : ratios.detach().cpu(),\n",
    "                    'loss' : loss.detach().cpu(),\n",
    "                    'v_loss' : v_loss.detach().cpu(),\n",
    "                    'entropy' : entropy.detach().cpu()}\n",
    "        \n",
    "        return pg_loss, pg_dict\n",
    "    \n",
    "    def from_batch_state(self, batch_state):\n",
    "        lps = batch_state.model_gathered_logprobs\n",
    "        base_lps = batch_state.base_gathered_logprobs\n",
    "        \n",
    "        \n",
    "        mask = batch_state.mask\n",
    "        rewards = batch_state.rewards\n",
    "        \n",
    "        if self.scale_rewards:\n",
    "            if self.mean_reward is None:\n",
    "                self.mean_reward = rewards.mean()\n",
    "            else:\n",
    "                self.mean_reward = (1-self.gamma)*rewards.mean() + self.gamma*self.mean_reward\n",
    "                \n",
    "            rewards = rewards - self.mean_reward\n",
    "            \n",
    "        traj_rewards = batch_state.trajectory_rewards\n",
    "        \n",
    "        values = batch_state.state_values\n",
    "        ref_values = batch_state.ref_state_values\n",
    "        \n",
    "        loss, pg_dict = self(lps, base_lps, mask, rewards, \n",
    "                             values, ref_values, traj_rewards)\n",
    "        \n",
    "        return loss, pg_dict\n",
    "            \n",
    "    def compute_kl_reward(self, lps, base_lps):\n",
    "        kl = lps - base_lps\n",
    "        kl_reward = -self.kl_coef * kl.detach()\n",
    "        return kl_reward\n",
    "    \n",
    "    def value_loss(self, values, old_values, rewards):\n",
    "        if values is None:\n",
    "            v_loss = to_device(torch.tensor(0.))\n",
    "        else:\n",
    "            \n",
    "            v_loss = F.mse_loss(values, rewards, reduction='none')\n",
    "            \n",
    "            if old_values is not None:\n",
    "                min_v = old_values - self.v_cliprange\n",
    "                max_v = old_values + self.v_cliprange\n",
    "                \n",
    "                values_clipped = torch.max(torch.min(values, max_v), min_v)\n",
    "                v_loss2 = F.mse_loss(values_clipped, rewards, reduction='none')\n",
    "                \n",
    "                v_loss = torch.max(v_loss, v_loss2)\n",
    "            \n",
    "            v_loss = self.v_coef*v_loss\n",
    "            \n",
    "        return v_loss\n",
    "    \n",
    "    def compute_advantages(self, rewards, values):\n",
    "        \n",
    "        if values is None:\n",
    "            advantages = rewards\n",
    "        else:\n",
    "            advantages = compute_advantages(rewards, values.detach(), self.gamma, self.lam)\n",
    "            \n",
    "        return advantages\n",
    "    \n",
    "    def update_kl(self, lps, base_lps, mask):\n",
    "        \n",
    "        if (self.kl_target is not None) and (self.kl_horizon is not None):\n",
    "            kl = (lps - base_lps).detach()\n",
    "            kl = (kl*mask).sum(-1)/mask.sum(-1)\n",
    "            kl = kl.cpu().mean()\n",
    "            \n",
    "            error = torch.clip(kl/self.kl_target - 1, -0.2, 0.2)\n",
    "            factor = 1 + error * lps.shape[0]/self.kl_horizon\n",
    "            self.kl_coef *= factor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mrl)",
   "language": "python",
   "name": "mrl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
