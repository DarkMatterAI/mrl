{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp policy_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient\n",
    "\n",
    "> Policy gradient modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from mrl.imports import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *\n",
    "from mrl.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradients\n",
    "\n",
    "The code in this module implements several policy gradient algorithms\n",
    "\n",
    "- `PolicyGradient` - implements [Policy Gradients](https://papers.nips.cc/paper/1999/hash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html)\n",
    "\n",
    "- `TRPO` - implements [Trust Region Policy Optimization](https://arxiv.org/pdf/1502.05477.pdf)\n",
    "\n",
    "- `PPO` - implemeents [Proximal Policy Optimization](https://arxiv.org/pdf/1707.06347.pdf)\n",
    "\n",
    "### Current Limitations\n",
    "\n",
    "The implementations below are designed for the scenario where the output of the model is a series of actions over time. Importantly, rewards are discounted going backwards, meaning the disccounted reward at very timestep contains some of the future rewards. If your model does not predict a series of rewards (ie predicts a single graph), you may need to revisit these assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class BasePolicy():\n",
    "    '''\n",
    "    BasePolicy - base policy class\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `gamma float`: discount factor\n",
    "    '''\n",
    "    def __init__(self, gamma=1.):\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def discount_rewards(self, rewards, mask, traj_rewards=None):\n",
    "        '''\n",
    "        discount_rewards - discounts rewards\n",
    "        \n",
    "        Inputs:\n",
    "        \n",
    "        - `rewards torch.Tensor[bs]`: reward tensor (one reward per batch item)\n",
    "        \n",
    "        - `mask torch.BoolTensor[bs, sl]`: mask (ie for padding). `True` indicates \n",
    "        values that will be kept, `False` indicates values that will be masked\n",
    "        \n",
    "        - `traj_rewards Optional[torch.Tensor[bs, sl]]`: trajectory rewards. \n",
    "        Has a reward value for each time point\n",
    "        '''\n",
    "        rewards = scatter_rewards(rewards, mask)\n",
    "\n",
    "        if traj_rewards is not None:\n",
    "            rewards += traj_rewards\n",
    "\n",
    "        discounted = discount_rewards(rewards, self.gamma)\n",
    "\n",
    "        return discounted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"BasePolicy.discount_rewards\" class=\"doc_header\"><code>BasePolicy.discount_rewards</code><a href=\"__main__.py#L14\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>BasePolicy.discount_rewards</code>(**`rewards`**, **`mask`**, **`traj_rewards`**=*`None`*)\n",
       "\n",
       "discount_rewards - discounts rewards\n",
       "\n",
       "Inputs:\n",
       "\n",
       "- `rewards torch.Tensor[bs]`: reward tensor (one reward per batch item)\n",
       "\n",
       "- `mask torch.BoolTensor[bs, sl]`: mask (ie for padding). `True` indicates \n",
       "values that will be kept, `False` indicates values that will be masked\n",
       "\n",
       "- `traj_rewards Optional[torch.Tensor[bs, sl]]`: trajectory rewards. \n",
       "Has a reward value for each time point"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(BasePolicy.discount_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradients\n",
    "\n",
    "`PolicyGradient` implements standard [Policy Gradients](https://papers.nips.cc/paper/1999/hash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html) following:\n",
    "\n",
    "$$ \\nabla_\\theta J(\\theta) = \\mathbb{E}_\\pi [R(s,a) \\nabla_\\theta \\ln \\pi_\\theta(a \\vert s)] $$\n",
    "\n",
    "When we generate a sample through autoregressive Monte Carlo sampling, we create a sequence of actions which we represent as a tensor of size `(bs, sl)`. \n",
    "\n",
    "For each step in this series, we have a probability disribution over all possible actions. This give us a tensor of log probabilities of size `(bs, sl, n_actions)`. We can then gather the log probabilities for the actions we actually took, giving us a tensor of gathered log probabilities of size `(bs, sl)`.\n",
    "\n",
    "We also have a set of rewards associated with each sample. In the context of generating compounds, we most often have a single reward for each sampling trajectory that represents the final score of he whole molecule. This would be a tensor of size `(bs)`. If applicable, we can also have a tensor of trajectory rewards which has a reward for each sampling timestep. This trajectory reward tensor would be of size `(bs, sl)`.\n",
    "\n",
    "These rewards are discounted over all timesteps using `discount_rewards`, then scaled using `whiten`. This gives is our final tensor of rewards of size `(bs, sl)`.\n",
    "\n",
    "Now we can compute the empirical expecttion $\\mathbb{E}_\\pi [R(s,a) \\nabla_\\theta \\ln \\pi_\\theta(a \\vert s)]$ by multiplying the gathered log probabilities by the discounted rewards and taking the mean over the batch.\n",
    "\n",
    "Then of course we want to maximize this expectation, so we use gradient descent to minimize $-\\mathbb{E}_\\pi [R(s,a) \\nabla_\\theta \\ln \\pi_\\theta(a \\vert s)]$\n",
    "\n",
    "This basically tells the model to increase the probability of sample paths that had above-average rewards within the batch, and decrease the probability of sample paths with below-average rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class PolicyGradient(BasePolicy):\n",
    "    '''\n",
    "    PolicyGradient - Basic policy gradient implementation\n",
    "    \n",
    "    papers.nips.cc/paper/1999/hash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `discount bool`: if True, rewards are discounted over all timesteps\n",
    "    \n",
    "    - `gamma float`: discount factor (ignored if `discount=False`)\n",
    "    \n",
    "    - `ratio True`: if True, model log probbilities are replaced with the \n",
    "    ratio between main model log probabilities and baseline model log probabilities, \n",
    "    a technique used in more sophistocated policy gradient algorithms. This can \n",
    "    improve stability\n",
    "    \n",
    "    - `scale_rewards bool`: if True, rewards are mean-scaled before discounting. \n",
    "    This can lead to quicker convergence \n",
    "    '''\n",
    "    def __init__(self, discount=True, gamma=0.97, ratio=False, scale_rewards=True):\n",
    "        super().__init__(gamma)\n",
    "        self.discount = discount\n",
    "        self.ratio = ratio\n",
    "        self.scale_rewards = scale_rewards\n",
    "        self.mean_reward = None\n",
    "        \n",
    "    def __call__(self, lps, mask, rewards, base_lps=None, traj_rewards=None):\n",
    "        '''\n",
    "        Inputs:\n",
    "        \n",
    "        - `lps torch.FloatTensor[bs, sl]`: gathered log probabilities\n",
    "        \n",
    "        - `mask torch.BoolTensor[bs, sl]`: padding mask. `True` indicates \n",
    "        values that will be kept, `False` indicates values that will be masked\n",
    "        \n",
    "        - `rewards torch.FloatTensor[bs]`: reward tensor (one reward per batch item)\n",
    "        \n",
    "        - `base_lps Optional[torch.FloatTensor[bs, sl]]`: optional \n",
    "        base model gathered log probabilities\n",
    "        \n",
    "        - `traj_rewards Optional[torch.FloatTensor[bs, sl]]`: optional tensor of \n",
    "        trajectory rewards with one reward value per timestep\n",
    "        '''\n",
    "        \n",
    "        if self.ratio:\n",
    "            lps = (lps - base_lps.detach()).exp()\n",
    "            \n",
    "        if not self.discount:\n",
    "            pg_loss = -((lps*mask).sum(-1)*rewards)/mask.sum(-1)\n",
    "            \n",
    "        else:\n",
    "            rewards = self.discount_rewards(rewards, mask, traj_rewards)\n",
    "            rewards = whiten(rewards, mask=mask)\n",
    "            pg_loss = -(lps*rewards*mask).sum(-1)/mask.sum(-1)\n",
    "            \n",
    "        pg_dict = {'loss':pg_loss.detach().cpu(), \n",
    "                   'rewards':rewards.detach().cpu()}\n",
    "        \n",
    "        self.last_outputs = pg_dict\n",
    "            \n",
    "        return pg_loss, pg_dict\n",
    "    \n",
    "    def from_batch_state(self, batch_state):\n",
    "        lps = batch_state.model_gathered_logprobs\n",
    "        base_lps = batch_state.base_gathered_logprobs\n",
    "        mask = batch_state.mask\n",
    "        \n",
    "        rewards = batch_state.rewards_final\n",
    "        \n",
    "        if self.scale_rewards:\n",
    "            if self.mean_reward is None:\n",
    "                self.mean_reward = rewards.mean()\n",
    "            else:\n",
    "                self.mean_reward = (1-self.gamma)*rewards.mean() + self.gamma*self.mean_reward\n",
    "                \n",
    "            rewards = rewards - self.mean_reward\n",
    "        \n",
    "        traj_rewards = batch_state.trajectory_rewards\n",
    "        \n",
    "        loss, pg_dict = self(lps, mask, rewards, base_lps, traj_rewards)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"PolicyGradient.__call__\" class=\"doc_header\"><code>PolicyGradient.__call__</code><a href=\"__main__.py#L30\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>PolicyGradient.__call__</code>(**`lps`**, **`mask`**, **`rewards`**, **`base_lps`**=*`None`*, **`traj_rewards`**=*`None`*)\n",
       "\n",
       "Inputs:\n",
       "\n",
       "- `lps torch.FloatTensor[bs, sl]`: gathered log probabilities\n",
       "\n",
       "- `mask torch.BoolTensor[bs, sl]`: padding mask. `True` indicates \n",
       "values that will be kept, `False` indicates values that will be masked\n",
       "\n",
       "- `rewards torch.FloatTensor[bs]`: reward tensor (one reward per batch item)\n",
       "\n",
       "- `base_lps Optional[torch.FloatTensor[bs, sl]]`: optional \n",
       "base model gathered log probabilities\n",
       "\n",
       "- `traj_rewards Optional[torch.FloatTensor[bs, sl]]`: optional tensor of \n",
       "trajectory rewards with one reward value per timestep"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(PolicyGradient.__call__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trust Region Policy Optimization\n",
    "\n",
    "[Trust Region Policy Optimization](https://arxiv.org/pdf/1502.05477.pdf) (TRPO) adapts the policy gradient algorithm by constraining the maximum update size based on how far the current agent has deviated from the baseline agent.\n",
    "\n",
    "$$ J(\\theta) = \\mathbb{E}_{s \\sim \\rho^{\\pi_{\\theta_\\text{old}}}, a \\sim \\pi_{\\theta_\\text{old}}} \\big[ \\frac{\\pi_\\theta(a \\vert s)}{\\pi_{\\theta_\\text{old}}(a \\vert s)} \\hat{A}_{\\theta_\\text{old}}(s, a) \\big] $$\n",
    "\n",
    "Subject to a KL constraint between the current policy and the baseline policy\n",
    "\n",
    "$$ \\mathbb{E}_{s \\sim \\rho^{\\pi_{\\theta_\\text{old}}}} [D_\\text{KL}(\\pi_{\\theta_\\text{old}}(.\\vert s) \\| \\pi_\\theta(.\\vert s)] \\leq \\delta $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class TRPO(BasePolicy):\n",
    "    '''\n",
    "    TRPO - Trust Region Policy Optimization\n",
    "    \n",
    "    arxiv.org/pdf/1502.05477.pdf\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `gamma float`: discount factor\n",
    "    \n",
    "    - `kl_target float`: target maximum KL divergence from baseline policy\n",
    "    \n",
    "    - `beta float`: coefficient for the KL loss\n",
    "    \n",
    "    - `eta float`: coefficient for penalizing KL higher than `2*kl_target`\n",
    "    \n",
    "    - `lam float`: lambda coefficient for advantage calculation\n",
    "    \n",
    "    - `v_coef float`: value function loss coefficient\n",
    "    \n",
    "    - `scale_rewards bool`: if True, rewards are mean-scaled before discounting. \n",
    "    This can lead to quicker convergence \n",
    "    '''\n",
    "    def __init__(self, gamma, kl_target, beta=1., eta=50, lam=0.95, \n",
    "                 v_coef=0.5, scale_rewards=True):\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.eta = eta\n",
    "        self.lam = lam\n",
    "        self.kl_target = kl_target\n",
    "        self.v_coef = v_coef\n",
    "        self.scale_rewards = scale_rewards\n",
    "        self.mean_reward = None\n",
    "        \n",
    "    def __call__(self, lps_g, base_lps_g, lps, base_lps, mask, \n",
    "                 rewards, values, traj_rewards=None):\n",
    "        '''\n",
    "        Inputs:\n",
    "        \n",
    "        - `lps_g torch.FloatTensor[bs, sl]`: model gathered log probabilities\n",
    "        \n",
    "        - `base_lps_g torch.FloatTensor[bs, sl]`: baseline model \n",
    "        gathered log probabilities\n",
    "        \n",
    "        - `lps torch.FloatTensor[bs, sl, n_actions]`: model full log probabilities\n",
    "        \n",
    "        - `base_lps torch.FloatTensor[bs, sl, n_actions]`: baseline model \n",
    "        full log probabilities\n",
    "        \n",
    "        - `mask torch.BoolTensor[bs, sl]`: padding mask. `True` indicates \n",
    "        values that will be kept, `False` indicates values that will be masked\n",
    "        \n",
    "        - `rewards torch.FloatTensor[bs]`: reward tensor (one reward per batch item)\n",
    "        \n",
    "        - `values torch.FloatTensor[bs, sl]`: state value predictions\n",
    "        \n",
    "        - `traj_rewards Optional[torch.FloatTensor[bs, sl]]`: optional tensor of \n",
    "        trajectory rewards with one reward value per timestep\n",
    "        '''\n",
    "    \n",
    "        discounted_rewards = self.discount_rewards(rewards, mask, traj_rewards)\n",
    "        advantages = self.compute_advantages(discounted_rewards, values)\n",
    "        advantages = whiten(advantages, mask=mask)\n",
    "        \n",
    "        v_loss = self.value_loss(values, discounted_rewards)\n",
    "        \n",
    "        ratios = (lps_g - base_lps_g.detach()).exp()\n",
    "        \n",
    "        loss1 = -(ratios*advantages*mask).sum(-1)/mask.sum(-1)\n",
    "        \n",
    "        kl = torch.distributions.kl.kl_divergence(\n",
    "                    Categorical(logits=base_lps),\n",
    "                    Categorical(logits=lps))\n",
    "        \n",
    "        kl = (kl*mask).sum(-1)/mask.sum(-1)\n",
    "        loss2 = self.beta*kl\n",
    "        \n",
    "        loss3 = self.eta * torch.maximum(to_device(torch.tensor(0.)), \n",
    "                                         kl - 2.0*self.kl_target)\n",
    "\n",
    "        pg_loss = loss1 + loss2 + loss3 + v_loss.mean(-1)\n",
    "        \n",
    "        pg_dict = { 'loss' : pg_loss.detach().cpu(),\n",
    "                    'pg_discounted' : discounted_rewards.detach().cpu(),\n",
    "                    'pg_advantage' : advantages.detach().cpu(),\n",
    "                    'ratios' : ratios.detach().cpu(),\n",
    "                    'kl' : kl.detach().cpu(),\n",
    "                    'loss1' : loss1.detach().cpu(),\n",
    "                    'loss2' : loss2.detach().cpu(),\n",
    "                    'loss3' : loss3.detach().cpu(),\n",
    "                    'v_loss' : v_loss.detach().cpu()}\n",
    "        \n",
    "        self.last_outputs = pg_dict\n",
    "        \n",
    "        return pg_loss, pg_dict\n",
    "    \n",
    "    def from_batch_state(self, batch_state):\n",
    "        lps_g = batch_state.model_gathered_logprobs\n",
    "        base_lps_g = batch_state.base_gathered_logprobs\n",
    "        \n",
    "        lps = batch_state.model_logprobs\n",
    "        base_lps = batch_state.base_logprobs\n",
    "        \n",
    "        mask = batch_state.mask\n",
    "        rewards = batch_state.rewards_final\n",
    "        \n",
    "        if self.scale_rewards:\n",
    "            if self.mean_reward is None:\n",
    "                self.mean_reward = rewards.mean()\n",
    "            else:\n",
    "                self.mean_reward = (1-self.gamma)*rewards.mean() + self.gamma*self.mean_reward\n",
    "                \n",
    "            rewards = rewards - self.mean_reward\n",
    "        \n",
    "        traj_rewards = batch_state.trajectory_rewards\n",
    "        \n",
    "        values = batch_state.state_values\n",
    "        \n",
    "        loss, pg_dict = self(lps_g, base_lps_g, lps, base_lps, mask, \n",
    "                             rewards, values, traj_rewards)\n",
    "        return loss\n",
    "\n",
    "    def compute_advantages(self, rewards, values):\n",
    "\n",
    "        if values is None:\n",
    "            advantages = rewards\n",
    "        else:\n",
    "            advantages = compute_advantages(rewards, values.detach(), self.gamma, self.lam)\n",
    "\n",
    "        return advantages\n",
    "\n",
    "    def value_loss(self, values, rewards):\n",
    "        if values is None:\n",
    "            v_loss = to_device(torch.tensor(0.))\n",
    "        else:\n",
    "            v_loss = self.v_coef*F.mse_loss(values, rewards, reduction='none')\n",
    "\n",
    "        return v_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"TRPO.__call__\" class=\"doc_header\"><code>TRPO.__call__</code><a href=\"__main__.py#L37\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>TRPO.__call__</code>(**`lps_g`**, **`base_lps_g`**, **`lps`**, **`base_lps`**, **`mask`**, **`rewards`**, **`values`**, **`traj_rewards`**=*`None`*)\n",
       "\n",
       "Inputs:\n",
       "\n",
       "- `lps_g torch.FloatTensor[bs, sl]`: model gathered log probabilities\n",
       "\n",
       "- `base_lps_g torch.FloatTensor[bs, sl]`: baseline model \n",
       "gathered log probabilities\n",
       "\n",
       "- `lps torch.FloatTensor[bs, sl, n_actions]`: model full log probabilities\n",
       "\n",
       "- `base_lps torch.FloatTensor[bs, sl, n_actions]`: baseline model \n",
       "full log probabilities\n",
       "\n",
       "- `mask torch.BoolTensor[bs, sl]`: padding mask. `True` indicates \n",
       "values that will be kept, `False` indicates values that will be masked\n",
       "\n",
       "- `rewards torch.FloatTensor[bs]`: reward tensor (one reward per batch item)\n",
       "\n",
       "- `values torch.FloatTensor[bs, sl]`: state value predictions\n",
       "\n",
       "- `traj_rewards Optional[torch.FloatTensor[bs, sl]]`: optional tensor of \n",
       "trajectory rewards with one reward value per timestep"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TRPO.__call__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization\n",
    "\n",
    "[Proximal Policy Optimization](https://arxiv.org/pdf/1707.06347.pdf) (PPO) applies clipping to the surrogate objective along with the KL constraints\n",
    "\n",
    "\n",
    "$$ r(\\theta) = \\frac{\\pi_\\theta(a \\vert s)}{\\pi_{\\theta_\\text{old}}(a \\vert s)} $$\n",
    "\n",
    "$$ J(\\theta) = \\mathbb{E} [ r(\\theta) \\hat{A}_{\\theta_\\text{old}}(s, a) ] $$\n",
    "\n",
    "$$ J^\\text{CLIP} (\\theta) = \\mathbb{E} [ \\min( r(\\theta) \\hat{A}_{\\theta_\\text{old}}(s, a), \\text{clip}(r(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_{\\theta_\\text{old}}(s, a))] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export   \n",
    "\n",
    "class PPO(BasePolicy):\n",
    "    '''\n",
    "    PPO - Proximal policy optimization\n",
    "    \n",
    "    arxiv.org/pdf/1707.06347.pdf\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `gamma float`: discount factor\n",
    "    \n",
    "    - `kl_coef float`: KL reward coefficient\n",
    "    \n",
    "    - `lam float`: lambda coefficient for advantage calculation\n",
    "    \n",
    "    - `v_coef float`: value function loss coefficient\n",
    "    \n",
    "    - `cliprange float`: clip value for surrogate loss\n",
    "    \n",
    "    - `v_cliprange float`: clip value for value function predictions\n",
    "    \n",
    "    - `ent_coef float`: entropy regularization coefficient\n",
    "    \n",
    "    - `kl_target Optional[float]`: target value for adaptive KL penalty\n",
    "    \n",
    "    - `kl_horizon Optional[float]`: horizon for adaptive KL penalty\n",
    "    \n",
    "    - `scale_rewards bool`: if True, rewards are mean-scaled before discounting. \n",
    "    This can lead to quicker convergence \n",
    "    '''\n",
    "    def __init__(self, gamma, kl_coef, lam=0.95, v_coef=0.5, cliprange=0.2, \n",
    "                 v_cliprange=0.2, ent_coef=0.01, kl_target=None, \n",
    "                 kl_horizon=None, scale_rewards=True):\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.ent_coef = ent_coef\n",
    "        self.kl_coef = kl_coef\n",
    "        self.kl_target = kl_target\n",
    "        self.kl_horizon = kl_horizon\n",
    "        self.v_coef = v_coef\n",
    "        self.cliprange = cliprange\n",
    "        self.v_cliprange = v_cliprange\n",
    "        self.scale_rewards = scale_rewards\n",
    "        self.mean_reward = None\n",
    "        \n",
    "    def __call__(self, lps_g, base_lps_g, lps, mask, \n",
    "                 rewards, values, ref_values, traj_rewards=None):\n",
    "        \n",
    "        '''\n",
    "        Inputs:\n",
    "        \n",
    "        - `lps_g torch.FloatTensor[bs, sl]`: model gathered log probabilities\n",
    "        \n",
    "        - `base_lps_g torch.FloatTensor[bs, sl]`: baseline model \n",
    "        gathered log probabilities\n",
    "        \n",
    "        - `lps torch.FloatTensor[bs, sl, n_actions]`: model full log probabilities\n",
    "        \n",
    "        - `mask torch.BoolTensor[bs, sl]`: padding mask. `True` indicates \n",
    "        values that will be kept, `False` indicates values that will be masked\n",
    "        \n",
    "        - `rewards torch.FloatTensor[bs]`: reward tensor (one reward per batch item)\n",
    "        \n",
    "        - `values torch.FloatTensor[bs, sl]`: state value predictions\n",
    "        \n",
    "        - `ref_values torch.FloatTensor[bs, sl]`: baseline state value predictions\n",
    "        \n",
    "        - `traj_rewards Optional[torch.FloatTensor[bs, sl]]`: optional tensor of \n",
    "        trajectory rewards with one reward value per timestep\n",
    "        '''\n",
    "        \n",
    "        discounted_rewards = self.discount_rewards(rewards, mask, traj_rewards)\n",
    "        kl_reward = self.compute_kl_reward(lps_g, base_lps_g)\n",
    "        \n",
    "        discounted_rewards = discounted_rewards + kl_reward\n",
    "        advantages = self.compute_advantages(discounted_rewards, values)\n",
    "        advantages = whiten(advantages, mask=mask)\n",
    "        \n",
    "        v_loss = self.value_loss(values, ref_values, discounted_rewards)\n",
    "        \n",
    "        ratios = (lps_g - base_lps_g.detach()).exp()\n",
    "        ratios_clipped = torch.clamp(ratios, 1.0-self.cliprange, 1.0+self.cliprange)\n",
    "        \n",
    "        loss1 = -(ratios*advantages)\n",
    "        loss2 = -(ratios_clipped*advantages)\n",
    "        \n",
    "        loss = torch.maximum(loss1, loss2)\n",
    "        loss = (loss*mask).sum(-1)/mask.sum(-1)\n",
    "        \n",
    "        entropy = Categorical(logits=lps).entropy().mean(-1)\n",
    "                \n",
    "        pg_loss = loss + v_loss.mean(-1) - self.ent_coef*entropy\n",
    "        \n",
    "        self.update_kl(lps_g, base_lps_g, mask)\n",
    "         \n",
    "        pg_dict = { 'loss' : pg_loss.detach().cpu(),\n",
    "                    'pg_discounted' : discounted_rewards,\n",
    "                    'pg_advantage' : advantages,\n",
    "                    'ratios' : ratios.detach().cpu(),\n",
    "                    'ppo_loss' : loss.detach().cpu(),\n",
    "                    'v_loss' : v_loss.detach().cpu(),\n",
    "                    'entropy' : entropy.detach().cpu()}\n",
    "        \n",
    "        self.last_outputs = pg_dict\n",
    "        \n",
    "        return pg_loss, pg_dict\n",
    "    \n",
    "    def from_batch_state(self, batch_state):\n",
    "        lps_g = batch_state.model_gathered_logprobs\n",
    "        base_lps_g = batch_state.base_gathered_logprobs\n",
    "        lps = batch_state.model_logprobs\n",
    "        \n",
    "        mask = batch_state.mask\n",
    "        rewards = batch_state.rewards_final\n",
    "        \n",
    "        if self.scale_rewards:\n",
    "            if self.mean_reward is None:\n",
    "                self.mean_reward = rewards.mean()\n",
    "            else:\n",
    "                self.mean_reward = (1-self.gamma)*rewards.mean() + self.gamma*self.mean_reward\n",
    "                \n",
    "            rewards = rewards - self.mean_reward\n",
    "            \n",
    "        traj_rewards = batch_state.trajectory_rewards\n",
    "        \n",
    "        values = batch_state.state_values\n",
    "        ref_values = batch_state.ref_state_values\n",
    "        \n",
    "        loss, pg_dict = self(lps_g, base_lps_g, lps, mask, rewards, \n",
    "                             values, ref_values, traj_rewards)\n",
    "        \n",
    "        return loss\n",
    "            \n",
    "    def compute_kl_reward(self, lps_g, base_lps_g):\n",
    "        kl = lps_g - base_lps_g\n",
    "        kl_reward = -self.kl_coef * kl.detach()\n",
    "        return kl_reward\n",
    "    \n",
    "    def value_loss(self, values, old_values, rewards):\n",
    "        if values is None:\n",
    "            v_loss = to_device(torch.tensor(0.))\n",
    "        else:\n",
    "            \n",
    "            v_loss = F.mse_loss(values, rewards, reduction='none')\n",
    "            \n",
    "            if old_values is not None:\n",
    "                min_v = old_values - self.v_cliprange\n",
    "                max_v = old_values + self.v_cliprange\n",
    "                \n",
    "                values_clipped = torch.max(torch.min(values, max_v), min_v)\n",
    "                v_loss2 = F.mse_loss(values_clipped, rewards, reduction='none')\n",
    "                \n",
    "                v_loss = torch.max(v_loss, v_loss2)\n",
    "            \n",
    "            v_loss = self.v_coef*v_loss\n",
    "            \n",
    "        return v_loss\n",
    "    \n",
    "    def compute_advantages(self, rewards, values):\n",
    "        \n",
    "        if values is None:\n",
    "            advantages = rewards\n",
    "        else:\n",
    "            advantages = compute_advantages(rewards, values.detach(), self.gamma, self.lam)\n",
    "            \n",
    "        return advantages\n",
    "    \n",
    "    def update_kl(self, lps_g, base_lps_g, mask):\n",
    "        \n",
    "        if (self.kl_target is not None) and (self.kl_horizon is not None):\n",
    "            kl = (lps_g - base_lps_g).detach()\n",
    "            kl = (kl*mask).sum(-1)/mask.sum(-1)\n",
    "            kl = kl.cpu().mean()\n",
    "            \n",
    "            error = torch.clip(kl/self.kl_target - 1, -0.2, 0.2)\n",
    "            factor = 1 + error * lps_g.shape[0]/self.kl_horizon\n",
    "            self.kl_coef *= factor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"PPO.__call__\" class=\"doc_header\"><code>PPO.__call__</code><a href=\"__main__.py#L47\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>PPO.__call__</code>(**`lps_g`**, **`base_lps_g`**, **`lps`**, **`mask`**, **`rewards`**, **`values`**, **`ref_values`**, **`traj_rewards`**=*`None`*)\n",
       "\n",
       "Inputs:\n",
       "\n",
       "- `lps_g torch.FloatTensor[bs, sl]`: model gathered log probabilities\n",
       "\n",
       "- `base_lps_g torch.FloatTensor[bs, sl]`: baseline model \n",
       "gathered log probabilities\n",
       "\n",
       "- `lps torch.FloatTensor[bs, sl, n_actions]`: model full log probabilities\n",
       "\n",
       "- `mask torch.BoolTensor[bs, sl]`: padding mask. `True` indicates \n",
       "values that will be kept, `False` indicates values that will be masked\n",
       "\n",
       "- `rewards torch.FloatTensor[bs]`: reward tensor (one reward per batch item)\n",
       "\n",
       "- `values torch.FloatTensor[bs, sl]`: state value predictions\n",
       "\n",
       "- `ref_values torch.FloatTensor[bs, sl]`: baseline state value predictions\n",
       "\n",
       "- `traj_rewards Optional[torch.FloatTensor[bs, sl]]`: optional tensor of \n",
       "trajectory rewards with one reward value per timestep"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(PPO.__call__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mrl)",
   "language": "python",
   "name": "mrl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
