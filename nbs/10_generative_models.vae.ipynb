{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp g_models.vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Model\n",
    "\n",
    "> VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from mrl.imports import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *\n",
    "from mrl.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "class VAE(Encoder_Decoder):\n",
    "    def __init__(self, encoder, decoder, prior=None, bos_idx=0):\n",
    "        transition = VAE_Transition(encoder.d_latent)\n",
    "        super().__init__(encoder, decoder, transition, prior)\n",
    "            \n",
    "        self.bos_idx = bos_idx\n",
    "        \n",
    "    def forward(self, x, decoder_input=None):\n",
    "        \n",
    "        z = self.encoder(x)\n",
    "        z, kl_loss = self.transition(z)\n",
    "            \n",
    "        if decoder_input is None:\n",
    "            decoder_input = x\n",
    "            \n",
    "        output, hiddens, encoded = self.decoder(decoder_input, z)\n",
    "        return output, kl_loss\n",
    "    \n",
    "    def encode(self, x, decoder_input=None):\n",
    "        z = self.encoder(x)\n",
    "        z, kl_loss = self.transition(z)\n",
    "            \n",
    "        if decoder_input is None:\n",
    "            decoder_input = x\n",
    "            \n",
    "        output, hiddens, encoded = self.decoder(decoder_input, z)\n",
    "        return encoded\n",
    "    \n",
    "    def to_latent(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z, kl_loss = self.transition(z)\n",
    "        return z\n",
    "\n",
    "    def sample(self, bs, sl, z=None, temperature=1., multinomial=True, z_scale=1.):\n",
    "        \n",
    "        preds = idxs = to_device(torch.tensor([self.bos_idx]*bs).long().unsqueeze(-1))\n",
    "        lps = []\n",
    "        \n",
    "        if z is None:\n",
    "            if self.prior is not None:\n",
    "                z = to_device(self.prior.rsample([bs]))\n",
    "            else:\n",
    "                prior = Normal(torch.zeros((self.encoder.d_latent)),\n",
    "                               torch.ones((self.encoder.d_latent)))\n",
    "                z = to_device(self.prior.rsample([bs]))\n",
    "        \n",
    "        hiddens = None\n",
    "        \n",
    "        for i in range(sl):\n",
    "            x, hiddens, encoded = self.decoder(idxs, z, hiddens)\n",
    "            x.div_(temperature)\n",
    "            \n",
    "            idxs, lp = x_to_preds(x, multinomial=multinomial)\n",
    "            \n",
    "            lps.append(lp)            \n",
    "            preds = torch.cat([preds, idxs], -1)\n",
    "            \n",
    "        return preds[:, 1:], torch.cat(lps,-1)\n",
    "    \n",
    "    def sample_no_grad(self, bs, sl, z=None, temperature=1., multinomial=True):\n",
    "        with torch.no_grad():\n",
    "            return self.sample(bs, sl, z=z, temperature=temperature, multinomial=multinomial)\n",
    "        \n",
    "        \n",
    "    def get_rl_tensors(self, x, y, temperature=1., latent=None):\n",
    "        \n",
    "        if type(x) == list:\n",
    "            if latent is None:\n",
    "                latent = self.encoder(x[0])\n",
    "                z,_ = self.transition(latent)\n",
    "            else:\n",
    "                z = latent\n",
    "            output, hiddens, encoded = self.decoder(x[1], z)\n",
    "        else:\n",
    "            if latent is None:\n",
    "                latent = self.encoder(x)\n",
    "                z,_ = self.transition(latent)\n",
    "            else:\n",
    "                z = latent\n",
    "            output, hiddens, encoded = self.decoder(x, z)\n",
    "        \n",
    "        output.div_(temperature)\n",
    "        lps = F.log_softmax(output, -1)\n",
    "        \n",
    "        if self.prior.trainable:\n",
    "            prior_lps = self.prior.log_prob(z)\n",
    "            prior_lps = prior_lps.mean(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "            pass_through = torch.zeros(prior_lps.shape).float().to(prior_lps.device)\n",
    "            pass_through = pass_through + prior_lps - prior_lps.detach() # add to gradient chain\n",
    "            lps = lps + pass_through\n",
    "            \n",
    "        lps_gathered = gather_lps(lps, y)\n",
    "        return output, lps, lps_gathered, encoded\n",
    "            \n",
    "    def set_prior_from_stats(self, mu, logvar, trainable=False):\n",
    "        mu = mu.detach()\n",
    "        logvar = logvar.detach()\n",
    "        self.prior = NormalPrior(mu, logvar, trainable)\n",
    "        \n",
    "    def set_prior_from_latent(self, z, trainable=False):\n",
    "        mu, logvar = self.transition.get_stats(z)\n",
    "        self.set_prior_from_stats(mu, logvar, trainable)\n",
    "        \n",
    "    def set_prior_from_encoder(self, x, trainable=False):\n",
    "        assert x.shape[0]==1, \"Must set prior from a single input\"\n",
    "        z = self.encoder(x)\n",
    "        z = z.squeeze(0)\n",
    "        self.set_prior_from_latent(z, trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LSTM_Encoder(32, 64, 128, 2, 128)\n",
    "decoder = Conditional_LSTM_Block(32, 64, 128, 64, 128, 2,\n",
    "                                condition_hidden=True, condition_output=True)\n",
    "vae = VAE(encoder, decoder)\n",
    "\n",
    "ints = torch.randint(0, 31, (8, 10))\n",
    "x = ints[:,:-1]\n",
    "y = ints[:,1:]\n",
    "\n",
    "_ = vae(x)\n",
    "\n",
    "decoder = Conditional_LSTM_Block(32, 64, 128, 64, 128, 2,\n",
    "                                condition_hidden=False, condition_output=True)\n",
    "vae = VAE(encoder, decoder)\n",
    "\n",
    "_ = vae(x)\n",
    "\n",
    "decoder = Conditional_LSTM_Block(32, 64, 128, 64, 128, 2,\n",
    "                                condition_hidden=True, condition_output=False)\n",
    "vae = VAE(encoder, decoder)\n",
    "\n",
    "_ = vae(x)\n",
    "\n",
    "_ = vae.sample(8, 16)\n",
    "\n",
    "z = vae.prior.sample([8])\n",
    "_ = vae.sample(8, 16, z=z)\n",
    "\n",
    "o,lp,lpg,e = vae.get_rl_tensors(x,y)\n",
    "\n",
    "vae.set_prior_from_encoder(x[0].unsqueeze(0), trainable=True);\n",
    "\n",
    "o,lp,lpg,e = vae.get_rl_tensors(x,y)\n",
    "loss = lpg.mean()\n",
    "assert vae.prior.loc.grad is None\n",
    "loss.backward()\n",
    "assert vae.prior.loc.grad is not None\n",
    "\n",
    "latent = torch.randn((x.shape[0], 128))\n",
    "o,lp,lpg,e = vae.get_rl_tensors(x,y,latent=latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LSTM_VAE(VAE):\n",
    "    def __init__(self, d_vocab, d_embedding, d_hidden, n_layers, d_latent,\n",
    "                enc_drop=0., dec_drop=0., condition_hidden=True, condition_output=True,\n",
    "                prior=None, bos_idx=0):\n",
    "        \n",
    "        encoder = LSTM_Encoder(d_vocab, d_embedding, d_hidden, \n",
    "                               n_layers, d_latent, dropout=enc_drop)\n",
    "        \n",
    "        decoder = Conditional_LSTM_Block(d_vocab, d_embedding, d_hidden, d_embedding,\n",
    "                                d_latent, n_layers, lstm_drop=dec_drop, lin_drop=dec_drop, \n",
    "                                condition_hidden=condition_hidden, condition_output=condition_output)\n",
    "        \n",
    "        super().__init__(encoder, decoder, prior, bos_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = LSTM_VAE(32, 64, 128, 2, 128, condition_hidden=True, condition_output=True)\n",
    "\n",
    "ints = torch.randint(0, 31, (8, 10))\n",
    "x = ints[:,:-1]\n",
    "y = ints[:,1:]\n",
    "\n",
    "_ = vae(x)\n",
    "\n",
    "_ = vae.sample(8, 16)\n",
    "\n",
    "z = vae.prior.sample([8])\n",
    "_ = vae.sample(8, 16, z=z)\n",
    "\n",
    "_ = vae.get_rl_tensors(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Conv_VAE(VAE):\n",
    "    def __init__(self, d_vocab, d_embedding, conv_filters, kernel_sizes, strides, conv_drops,\n",
    "                 d_hidden, n_layers, d_latent, dec_drop=0., \n",
    "                 condition_hidden=True, condition_output=True,\n",
    "                 prior=None, bos_idx=0):\n",
    "        \n",
    "        encoder = Conv_Encoder(d_vocab, d_embedding, d_latent, \n",
    "                               conv_filters, kernel_sizes, strides, conv_drops)\n",
    "        \n",
    "        decoder = Conditional_LSTM_Block(d_vocab, d_embedding, d_hidden, d_embedding,\n",
    "                                d_latent, n_layers, lstm_drop=dec_drop, lin_drop=dec_drop, \n",
    "                                condition_hidden=condition_hidden, condition_output=condition_output)\n",
    "        \n",
    "        super().__init__(encoder, decoder, prior, bos_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = Conv_VAE(32, 64, [128, 256], [7,7], [2,2], [0.1,0.1], 128, 2, 128, \n",
    "               condition_hidden=False, condition_output=True)\n",
    "\n",
    "ints = torch.randint(0, 31, (8, 10))\n",
    "x = ints[:,:-1]\n",
    "y = ints[:,1:]\n",
    "\n",
    "_ = vae(x)\n",
    "\n",
    "_ = vae.sample(8, 16)\n",
    "\n",
    "z = vae.prior.sample([8])\n",
    "_ = vae.sample(8, 16, z=z)\n",
    "\n",
    "_ = vae.get_rl_tensors(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "        \n",
    "class MLP_VAE(VAE):\n",
    "    def __init__(self, d_vocab, d_embedding, encoder_d_in, encoder_dims, encoder_drops,\n",
    "                 d_hidden, n_layers, d_latent, dec_drop=0., \n",
    "                 condition_hidden=True, condition_output=True,\n",
    "                 prior=None, bos_idx=0):\n",
    "        \n",
    "        encoder = MLP_Encoder(encoder_d_in, encoder_dims, d_latent, encoder_drops)\n",
    "        \n",
    "        decoder = Conditional_LSTM_Block(d_vocab, d_embedding, d_hidden, d_embedding,\n",
    "                                d_latent, n_layers, lstm_drop=dec_drop, lin_drop=dec_drop, \n",
    "                                condition_hidden=condition_hidden, condition_output=condition_output)\n",
    "        \n",
    "        super().__init__(encoder, decoder, prior, bos_idx)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = MLP_VAE(32, 64, 128, [64, 32], [0.1, 0.1], 128, 2, 128, \n",
    "               condition_hidden=False, condition_output=True)\n",
    "\n",
    "ints = torch.randint(0, 31, (8, 10))\n",
    "x = ints[:,:-1]\n",
    "y = ints[:,1:]\n",
    "\n",
    "condition = torch.randn((8,128))\n",
    "\n",
    "\n",
    "_ = vae(condition, x)\n",
    "\n",
    "_ = vae.sample(8, 16)\n",
    "\n",
    "z = vae.prior.sample([8])\n",
    "_ = vae.sample(8, 16, z=z)\n",
    "\n",
    "_ = vae.get_rl_tensors([condition,x],y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
