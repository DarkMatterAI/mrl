{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "\n",
    "> Pytorch model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from mrl.imports import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LinearBlock(nn.Module):\n",
    "    '''\n",
    "    LinearBlock - Combined linear, batchnorm, ReLU and dropout. \n",
    "    Layers are executed in the order linear, batchnorm, ReLU, dropout. \n",
    "    Batchnorm, activation and dropout layers are optional\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `d_in int`: number of input dimensions\n",
    "        \n",
    "    - `d_out int`: number of output dimensions\n",
    "        \n",
    "    - `act bool`: if True, applies a ReLU activation\n",
    "        \n",
    "    - `bn bool`: if True, applies 1d batchnorm\n",
    "        \n",
    "    - `dropout float`: dropout percentage\n",
    "        \n",
    "    - `**lin_kwargs dict`: keyword args passed to nn.Linear\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, d_in, d_out, act=True, bn=False, dropout=0., **lin_kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = [nn.Linear(d_in, d_out, **lin_kwargs)]\n",
    "        \n",
    "        if bn:\n",
    "            layers.append(nn.BatchNorm1d(d_out))\n",
    "            \n",
    "        if act:\n",
    "            layers.append(nn.ReLU())\n",
    "            \n",
    "        if dropout>0.:\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = LinearBlock(128, 64, bn=True, dropout=0.5)\n",
    "_ = layer(torch.randn(16,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class ValueHead(nn.Module):\n",
    "    '''\n",
    "    ValueHead - used in RL algorithms to predict state values\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `d_in int`: number of input dimensions\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, d_in, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.layer = nn.Linear(d_in, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer(self.drop(x)).squeeze(-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    '''\n",
    "    Conv - base module for convolutions\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `d_in int`: number of input dimensions\n",
    "\n",
    "    - `d_out int`: number of output dimensions\n",
    "\n",
    "    - `ks int`: kernel size\n",
    "\n",
    "    - `stride int`: stride\n",
    "\n",
    "    - `padding [int, None]`: padding. If None, derived from kernel size\n",
    "\n",
    "    - `ndim int`: conv dimension (1D conv, 2D conv, 3D conv)\n",
    "\n",
    "    - `act bool`: if True, applies a ReLU activation\n",
    "\n",
    "    - `bn bool`: if True, applies batchnorm consistent with `ndim`\n",
    "\n",
    "    - `dropout float`: dropout percentage\n",
    "\n",
    "    - `**conv_kwargs dict`: keyword args passed to nn.Conv\n",
    "    '''\n",
    "    def __init__(self, d_in, d_out, ks=3, stride=1, padding=None, ndim=2, \n",
    "                 act=True, bn=False, dropout=0., **conv_kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        if padding is None:\n",
    "            padding = (ks-1)//2\n",
    "            \n",
    "        if ndim==1:\n",
    "            conv_func = nn.Conv1d\n",
    "            bn_func = nn.BatchNorm1d\n",
    "        elif ndim==2:\n",
    "            conv_func = nn.Conv2d\n",
    "            bn_func = nn.BatchNorm2d\n",
    "        else:\n",
    "            conv_func = nn.Conv3d\n",
    "            bn_func = nn.BatchNorm3d\n",
    "        \n",
    "        layers = [conv_func(d_in, d_out, ks, stride, padding=padding, **conv_kwargs)]\n",
    "        \n",
    "        if bn:\n",
    "            layers.append(bn_func(d_out))\n",
    "            \n",
    "        if act:\n",
    "            layers.append(nn.ReLU())\n",
    "            \n",
    "        if dropout>0.:\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "class Conv1d(Conv):\n",
    "    '''\n",
    "    Conv1d - 1D convolution\n",
    "    \n",
    "    Inputs:\n",
    "        \n",
    "    - `d_in int`: number of input dimensions\n",
    "\n",
    "    - `d_out int`: number of output dimensions\n",
    "\n",
    "    - `ks int`: kernel size\n",
    "\n",
    "    - `stride int`: stride\n",
    "\n",
    "    - `padding [int, None]`: padding. If None, derived from kernel size\n",
    "\n",
    "    - `act bool`: if True, applies a ReLU activation\n",
    "\n",
    "    - `bn bool`: if True, applies batchnorm consistent with `ndim`\n",
    "\n",
    "    - `dropout float`: dropout percentage\n",
    "\n",
    "    - `**conv_kwargs dict`: keyword args passed to nn.Conv1D\n",
    "    '''\n",
    "    def __init__(self, d_in, d_out, ks=3, stride=1, padding=None, \n",
    "                 act=True, bn=False, dropout=0., **conv_kwargs):\n",
    "        super().__init__(d_in, d_out, ks, stride, padding, ndim=1, \n",
    "                 act=act, bn=bn, dropout=dropout, **conv_kwargs)\n",
    "        \n",
    "class Conv2d(Conv):\n",
    "    '''\n",
    "    Conv2d - 2D convolution\n",
    "    \n",
    "    Inputs:\n",
    "        \n",
    "    - `d_in int`: number of input dimensions\n",
    "\n",
    "    - `d_out int`: number of output dimensions\n",
    "\n",
    "    - `ks int`: kernel size\n",
    "\n",
    "    - `stride int`: stride\n",
    "\n",
    "    - `padding [int, None]`: padding. If None, derived from kernel size\n",
    "\n",
    "    - `act bool`: if True, applies a ReLU activation\n",
    "\n",
    "    - `bn bool`: if True, applies batchnorm consistent with `ndim`\n",
    "\n",
    "    - `dropout float`: dropout percentage\n",
    "\n",
    "    - `**conv_kwargs dict`: keyword args passed to nn.Conv2D\n",
    "    '''\n",
    "    def __init__(self, d_in, d_out, ks=3, stride=1, padding=None, \n",
    "                 act=True, bn=False, dropout=0., **conv_kwargs):\n",
    "        super().__init__(d_in, d_out, ks, stride, padding, ndim=2, \n",
    "                 act=act, bn=bn, dropout=dropout, **conv_kwargs)\n",
    "        \n",
    "class Conv3d(Conv):\n",
    "    '''\n",
    "    Conv3d - 3D convolution\n",
    "    \n",
    "    Inputs:\n",
    "        \n",
    "    - `d_in int`: number of input dimensions\n",
    "\n",
    "    - `d_out int`: number of output dimensions\n",
    "\n",
    "    - `ks int`: kernel size\n",
    "\n",
    "    - `stride int`: stride\n",
    "\n",
    "    - `padding [int, None]`: padding. If None, derived from kernel size\n",
    "\n",
    "    - `act bool`: if True, applies a ReLU activation\n",
    "\n",
    "    - `bn bool`: if True, applies batchnorm consistent with `ndim`\n",
    "\n",
    "    - `dropout float`: dropout percentage\n",
    "\n",
    "    - `**conv_kwargs dict`: keyword args passed to nn.Conv3D\n",
    "    '''\n",
    "    def __init__(self, d_in, d_out, ks=3, stride=1, padding=None, \n",
    "                 act=True, bn=False, dropout=0., **conv_kwargs):\n",
    "        super().__init__(d_in, d_out, ks, stride, padding, ndim=3, \n",
    "                 act=act, bn=bn, dropout=dropout, **conv_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class SphericalDistribution(torch.distributions.Distribution):\n",
    "    '''\n",
    "    SphericalDistribution - samples from points on the surface of a sphere\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `loc torch.Tensor`: vector of means\n",
    "        \n",
    "    - `scale torch.Tensor`: vector of variances\n",
    "    '''\n",
    "    def __init__(self, loc, scale, validate_args=False):\n",
    "        super().__init__(loc.shape[0], validate_args=validate_args)\n",
    "        self.dim = loc.shape[0]\n",
    "        self.loc = loc\n",
    "        self.scale = scale\n",
    "        self.dist = Normal(self.loc, self.scale)\n",
    "        \n",
    "    def sample(self, n):\n",
    "        s = self.dist.sample(n)\n",
    "        s = F.normalize(s, p=2, dim=-1)\n",
    "        return s\n",
    "    \n",
    "    def rsample(self, n):\n",
    "        s = self.dist.rsample(n)\n",
    "        s = F.normalize(s, p=2, dim=-1)\n",
    "        return s\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Spherical(loc: {self.loc.size()}, scale: {self.scale.size()})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Prior(nn.Module):\n",
    "    '''\n",
    "    Prior - base class for trainable priors\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def get_dist(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def log_prob(self, x):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def sample(self, n):\n",
    "        if type(n)==int:\n",
    "            n = [n]\n",
    "        return self.get_dist().sample(n)\n",
    "    \n",
    "    def rsample(self, n):\n",
    "        if type(n)==int:\n",
    "            n = [n]\n",
    "        return self.get_dist().rsample(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class NormalPrior(Prior):\n",
    "    '''\n",
    "    NormalPrior - normal distribution prior\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `loc torch.Tensor`: vector of means\n",
    "        \n",
    "    - `log_scale torch.Tensor`: vector of log-variances\n",
    "        \n",
    "    - `trainable bool`: if True, `loc` and `scale` are trainable\n",
    "        \n",
    "    Note that log-variances are used for stability. Optimizing \n",
    "    the variance directly can cause issues with gradient descent \n",
    "    sending variance values negative\n",
    "    '''\n",
    "    def __init__(self, loc, log_scale, trainable=True):\n",
    "        super().__init__()\n",
    "        if trainable:\n",
    "            loc = nn.Parameter(loc)\n",
    "            log_scale = nn.Parameter(log_scale)\n",
    "            self.loc = loc\n",
    "            self.log_scale = log_scale\n",
    "        else:\n",
    "            self.register_buffer('loc', loc)\n",
    "            self.register_buffer('log_scale', log_scale)\n",
    "            \n",
    "        self.trainable = trainable\n",
    "        \n",
    "    def get_dist(self):\n",
    "        return Normal(self.loc, self.log_scale.exp())\n",
    "    \n",
    "    def log_prob(self, x):\n",
    "        var = self.log_scale.exp().pow(2)\n",
    "        return -((x - self.loc) ** 2) / (2 * var) - self.log_scale - math.log(math.sqrt(2 * math.pi))\n",
    "    \n",
    "class SphericalPrior(NormalPrior):\n",
    "    '''\n",
    "    SphericalPrior - spherical distribution prior\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `loc torch.Tensor`: vector of means\n",
    "        \n",
    "    - `log_scale torch.Tensor`: vector of log-variances\n",
    "        \n",
    "    - `trainable bool`: if True, `loc` and `scale` are trainable\n",
    "        \n",
    "    Note that log-variances are used for stability. Optimizing \n",
    "    the variance directly can cause issues with gradient descent \n",
    "    sending variance values negative\n",
    "    '''\n",
    "    def __init__(self, loc, log_scale, trainable=True):\n",
    "        super().__init__(loc, log_scale, trainable)\n",
    "        \n",
    "    def get_dist(self):\n",
    "        return SphericalDistribution(self.loc, self.log_scale.exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = NormalPrior(torch.zeros((64,)), torch.zeros((64,)), trainable=True)\n",
    "assert p.rsample(5).requires_grad\n",
    "assert not p.sample(5).requires_grad\n",
    "\n",
    "p = SphericalPrior(torch.zeros((2,)), torch.zeros((2,)), trainable=False)\n",
    "assert not p.rsample(5).requires_grad\n",
    "assert not p.sample(5).requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class SequenceDropout(nn.Module):\n",
    "    '''\n",
    "    SequenceDropout - dropout along the sequence dimension\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `p float`: dropout probability\n",
    "        \n",
    "    - `batch_first bool`: if batch dimension is first in input tensors\n",
    "        \n",
    "    Samples a dropout mask that is constant in the sequence dimension\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, p, batch_first=True):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.batch_first = batch_first\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.training and self.p>0:\n",
    "            \n",
    "            if self.batch_first:\n",
    "                sizes = (x.shape[0], 1, *x.shape[2:])\n",
    "            else:\n",
    "                sizes = (1, x.shape[0], *x.shape[2:])\n",
    "            \n",
    "            mask = x.new_empty(*sizes).bernoulli_(1-self.p).bool()\n",
    "            x = x*mask\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Conditional_LSTM(nn.Module):\n",
    "    '''\n",
    "    Conditional_LSTM - Conditional LSTM module\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `d_embedding int`: embedding dimension\n",
    "\n",
    "    - `d_hidden int`: hidden dimension\n",
    "\n",
    "    - `d_output int`: output dimension\n",
    "\n",
    "    - `d_latent int`: latent vector dimension\n",
    "\n",
    "    - `n_layers int`: number of layers\n",
    "\n",
    "    - `condition_hidden bool`: if True, latent vector is used to initialize the \n",
    "    hidden state\n",
    "\n",
    "    - `condition_output bool`: if True, latent vector is concatenated to inputs\n",
    "\n",
    "    - `bidir bool`: if the LSTM should be bidirectional\n",
    "\n",
    "    - `input_dropout float`: dropout percentage on inputs\n",
    "\n",
    "    - `lstm_dropout float`: dropout on LSTM layers\n",
    "\n",
    "    - `batch_first bool`: if batch dimension is first on input tensors\n",
    "    '''\n",
    "    def __init__(self, d_embedding, d_hidden, d_output, d_latent, n_layers,\n",
    "                 condition_hidden=True, condition_output=True,\n",
    "                 bidir=False, input_dropout=0., lstm_dropout=0., batch_first=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_embedding = d_embedding\n",
    "        self.d_hidden = d_hidden\n",
    "        self.d_output = d_output\n",
    "        self.n_layers = n_layers\n",
    "        self.bidir = bidir\n",
    "        self.n_dir = 1 if not bidir else 2\n",
    "        self.batch_first = batch_first\n",
    "        self.condition_hidden = condition_hidden\n",
    "        self.condition_output = condition_output\n",
    "\n",
    "        self.input_drop = SequenceDropout(input_dropout)\n",
    "        self.lstm_drop = SequenceDropout(lstm_dropout)\n",
    "\n",
    "        self.lstms = []\n",
    "        self.hidden_sizes = []\n",
    "\n",
    "        for l in range(n_layers):\n",
    "            if l==0:\n",
    "                input_size = d_embedding if not self.condition_output else d_embedding+d_latent\n",
    "            else:\n",
    "                input_size = d_hidden\n",
    "\n",
    "            output_size = d_output if l==n_layers-1 else d_hidden\n",
    "            output_size = output_size // self.n_dir\n",
    "\n",
    "            hidden_size = (self.n_dir, 1, output_size)\n",
    "            self.hidden_sizes.append(hidden_size)\n",
    "\n",
    "            lstm = nn.LSTM(input_size, output_size, 1, batch_first=batch_first,\n",
    "                           bidirectional=bidir)\n",
    "            self.lstms.append(lstm)\n",
    "\n",
    "        self.lstms = nn.ModuleList(self.lstms)\n",
    "\n",
    "        if self.condition_hidden:\n",
    "            to_hidden = []\n",
    "            for size in self.hidden_sizes:\n",
    "                ndir, _, dim = size\n",
    "                to_hidden.append(nn.Linear(d_latent, ndir*dim*2))\n",
    "\n",
    "            self.to_hidden = nn.ModuleList(to_hidden)\n",
    "\n",
    "    def forward(self, x, z, hiddens=None):\n",
    "        '''\n",
    "        Inputs:\n",
    "        \n",
    "        - `x torch.Tensor[(bs, sl, d_embedding) or (sl, bs, d_embedding)]`: input tensor\n",
    "            \n",
    "        - `z torch.Tensor[bs, d_latent]`: latent vector\n",
    "            \n",
    "        - `hiddens Optional[list[(hidden_state, cell_state)]]`: hidden state. If None, \n",
    "        a new hidden state is initialized\n",
    "        \n",
    "        '''\n",
    "\n",
    "        x = self.prepare_x(x, z)\n",
    "        x = self.input_drop(x)\n",
    "        bs = x.shape[0] if self.batch_first else x.shape[1]\n",
    "\n",
    "        hiddens = self.get_hidden(z, hiddens, bs)\n",
    "        hiddens = to_device(hiddens, x.device)\n",
    "\n",
    "        new_hiddens = []\n",
    "        for i, lstm in enumerate(self.lstms):\n",
    "            x, (h,c) = lstm(x, hiddens[i])\n",
    "            new_hiddens.append((h.detach(), c.detach()))\n",
    "            if i<self.n_layers:\n",
    "                x = self.lstm_drop(x)\n",
    "\n",
    "        return x, new_hiddens\n",
    "\n",
    "    def prepare_x(self, x, z):\n",
    "        '''\n",
    "        Concatenate x to latent vector if `self.condition_output`\n",
    "        '''\n",
    "        if self.condition_output:\n",
    "            if self.batch_first:\n",
    "                sl = x.shape[1]\n",
    "                z_ = z.unsqueeze(1).repeat(1,sl,1)\n",
    "            else:\n",
    "                sl = x.shape[0]\n",
    "                z_ = z.unsqueeze(0).repeat(sl,1,1)\n",
    "\n",
    "            x = torch.cat([x, z_], -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_hidden(self, z, hiddens, bs):\n",
    "        '''\n",
    "        Initializes hidden state.\n",
    "        '''\n",
    "        if hiddens is None:\n",
    "            if self.condition_hidden:\n",
    "                hiddens = self.latent_to_hidden(z)\n",
    "\n",
    "            else:\n",
    "                hiddens = self.get_new_hidden(bs)\n",
    "\n",
    "        return hiddens\n",
    "\n",
    "    def latent_to_hidden(self, z):\n",
    "        '''\n",
    "        converts latent vector `z` into new hidden state\n",
    "        '''\n",
    "        hiddens = []\n",
    "        for layer in self.to_hidden:\n",
    "            h = layer(z)\n",
    "            h,c = torch.chunk(h, 2, dim=-1)\n",
    "            bs, _ = h.shape\n",
    "            h = h.contiguous().reshape(bs, self.n_dir, -1).permute(1,0,2)\n",
    "            c = c.contiguous().reshape(bs, self.n_dir, -1).permute(1,0,2)\n",
    "            hiddens.append((h,c))\n",
    "\n",
    "        return hiddens\n",
    "\n",
    "    def get_new_hidden(self, bs):\n",
    "        '''\n",
    "        initializes new zeroed hidden states\n",
    "        '''\n",
    "        hiddens = []\n",
    "        for hs in self.hidden_sizes:\n",
    "            h = torch.zeros(hs).repeat(1,bs,1)\n",
    "            c = torch.zeros(hs).repeat(1,bs,1)\n",
    "            hiddens.append((h,c))\n",
    "\n",
    "        return hiddens\n",
    "\n",
    "    def mixup_hiddens(self, hiddens):\n",
    "        '''\n",
    "        shuffles hidden states between layers for style mixing\n",
    "        '''\n",
    "        new_hiddens = []\n",
    "        for item in hiddens:\n",
    "            h,c = item\n",
    "            shuffle = to_device(torch.randperm(h.shape[1]))\n",
    "            h = h[:,shuffle]\n",
    "            c = c[:,shuffle]\n",
    "            new_hiddens.append((h,c))\n",
    "        return new_hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_embedding=64\n",
    "d_hidden=128\n",
    "d_latent = 32\n",
    "n_layers = 2\n",
    "\n",
    "l1 = Conditional_LSTM(d_embedding, d_hidden, d_embedding, d_latent, n_layers,\n",
    "                     condition_hidden=True, condition_output=True, \n",
    "                     bidir=False, batch_first=True)\n",
    "\n",
    "l2 = Conditional_LSTM(d_embedding, d_hidden, d_embedding, d_latent, n_layers,\n",
    "                     condition_hidden=True, condition_output=True, \n",
    "                     bidir=True, batch_first=True)\n",
    "\n",
    "l3 = Conditional_LSTM(d_embedding, d_hidden, d_embedding, d_latent, n_layers,\n",
    "                     condition_hidden=False, condition_output=True, \n",
    "                     bidir=False, batch_first=True)\n",
    "\n",
    "l4 = Conditional_LSTM(d_embedding, d_hidden, d_embedding, d_latent, n_layers,\n",
    "                     condition_hidden=True, condition_output=False, \n",
    "                     bidir=False, batch_first=True)\n",
    "\n",
    "l5 = Conditional_LSTM(d_embedding, d_hidden, d_embedding, d_latent, n_layers,\n",
    "                     condition_hidden=False, condition_output=False, \n",
    "                     bidir=True, batch_first=True, input_dropout=0.5, lstm_dropout=0.5)\n",
    "\n",
    "bs = 12\n",
    "x = torch.randn((bs, 21, d_embedding))\n",
    "z = torch.randn((bs, d_latent))\n",
    "\n",
    "_ = l1(x,z)\n",
    "_ = l1(x,z, l1.latent_to_hidden(z))\n",
    "\n",
    "_ = l2(x,z)\n",
    "_ = l2(x,z, l2.latent_to_hidden(z))\n",
    "\n",
    "_ = l3(x,z)\n",
    "_ = l3(x,z, l3.get_new_hidden(bs))\n",
    "\n",
    "_ = l4(x,z)\n",
    "_ = l4(x,z, l4.get_new_hidden(bs))\n",
    "\n",
    "_ = l5(x,z)\n",
    "_ = l5(x,None)\n",
    "_ = l5(x,z, l5.get_new_hidden(bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l5.get_new_hidden(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(torch.Size([2, 5, 64]), torch.Size([2, 5, 64])),\n",
       " (torch.Size([2, 5, 32]), torch.Size([2, 5, 32]))]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i[0].shape, i[1].shape) for i in l5.get_new_hidden(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "        \n",
    "class LSTM(Conditional_LSTM):\n",
    "    '''\n",
    "    LSTM - LSTM module\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `d_embedding int`: embedding dimension\n",
    "\n",
    "    - `d_hidden int`: hidden dimension\n",
    "\n",
    "    - `d_output int`: output dimension\n",
    "\n",
    "    - `n_layers int`: number of layers\n",
    "\n",
    "    - `bidir bool`: if the LSTM should be bidirectional\n",
    "\n",
    "    - `input_dropout float`: dropout percentage on inputs\n",
    "\n",
    "    - `lstm_dropout float`: dropout on LSTM layers\n",
    "\n",
    "    - `batch_first bool`: if batch dimension is first on input tensors\n",
    "    '''\n",
    "    def __init__(self, d_embedding, d_hidden, d_output, n_layers, \n",
    "                 bidir=False, input_dropout=0., lstm_dropout=0., \n",
    "                 batch_first=True):\n",
    "        \n",
    "        super().__init__(d_embedding, d_hidden, d_output, 0, n_layers,\n",
    "                 condition_hidden=False, condition_output=False,\n",
    "                 bidir=bidir, input_dropout=input_dropout, \n",
    "                 lstm_dropout=lstm_dropout, batch_first=batch_first)\n",
    "\n",
    "    def forward(self, x, hiddens=None):\n",
    "        '''\n",
    "        Inputs:\n",
    "        \n",
    "        - `x torch.Tensor[(bs, sl, d_embedding) or (sl, bs, d_embedding)]:` input tensor\n",
    "                        \n",
    "        - `hiddens Optional[list[(hidden_state, cell_state)]]`: hidden state. If None, \n",
    "        a new hidden state is initialized\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        x, new_hiddens = super().forward(x, None, hiddens)\n",
    "            \n",
    "        return x, new_hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = LSTM(d_embedding, d_hidden, d_embedding, n_layers, bidir=False, batch_first=True)\n",
    "\n",
    "l2 = LSTM(d_embedding, d_hidden, d_embedding, n_layers, bidir=True, batch_first=True,\n",
    "         input_dropout=0.5, lstm_dropout=0.5)\n",
    "\n",
    "_ = l1(x)\n",
    "_ = l1(x, l1.get_new_hidden(bs))\n",
    "\n",
    "_ = l2(x)\n",
    "_ = l2(x, l2.get_new_hidden(bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Conditional_LSTM_Block(nn.Module):\n",
    "    '''\n",
    "    Conditional_LSTM_Block - combines Embedding, Conditional LSTM, \n",
    "    and output layer\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `d_vocab int`: vocab size\n",
    "    \n",
    "    - `d_embedding int`: embedding dimension\n",
    "\n",
    "    - `d_hidden int`: hidden dimension\n",
    "\n",
    "    - `d_output int`: output dimension\n",
    "\n",
    "    - `d_latent int`: latent vector dimension\n",
    "\n",
    "    - `n_layers int`: number of layers\n",
    "    \n",
    "    - `input_dropout float`: dropout percentage on inputs\n",
    "\n",
    "    - `lstm_dropout float`: dropout on LSTM layers\n",
    "    \n",
    "    - `bidir bool`: if the LSTM should be bidirectional\n",
    "    \n",
    "    - `condition_hidden bool`: if True, latent vector is used to initialize the\n",
    "    hidden state\n",
    "\n",
    "    - `condition_output bool`: if True, latent vector is concatenated to inputs\n",
    "        \n",
    "    - `forward_rollout bool`: if model should generate outputs through rollout \n",
    "    with teacher forcing\n",
    "        \n",
    "    - `p_force float`: teacher forcing frequency\n",
    "        \n",
    "    - `p_force_decay float`: teacher forcing decay rate\n",
    "    '''\n",
    "    def __init__(self, d_vocab, d_embedding, d_hidden, d_output, d_latent, n_layers,\n",
    "                 input_dropout=0., lstm_dropout=0., bidir=False,\n",
    "                 condition_hidden=True, condition_output=False,\n",
    "                 forward_rollout=False, p_force=1., p_force_decay=1.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(d_vocab, d_embedding)\n",
    "        self.lstm = Conditional_LSTM(d_embedding, d_hidden, d_output, d_latent, n_layers,\n",
    "                                    condition_hidden=condition_hidden, condition_output=condition_output,\n",
    "                                     bidir=bidir, input_dropout=input_dropout, lstm_dropout=lstm_dropout)\n",
    "        \n",
    "        self.head = nn.Linear(d_output, d_vocab)\n",
    "        self.forward_rollout = forward_rollout\n",
    "        self.p_force = p_force\n",
    "        self.p_force_decay = p_force_decay\n",
    "        \n",
    "    def forward(self, x, z, hiddens=None):\n",
    "        '''\n",
    "        Inputs:\n",
    "        \n",
    "        - `x torch.Tensor[(bs, sl, d_embedding) or (sl, bs, d_embedding)]`: input tensor\n",
    "            \n",
    "        - `z torch.Tensor[bs, d_latent]`: latent vector\n",
    "            \n",
    "        - `hiddens Optional[list[(hidden_state, cell_state)]]`: hidden state. If None, \n",
    "        a new hidden state is initialized\n",
    "        '''\n",
    "        if self.forward_rollout:\n",
    "            output, hiddens, encoded = self._forward_rollout(x,z,hiddens)\n",
    "        else:\n",
    "            output, hiddens, encoded = self._forward(x,z,hiddens)\n",
    "            \n",
    "        return output, hiddens, encoded \n",
    "        \n",
    "        \n",
    "    def _forward(self, x, z, hiddens=None):\n",
    "        x = self.embedding(x)\n",
    "        encoded, hiddens = self.lstm(x, z, hiddens)\n",
    "        output = self.head(encoded)\n",
    "        \n",
    "        return output, hiddens, encoded\n",
    "    \n",
    "    def _forward_rollout(self, x, z, hiddens=None):\n",
    "        '''\n",
    "        _forward_rollout - forward with self-rollout\n",
    "        \n",
    "        At each time step, the model generates a new output. With `p_force`, the \n",
    "        corret next step from the input is sent to the model, or with `1-p_force` \n",
    "        the model's last prediction is sent as the next step\n",
    "        '''\n",
    "        bs = x.shape[0] \n",
    "        sl = x.shape[1]\n",
    "        \n",
    "        idxs = x[:,0].unsqueeze(-1)\n",
    "        \n",
    "        output = []\n",
    "        encoded = []\n",
    "        \n",
    "        for i in range(1, sl):\n",
    "            output_iter, hiddens, encoded_iter = self._forward(idxs, z, hiddens=hiddens)\n",
    "            output.append(output_iter)\n",
    "            encoded.append(encoded_iter)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                idxs = F.softmax(output_iter, -1).argmax(-1)\n",
    "                \n",
    "            if np.random.random()<self.p_force:\n",
    "                idxs = x[:,i].unsqueeze(-1)\n",
    "                \n",
    "        output = torch.cat(output, 1)\n",
    "        encoded = torch.cat(encoded, 1)\n",
    "        self.p_force = self.p_force * self.force_decay\n",
    "        \n",
    "        return output, hiddens, encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LSTM_Block(nn.Module):\n",
    "    '''\n",
    "    LSTM_Block - combines Embedding, LSTM, and output layer\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `d_vocab int`: vocab size\n",
    "    \n",
    "    - `d_embedding int`: embedding dimension\n",
    "\n",
    "    - `d_hidden int`: hidden dimension\n",
    "\n",
    "    - `d_output int`: output dimension\n",
    "\n",
    "    - `n_layers int`: number of LSTM layers\n",
    "\n",
    "    - `input_dropout float`: dropout percentage on inputs\n",
    "\n",
    "    - `lstm_dropout float`: dropout on LSTM layers\n",
    "    \n",
    "    - `bidir bool`: if the LSTM should be bidirectional\n",
    "    '''\n",
    "    def __init__(self, d_vocab, d_embedding, d_hidden, d_output, n_layers,\n",
    "                 input_dropout=0., lstm_dropout=0., bidir=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(d_vocab, d_embedding)\n",
    "        self.lstm = LSTM(d_embedding, d_hidden, d_output, n_layers,\n",
    "                                     bidir=bidir, input_dropout=input_dropout, lstm_dropout=lstm_dropout)\n",
    "        \n",
    "        self.head = nn.Linear(d_output, d_vocab)\n",
    "        \n",
    "    def forward(self, x, hiddens=None):\n",
    "        '''\n",
    "        Inputs:\n",
    "        \n",
    "        - `x torch.Tensor[(bs, sl, d_embedding) or (sl, bs, d_embedding)]`: input tensor\n",
    "                        \n",
    "        - `hiddens Optional[list[(hidden_state, cell_state)]]`: hidden state. If None, \n",
    "        a new hidden state is initialized\n",
    "        \n",
    "        '''\n",
    "        x = self.embedding(x)\n",
    "        encoded, hiddens = self.lstm(x, hiddens)\n",
    "        output = self.head(encoded)\n",
    "        \n",
    "        return output, hiddens, encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    Base encoder module. All encoders have a \n",
    "    d_latent attribute which is referenced by other modules\n",
    "    '''\n",
    "    def __init__(self, d_latent):\n",
    "        super().__init__()\n",
    "        self.d_latent = d_latent\n",
    "\n",
    "class LSTM_Encoder(Encoder):\n",
    "    '''\n",
    "    LSTM_Encoder - LSTM-based encoder\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    \n",
    "    - `d_vocab int`: vocab size\n",
    "    \n",
    "    - `d_embedding int`: embedding dimension\n",
    "\n",
    "    - `d_hidden int`: hidden dimension\n",
    "\n",
    "    - `n_layers int`: number of LSTM layers\n",
    "    \n",
    "    - `d_latent int`: latent space dimension\n",
    "\n",
    "    - `input_dropout float`: dropout percentage on inputs\n",
    "\n",
    "    - `lstm_dropout float`: dropout on LSTM layers\n",
    "        \n",
    "    Generates latent vector from hidden states from the last LSTM layer\n",
    "    '''\n",
    "    def __init__(self, d_vocab, d_embedding, d_hidden, n_layers, d_latent, \n",
    "                 input_dropout=0., lstm_dropout=0.):\n",
    "        super().__init__(d_latent)\n",
    "        self.embedding = nn.Embedding(d_vocab, d_embedding)\n",
    "        self.lstm = LSTM(d_embedding, d_hidden, d_hidden, n_layers, \n",
    "                         bidir=True, batch_first=True, \n",
    "                         input_dropout=input_dropout, lstm_dropout=lstm_dropout)\n",
    "        self.head = nn.Linear(d_hidden*2, d_latent)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, hiddens = self.lstm(x)\n",
    "        hidden = torch.cat(list(torch.cat(hiddens[-1], -1)), -1) # concatenate hidden/cell states of last layer\n",
    "        latent = self.head(hidden)\n",
    "        return latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_latent = 128\n",
    "l = LSTM_Encoder(32, 64, 128, 2, 128, input_dropout=0.5, lstm_dropout=0.5)\n",
    "assert l(torch.randint(0,31, (10,15))).shape[-1] == d_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class MLP_Encoder(Encoder):\n",
    "    '''\n",
    "    MLP_Encoder - MLP-based encoder\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `d_in int`: number of input dimensions\n",
    "        \n",
    "    - `dims list[int]`: list of layer sizes ie `[1024, 512, 256]`\n",
    "        \n",
    "    - `dropouts list[float]`: list of dropout pobabilities ie `[0.2, 0.2, 0.3]`\n",
    "    '''\n",
    "    def __init__(self, d_in, dims, d_latent, dropouts):\n",
    "        super().__init__(d_latent)\n",
    "        \n",
    "        dims = [d_in]+dims\n",
    "        \n",
    "        acts = [True]*(len(dims)-1)\n",
    "        bns = [True]*(len(dims)-1)\n",
    "        layers = [LinearBlock(d_in, d_out, act=a, bn=b, dropout=p)\n",
    "                 for d_in, d_out, a, b, p in zip(dims[:-1], dims[1:], acts, bns, dropouts)]\n",
    "        layers.append(nn.Linear(dims[-1], d_latent))\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MLP_Encoder(128, [64, 32, 16], d_latent, [0.1, 0.1, 0.1])\n",
    "assert m(torch.randn(8,128)).shape[-1] == d_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Conv_Encoder(Encoder):\n",
    "    '''\n",
    "    Conv_Encoder - 1D conv encoder\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `d_vocab int`: vocab size\n",
    "    \n",
    "    - `d_embedding int`: embedding dimension\n",
    "\n",
    "    - `d_latent int`: latent vector dimension\n",
    "\n",
    "    - `filters list[int]`: filter sizes for conv layers ie `[64, 128, 256]`\n",
    "\n",
    "    - `kernel_sizes list[int]`: kernel sizes for conv layers ie `[5, 5, 5]`\n",
    "\n",
    "    - `strides list[int]`: strides for conv layers ie `[2, 2, 2]`\n",
    "\n",
    "    - `dropouts list[float]`: list of dropout pobabilities ie `[0.2, 0.2, 0.3]`\n",
    "    '''\n",
    "    def __init__(self, d_vocab, d_embedding, d_latent, filters, kernel_sizes, strides, dropouts):\n",
    "        super().__init__(d_latent)\n",
    "        \n",
    "        self.embedding = nn.Embedding(d_vocab, d_embedding)\n",
    "        \n",
    "        filters = [d_embedding] + filters\n",
    "        \n",
    "        convs = [Conv1d(filters[i], filters[i+1], ks=kernel_sizes[i],\n",
    "                        stride=strides[i], act=True, bn=True, dropout=dropouts[i])\n",
    "                    for i in range(len(filters)-1)]\n",
    "\n",
    "        self.convs = nn.Sequential(*convs)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.head = nn.Linear(filters[-1], d_latent)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.convs(x)\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Conv_Encoder(32, 64, d_latent, [32, 16], [7,7], [2,2], [0.1, 0.1])\n",
    "assert c(torch.randint(0,31, (10,15))).shape[-1] == d_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    MLP - multi-layer perceptron\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `d_in int`: number of input dimensions\n",
    "        \n",
    "    - `dims list[int]`: list of layer sizes ie `[1024, 512, 256]`\n",
    "        \n",
    "    - `d_out int`: number of output dimensions\n",
    "        \n",
    "    - `drops list[float]`: list of dropout pobabilities ie `[0.2, 0.2, 0.3]`\n",
    "        \n",
    "    - `outrange Optional[list[float]]`: squashes the output to be between `outrange[0]` \n",
    "    and `outrange[1]`\n",
    "    '''\n",
    "    def __init__(self, d_in, dims, d_out, drops, outrange=None, bn=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        dims = [d_in]+dims\n",
    "        \n",
    "        acts = [True]*(len(dims)-1)\n",
    "        bns = [True]*(len(dims)-1) if bn else [False]*(len(dims)-1)\n",
    "        layers = [LinearBlock(d_in, d_out, act=a, bn=b, dropout=p)\n",
    "                 for d_in, d_out, a, b, p in zip(dims[:-1], dims[1:], acts, bns, drops)]\n",
    "        layers.append(nn.Linear(dims[-1], d_out))\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.outrange = outrange\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        \n",
    "        if self.outrange is not None:\n",
    "            x = torch.sigmoid(x) * (self.outrange[1]-self.outrange[0]) + self.outrange[0]\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrl",
   "language": "python",
   "name": "mrl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
