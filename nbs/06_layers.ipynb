{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp layers\n",
    "# all_slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "\n",
    "> Pytorch model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from mrl.imports import *\n",
    "from mrl.torch_imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, d_in, d_out, act=True, bn=False, dropout=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = [nn.Linear(d_in, d_out)]\n",
    "        \n",
    "        if bn:\n",
    "            layers.append(nn.BatchNorm1d(d_out))\n",
    "            \n",
    "        if act:\n",
    "            layers.append(nn.ReLU())\n",
    "            \n",
    "        if dropout>0.:\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, d_embedding, d_hidden, d_output, n_layers, \n",
    "                 bidir=False, dropout=0., batch_first=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_embedding = d_embedding\n",
    "        self.d_hidden = d_hidden\n",
    "        self.d_output = d_output\n",
    "        self.n_layers = n_layers\n",
    "        self.bidir = bidir\n",
    "        self.n_dir = 1 if not bidir else 2\n",
    "        self.batch_first = batch_first\n",
    "        \n",
    "        self.lstms = []\n",
    "        self.hidden_sizes = []\n",
    "        \n",
    "        for l in range(n_layers):\n",
    "            input_size = d_embedding if l==0 else d_hidden\n",
    "            output_size = d_output if l==n_layers-1 else d_hidden\n",
    "            \n",
    "            hidden_size = (self.n_dir, 1, output_size)\n",
    "            self.hidden_sizes.append(hidden_size)\n",
    "            \n",
    "            lstm = nn.LSTM(input_size, output_size, 1, batch_first=batch_first, \n",
    "                           dropout=dropout, bidirectional=bidir)\n",
    "            self.lstms.append(lstm)\n",
    "            \n",
    "        self.lstms = nn.ModuleList(self.lstms)\n",
    "        \n",
    "    def forward(self, x, hiddens=None):\n",
    "        \n",
    "        bs = x.shape[0] if self.batch_first else x.shape[1]\n",
    "        \n",
    "        if hiddens is None:\n",
    "            hiddens = self.get_new_hidden(bs)\n",
    "            hiddens = [(i[0].to(x.device), i[1].to(x.device)) for i in hiddens]\n",
    "            \n",
    "        new_hiddens = []\n",
    "        for i, lstm in enumerate(self.lstms):\n",
    "            x, (h,c) = lstm(x, hiddens[i])\n",
    "            new_hiddens.append((h.detach(), c.detach()))\n",
    "            \n",
    "        return x, new_hiddens\n",
    "            \n",
    "    def get_new_hidden(self, bs):\n",
    "        hiddens = []\n",
    "        for hs in self.hidden_sizes:\n",
    "            h = torch.zeros(hs).repeat(1,bs,1)\n",
    "            c = torch.zeros(hs).repeat(1,bs,1)\n",
    "            hiddens.append((h,c))\n",
    "        \n",
    "        return hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LSTMLM(nn.Module):\n",
    "    def __init__(self, d_vocab, d_embedding, d_hidden, n_layers, pad_idx, \n",
    "                 lstm_drop=0., bos_idx=0, bidir=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(d_vocab, d_embedding)\n",
    "        self.lstm = LSTM(d_embedding, d_hidden, d_embedding, n_layers, bidir=bidir, dropout=lstm_drop)\n",
    "        self.head = Linear(d_embedding, d_vocab, act=False, bn=False, dropout=0.)\n",
    "        self.bos_idx = bos_idx\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, hiddens = self.lstm(x)\n",
    "        self.last_hidden = hiddens\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "    \n",
    "    def sample(self, bs, sl, multinomial=True):\n",
    "        \n",
    "        preds = idxs = torch.tensor([vocab.stoi['bos']]*bs).long().unsqueeze(-1) # todo - cuda\n",
    "        lps = []\n",
    "\n",
    "        hiddens = self.lstm.get_new_hidden(bs)\n",
    "        \n",
    "        for i in range(sl):\n",
    "            x = self.embedding(idxs)\n",
    "            x, hiddens = self.lstm(x, hiddens)\n",
    "            x = self.head(x)\n",
    "\n",
    "            log_probs = F.log_softmax(x, -1).squeeze(1)\n",
    "            probs = log_probs.detach().exp()\n",
    "            \n",
    "            if multinomial:\n",
    "                idxs = torch.multinomial(probs, 1)\n",
    "            else:\n",
    "                idxs = x.argmax(-1)\n",
    "                \n",
    "            lps.append(torch.gather(log_probs, 1, idxs))\n",
    "            \n",
    "            preds = torch.cat([preds, idxs], -1)\n",
    "            \n",
    "        return preds[:, 1:], torch.cat(lps,-1)\n",
    "    \n",
    "    def sample_no_grad(self, bs, sl, multinomial=True):\n",
    "        with torch.no_grad():\n",
    "            return self.sample(bs, sl, multinomial=multinomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrl.dataloaders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/envs/mrl/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for boost::shared_ptr<RDKit::FilterCatalogEntry const> already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from mrl.chem import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrl.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = CharacterVocab(SMILES_CHAR_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMLM(len(vocab.itos), 256, 1024, 3, vocab.stoi['pad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('untracked_files/smiles_lm.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = []\n",
    "for i in range(400):\n",
    "        \n",
    "    preds, lps = model.sample(100,100)\n",
    "    s = [vocab.reconstruct(i) for i in preds]\n",
    "    smiles.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "437"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(smiles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = flatten_list_of_lists(smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43700"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mols = to_mols(smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9962471395881007"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in mols if i is not None])/len(smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999313501144165"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(smiles))/len(smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-20.8896, -21.9627, -24.3883, -23.2155, -21.5423, -21.4593, -23.0183,\n",
       "        -20.6185, -20.4622, -21.0812, -23.2870, -20.9693, -20.7390, -25.3426,\n",
       "        -21.4910, -27.4981, -22.5912, -20.7221, -21.7060, -21.6088, -21.0723,\n",
       "        -20.8114, -21.7904, -22.0713, -20.7259, -24.6049, -25.1872, -21.4254,\n",
       "        -20.9317, -20.7213, -21.4634, -20.2982, -20.8087, -21.3563, -21.1142,\n",
       "        -21.6108, -20.3180, -21.1286, -21.5118, -20.9709, -21.2088, -22.0678,\n",
       "        -25.4424, -20.6760, -22.2405, -20.4261, -21.4985, -21.9339, -22.6726,\n",
       "        -22.4086, -20.2079, -20.9287, -22.3963, -21.3464, -22.2943, -23.4055,\n",
       "        -20.6830, -20.9470, -21.4593, -21.3202, -22.2658, -20.9025, -20.7081,\n",
       "        -21.4945, -23.9573, -22.0222, -21.7784, -20.7909, -24.5765, -20.3598,\n",
       "        -20.1857, -21.5615, -19.6921, -21.4191, -22.1406, -20.0879, -22.7940,\n",
       "        -20.5922, -20.5122, -21.3518, -22.0540, -20.4816, -21.8322, -20.9540,\n",
       "        -20.7582, -21.0134, -20.5224, -22.3031, -20.6800, -20.2836, -22.4045,\n",
       "        -22.6099, -20.5144, -21.5761, -21.1554, -21.1296, -20.5617, -21.1211,\n",
       "        -22.2821, -21.0635], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lps.sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_chem.ipynb.\n",
      "Converted 02_template.filters.ipynb.\n",
      "Converted 03_template.template.ipynb.\n",
      "Converted 04_template.blocks.ipynb.\n",
      "Converted 05_torch_core.ipynb.\n",
      "Converted 06_layers.ipynb.\n",
      "Converted 07_dataloaders.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted template.overview.ipynb.\n",
      "Converted tutorials.ipynb.\n",
      "Converted tutorials.structure_enumeration.ipynb.\n",
      "Converted tutorials.template.advanced.ipynb.\n",
      "Converted tutorials.template.beginner.ipynb.\n",
      "Converted tutorials.template.intermediate.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
