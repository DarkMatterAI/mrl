{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp layers\n",
    "# all_slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "\n",
    "> Pytorch model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from mrl.imports import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, d_in, d_out, act=True, bn=False, dropout=0., **lin_kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = [nn.Linear(d_in, d_out, **lin_kwargs)]\n",
    "        \n",
    "        if bn:\n",
    "            layers.append(nn.BatchNorm1d(d_out))\n",
    "            \n",
    "        if act:\n",
    "            layers.append(nn.ReLU())\n",
    "            \n",
    "        if dropout>0.:\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, d_in, d_out, ks=3, stride=1, padding=None, ndim=2, \n",
    "                 act=True, bn=False, dropout=0., **conv_kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        if padding is None:\n",
    "            padding = (ks-1)//2\n",
    "            \n",
    "        if ndim==1:\n",
    "            conv_func = nn.Conv1d\n",
    "            bn_func = nn.BatchNorm1d\n",
    "        elif ndim==2:\n",
    "            conv_func = nn.Conv2d\n",
    "            bn_func = nn.BatchNorm2d\n",
    "        else:\n",
    "            conv_func = nn.Conv3d\n",
    "            bn_func = nn.BatchNorm3d\n",
    "        \n",
    "        layers = [conv_func(d_in, d_out, ks, stride, padding=padding, **conv_kwargs)]\n",
    "        \n",
    "        if bn:\n",
    "            layers.append(bn_func(d_out))\n",
    "            \n",
    "        if act:\n",
    "            layers.append(nn.ReLU())\n",
    "            \n",
    "        if dropout>0.:\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "class Conv1d(Conv):\n",
    "    def __init__(self, d_in, d_out, ks=3, stride=1, padding=None, \n",
    "                 act=True, bn=False, dropout=0., **conv_kwargs):\n",
    "        super().__init__(d_in, d_out, ks, stride, padding, ndim=1, \n",
    "                 act=act, bn=bn, dropout=dropout, **conv_kwargs)\n",
    "        \n",
    "class Conv2d(Conv):\n",
    "    def __init__(self, d_in, d_out, ks=3, stride=1, padding=None, \n",
    "                 act=True, bn=False, dropout=0., **conv_kwargs):\n",
    "        super().__init__(d_in, d_out, ks, stride, padding, ndim=2, \n",
    "                 act=act, bn=bn, dropout=dropout, **conv_kwargs)\n",
    "        \n",
    "class Conv3d(Conv):\n",
    "    def __init__(self, d_in, d_out, ks=3, stride=1, padding=None, \n",
    "                 act=True, bn=False, dropout=0., **conv_kwargs):\n",
    "        super().__init__(d_in, d_out, ks, stride, padding, ndim=3, \n",
    "                 act=act, bn=bn, dropout=dropout, **conv_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = Linear(128, 64, bn=True, dropout=0.5)\n",
    "_ = layer(torch.randn(16,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class SphericalDistribution(torch.distributions.Distribution):\n",
    "    def __init__(self, loc, scale, validate_args=False):\n",
    "        super().__init__(loc.shape[0], validate_args=validate_args)\n",
    "        self.dim = loc.shape[0]\n",
    "        self.loc = loc\n",
    "        self.scale = scale\n",
    "        self.dist = Normal(self.loc, self.scale)\n",
    "        \n",
    "    def sample(self, n):\n",
    "        s = self.dist.sample(n)\n",
    "        s = F.normalize(s, p=2, dim=-1)\n",
    "        return s\n",
    "    \n",
    "    def rsample(self, n):\n",
    "        s = self.dist.rsample(n)\n",
    "        s = F.normalize(s, p=2, dim=-1)\n",
    "        return s\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Spherical(loc: {self.loc.size()}, scale: {self.scale.size()})'\n",
    "\n",
    "class Prior(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def get_dist(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def log_prob(self, x):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def sample(self, n):\n",
    "        if type(n)==int:\n",
    "            n = [n]\n",
    "        return self.get_dist().sample(n)\n",
    "    \n",
    "    def rsample(self, n):\n",
    "        if type(n)==int:\n",
    "            n = [n]\n",
    "        return self.get_dist().rsample(n)\n",
    "    \n",
    "    \n",
    "class NormalPrior(Prior):\n",
    "    def __init__(self, loc, log_scale, trainable=True):\n",
    "        super().__init__()\n",
    "        if trainable:\n",
    "            loc = nn.Parameter(loc)\n",
    "            log_scale = nn.Parameter(log_scale)\n",
    "        self.loc = loc\n",
    "        self.log_scale = log_scale\n",
    "        self.trainable = trainable\n",
    "        \n",
    "    def get_dist(self):\n",
    "        return Normal(self.loc, self.log_scale.exp())\n",
    "    \n",
    "    def log_prob(self, x):\n",
    "        var = self.log_scale.exp().pow(2)\n",
    "        return -((x - self.loc) ** 2) / (2 * var) - self.log_scale - math.log(math.sqrt(2 * math.pi))\n",
    "    \n",
    "class SphericalPrior(NormalPrior):\n",
    "    def __init__(self, loc, log_scale, trainable=True):\n",
    "        super().__init__(loc, log_scale, trainable)\n",
    "        \n",
    "    def get_dist(self):\n",
    "        return SphericalDistribution(self.loc, self.log_scale.exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = NormalPrior(torch.zeros((64,)), torch.zeros((64,)), trainable=True)\n",
    "assert p.rsample(5).requires_grad\n",
    "assert not p.sample(5).requires_grad\n",
    "\n",
    "p = SphericalPrior(torch.zeros((2,)), torch.zeros((2,)), trainable=True)\n",
    "assert p.rsample(5).requires_grad\n",
    "assert not p.sample(5).requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Conditional_LSTM(nn.Module):\n",
    "    def __init__(self, d_embedding, d_hidden, d_output, d_latent, n_layers,\n",
    "                 condition_hidden=True, condition_output=True,\n",
    "                 bidir=False, dropout=0., batch_first=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_embedding = d_embedding\n",
    "        self.d_hidden = d_hidden\n",
    "        self.d_output = d_output\n",
    "        self.n_layers = n_layers\n",
    "        self.bidir = bidir\n",
    "        self.n_dir = 1 if not bidir else 2\n",
    "        self.batch_first = batch_first\n",
    "        self.condition_hidden = condition_hidden\n",
    "        self.condition_output = condition_output\n",
    "        \n",
    "        self.lstms = []\n",
    "        self.hidden_sizes = []\n",
    "        \n",
    "        for l in range(n_layers):\n",
    "            if l==0:\n",
    "                input_size = d_embedding if not self.condition_output else d_embedding+d_latent\n",
    "            else:\n",
    "                input_size = d_hidden\n",
    "                \n",
    "            output_size = d_output if l==n_layers-1 else d_hidden\n",
    "            output_size = output_size // self.n_dir\n",
    "            \n",
    "            hidden_size = (self.n_dir, 1, output_size)\n",
    "            self.hidden_sizes.append(hidden_size)\n",
    "            \n",
    "            lstm = nn.LSTM(input_size, output_size, 1, batch_first=batch_first, \n",
    "                           dropout=dropout, bidirectional=bidir)\n",
    "            self.lstms.append(lstm)\n",
    "            \n",
    "        self.lstms = nn.ModuleList(self.lstms)\n",
    "        \n",
    "        if self.condition_hidden:\n",
    "            to_hidden = []\n",
    "            for size in self.hidden_sizes:\n",
    "                ndir, _, dim = size\n",
    "                to_hidden.append(nn.Linear(d_latent, ndir*dim*2))\n",
    "                \n",
    "            self.to_hidden = nn.ModuleList(to_hidden)\n",
    "        \n",
    "    def forward(self, x, z, hiddens=None):\n",
    "        \n",
    "        bs = x.shape[0] if self.batch_first else x.shape[1]\n",
    "        sl = x.shape[1] if self.batch_first else x.shape[0]\n",
    "        \n",
    "        if self.condition_output:\n",
    "            if self.batch_first:\n",
    "                z_ = z.unsqueeze(1).repeat(1,sl,1)\n",
    "            else:\n",
    "                z_ = z.unsqueeze(0).repeat(sl,1,1)\n",
    "                \n",
    "            x = torch.cat([x, z_], -1)\n",
    "\n",
    "        if hiddens is None:\n",
    "            if self.condition_hidden:\n",
    "                hiddens = self.latent_to_hidden(z)\n",
    "                \n",
    "            else:\n",
    "                hiddens = self.get_new_hidden(bs)\n",
    "            \n",
    "            hiddens = to_device(hiddens, x.device)\n",
    "            \n",
    "        new_hiddens = []\n",
    "        for i, lstm in enumerate(self.lstms):\n",
    "            x, (h,c) = lstm(x, hiddens[i])\n",
    "            new_hiddens.append((h.detach(), c.detach()))\n",
    "            \n",
    "        return x, new_hiddens\n",
    "    \n",
    "    def latent_to_hidden(self, z):\n",
    "        hiddens = []\n",
    "        for layer in self.to_hidden:\n",
    "            h = layer(z)\n",
    "            h,c = torch.chunk(h, 2, dim=-1)\n",
    "            bs, _ = h.shape\n",
    "            h = h.contiguous().reshape(bs, self.n_dir, -1).permute(1,0,2)\n",
    "            c = c.contiguous().reshape(bs, self.n_dir, -1).permute(1,0,2)\n",
    "            hiddens.append((h,c))\n",
    "            \n",
    "        return hiddens\n",
    "            \n",
    "    def get_new_hidden(self, bs):\n",
    "        hiddens = []\n",
    "        for hs in self.hidden_sizes:\n",
    "            h = torch.zeros(hs).repeat(1,bs,1)\n",
    "            c = torch.zeros(hs).repeat(1,bs,1)\n",
    "            hiddens.append((h,c))\n",
    "        \n",
    "        return hiddens\n",
    "    \n",
    "    def mixup_hiddens(self, hiddens):\n",
    "        new_hiddens = []\n",
    "        for item in hiddens:\n",
    "            h,c = item\n",
    "            shuffle = to_device(torch.randperm(h.shape[1]))\n",
    "            h = h[:,shuffle]\n",
    "            c = c[:,shuffle]\n",
    "            new_hiddens.append((h,c))\n",
    "        return new_hiddens\n",
    "    \n",
    "class LSTM(Conditional_LSTM):\n",
    "    def __init__(self, d_embedding, d_hidden, d_output, n_layers, \n",
    "                 bidir=False, dropout=0., batch_first=True):\n",
    "        super().__init__(d_embedding, d_hidden, d_output, 0, n_layers,\n",
    "                 condition_hidden=False, condition_output=False,\n",
    "                 bidir=bidir, dropout=dropout, batch_first=batch_first)\n",
    "\n",
    "    def forward(self, x, hiddens=None):\n",
    "        \n",
    "        x, new_hiddens = super().forward(x, None, hiddens)\n",
    "            \n",
    "        return x, new_hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_embedding=64\n",
    "d_hidden=128\n",
    "d_latent = 32\n",
    "n_layers = 2\n",
    "\n",
    "l1 = Conditional_LSTM(d_embedding, d_hidden, d_embedding, d_latent, n_layers,\n",
    "                     condition_hidden=True, condition_output=True, \n",
    "                     bidir=False, batch_first=True)\n",
    "\n",
    "l2 = Conditional_LSTM(d_embedding, d_hidden, d_embedding, d_latent, n_layers,\n",
    "                     condition_hidden=True, condition_output=True, \n",
    "                     bidir=True, batch_first=True)\n",
    "\n",
    "l3 = Conditional_LSTM(d_embedding, d_hidden, d_embedding, d_latent, n_layers,\n",
    "                     condition_hidden=False, condition_output=True, \n",
    "                     bidir=False, batch_first=True)\n",
    "\n",
    "l4 = Conditional_LSTM(d_embedding, d_hidden, d_embedding, d_latent, n_layers,\n",
    "                     condition_hidden=True, condition_output=False, \n",
    "                     bidir=False, batch_first=True)\n",
    "\n",
    "l5 = Conditional_LSTM(d_embedding, d_hidden, d_embedding, d_latent, n_layers,\n",
    "                     condition_hidden=False, condition_output=False, \n",
    "                     bidir=True, batch_first=True)\n",
    "\n",
    "bs = 12\n",
    "x = torch.randn((bs, 21, d_embedding))\n",
    "z = torch.randn((bs, d_latent))\n",
    "\n",
    "_ = l1(x,z)\n",
    "_ = l1(x,z, l1.latent_to_hidden(z))\n",
    "\n",
    "_ = l2(x,z)\n",
    "_ = l2(x,z, l2.latent_to_hidden(z))\n",
    "\n",
    "_ = l3(x,z)\n",
    "_ = l3(x,z, l3.get_new_hidden(bs))\n",
    "\n",
    "_ = l4(x,z)\n",
    "_ = l4(x,z, l4.get_new_hidden(bs))\n",
    "\n",
    "_ = l5(x,z)\n",
    "_ = l5(x,None)\n",
    "_ = l5(x,z, l5.get_new_hidden(bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = LSTM(d_embedding, d_hidden, d_embedding, n_layers, bidir=False, batch_first=True)\n",
    "\n",
    "l2 = LSTM(d_embedding, d_hidden, d_embedding, n_layers, bidir=True, batch_first=True)\n",
    "\n",
    "_ = l1(x)\n",
    "_ = l1(x, l1.get_new_hidden(bs))\n",
    "\n",
    "_ = l2(x)\n",
    "_ = l2(x, l2.get_new_hidden(bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Conditional_LSTM_Block(nn.Module):\n",
    "    def __init__(self, d_vocab, d_embedding, d_hidden, d_output, d_latent, n_layers,\n",
    "                 lstm_drop=0., lin_drop=0., bidir=False,\n",
    "                 condition_hidden=True, condition_output=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(d_vocab, d_embedding)\n",
    "        self.lstm = Conditional_LSTM(d_embedding, d_hidden, d_output, d_latent, n_layers,\n",
    "                                    condition_hidden=condition_hidden, condition_output=condition_output,\n",
    "                                     bidir=bidir, dropout=lstm_drop)\n",
    "        \n",
    "        self.head = Linear(d_output, d_vocab, act=False, bn=False, dropout=lin_drop)\n",
    "        \n",
    "    def forward(self, x, z, hiddens=None):\n",
    "        x = self.embedding(x)\n",
    "        x, hiddens = self.lstm(x, z, hiddens)\n",
    "        x = self.head(x)\n",
    "        \n",
    "        return x, hiddens\n",
    "\n",
    "class LSTM_Block(nn.Module):\n",
    "    def __init__(self, d_vocab, d_embedding, d_hidden, d_output, n_layers,\n",
    "                 lstm_drop=0., lin_drop=0., bidir=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(d_vocab, d_embedding)\n",
    "        self.lstm = LSTM(d_embedding, d_hidden, d_output, n_layers,\n",
    "                                     bidir=bidir, dropout=lstm_drop)\n",
    "        \n",
    "        self.head = nn.Linear(d_output, d_vocab)\n",
    "        self.head_drop = nn.Dropout(lin_drop)\n",
    "        \n",
    "    def forward(self, x, hiddens=None):\n",
    "        x = self.embedding(x)\n",
    "        x, hiddens = self.lstm(x, hiddens)\n",
    "        x = self.head_drop(self.head(x))\n",
    "        \n",
    "        return x, hiddens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LSTM_LM(nn.Module):\n",
    "    def __init__(self, d_vocab, d_embedding, d_hidden, n_layers, \n",
    "                 lstm_drop=0., lin_drop=0., bos_idx=0, bidir=False, tie_weights=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.block = LSTM_Block(d_vocab, d_embedding, d_hidden, d_embedding, n_layers,\n",
    "                                lstm_drop=lstm_drop, lin_drop=lin_drop, bidir=bidir)\n",
    "        self.bos_idx = bos_idx\n",
    "        \n",
    "        if tie_weights:\n",
    "            self.block.embedding.weight = self.block.head.weight\n",
    "        \n",
    "    def forward(self, x, hiddens=None):\n",
    "        x, hiddens = self.block(x, hiddens)\n",
    "        return x\n",
    "    \n",
    "    def sample(self, bs, sl, temperature=1., multinomial=True):\n",
    "        \n",
    "        preds = idxs = to_device(torch.tensor([self.bos_idx]*bs).long().unsqueeze(-1))\n",
    "        lps = []\n",
    "\n",
    "        hiddens = None\n",
    "        \n",
    "        for i in range(sl):\n",
    "            x, hiddens = self.block(idxs, hiddens)\n",
    "            x.div_(temperature)\n",
    "            \n",
    "            idxs, lp = x_to_preds(x, multinomial=multinomial)\n",
    "            \n",
    "            lps.append(lp)            \n",
    "            preds = torch.cat([preds, idxs], -1)\n",
    "            \n",
    "        return preds[:, 1:], torch.cat(lps,-1)\n",
    "    \n",
    "    def sample_no_grad(self, bs, sl, temperature=1., multinomial=True):\n",
    "        with torch.no_grad():\n",
    "            return self.sample(bs, sl, temperature=temperature, multinomial=multinomial)\n",
    "        \n",
    "    def get_lps(self, x, y, temperature=1.):\n",
    "        x = self.forward(x)\n",
    "        x.div_(temperature)\n",
    "        \n",
    "        lps = F.log_softmax(x, -1)\n",
    "        lps = lps.gather(2, y.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        return lps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LSTM_LM(32, 64, 256, 2)\n",
    "ints = torch.randint(0, 31, (16, 10))\n",
    "x = ints[:,:-1]\n",
    "y = ints[:,1:]\n",
    "out = lm(x)\n",
    "lp = lm.get_lps(x,y)\n",
    "_ = lm.sample(8, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_latent):\n",
    "        super().__init__()\n",
    "        self.d_latent = d_latent\n",
    "\n",
    "class LSTM_Encoder(Encoder):\n",
    "    def __init__(self, d_vocab, d_embedding, d_hidden, n_layers, d_latent, dropout=0.):\n",
    "        super().__init__(d_latent)\n",
    "        self.embedding = nn.Embedding(d_vocab, d_embedding)\n",
    "        self.lstm = LSTM(d_embedding, d_hidden, d_hidden, n_layers, \n",
    "                                 bidir=True, batch_first=True, dropout=dropout)\n",
    "        self.head = nn.Linear(d_hidden*2, d_latent)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, hiddens = self.lstm(x)\n",
    "        hidden = torch.cat(list(torch.cat(hiddens[-1], -1)), -1) # concatenate hidden/cell states of last layer\n",
    "        latent = self.head(hidden)\n",
    "        return latent\n",
    "    \n",
    "class MLP_Encoder(Encoder):\n",
    "    def __init__(self, d_in, dims, d_latent, dropouts):\n",
    "        super().__init__(d_latent)\n",
    "        \n",
    "        dims = [d_in]+dims\n",
    "        \n",
    "        acts = [True]*(len(dims)-1)\n",
    "        bns = [True]*(len(dims)-1)\n",
    "        layers = [Linear(d_in, d_out, act=a, bn=b, dropout=p)\n",
    "                 for d_in, d_out, a, b, p in zip(dims[:-1], dims[1:], acts, bns, dropouts)]\n",
    "        layers.append(nn.Linear(dims[-1], d_latent))\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Conv_Encoder(Encoder):\n",
    "    def __init__(self, d_vocab, d_embedding, d_latent, filters, kernel_sizes, strides, dropouts):\n",
    "        super().__init__(d_latent)\n",
    "        \n",
    "        self.embedding = nn.Embedding(d_vocab, d_embedding)\n",
    "        \n",
    "        filters = [d_embedding] + filters\n",
    "        \n",
    "        convs = [Conv1d(filters[i], filters[i+1], ks=kernel_sizes[i],\n",
    "                        stride=strides[i], act=True, bn=True, dropout=dropouts[i])\n",
    "                    for i in range(len(filters)-1)]\n",
    "\n",
    "        self.convs = nn.Sequential(*convs)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.head = nn.Linear(filters[-1], d_latent)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.convs(x)\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_latent = 128\n",
    "l = LSTM_Encoder(32, 64, 128, 2, 128)\n",
    "assert l(torch.randint(0,31, (10,15))).shape[-1] == d_latent\n",
    "\n",
    "m = MLP_Encoder(128, [64, 32, 16], d_latent, [0.1, 0.1, 0.1])\n",
    "assert m(torch.randn(8,128)).shape[-1] == d_latent\n",
    "\n",
    "c = Conv_Encoder(32, 64, d_latent, [32, 16], [7,7], [2,2], [0.1, 0.1])\n",
    "assert c(torch.randint(0,31, (10,15))).shape[-1] == d_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class VAE_Transition(nn.Module):\n",
    "    def __init__(self, d_latent):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_latent = d_latent\n",
    "        self.transition = nn.Linear(d_latent, d_latent*2)\n",
    "        \n",
    "    def forward(self, x, z_scale=1.):\n",
    "        mu, logvar = self.get_stats(x)\n",
    "        z = z_scale*torch.randn(mu.shape).to(mu.device)\n",
    "        z = mu + z*torch.exp(0.5*logvar)\n",
    "        kl_loss = 0.5 * (logvar.exp() + mu.pow(2) - 1 - logvar).sum(1).mean()\n",
    "        return z, kl_loss\n",
    "    \n",
    "    def get_stats(self, x):\n",
    "        mu, logvar = torch.chunk(self.transition(x), 2, dim=-1)\n",
    "        return mu, logvar\n",
    "    \n",
    "class Norm_Transition(nn.Module):\n",
    "    def __init__(self, d_latent, p=2):\n",
    "        super().__init__()\n",
    "        self.d_latent = d_latent\n",
    "        self.p = p\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.normalize(x, p=self.p, dim=-1)\n",
    "        return x\n",
    "\n",
    "class PT_Transition(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Norm_Transition(128, p=2)\n",
    "x = torch.randn((8, 128))\n",
    "assert torch.allclose(t(x).pow(2).sum(-1), torch.ones(x.shape[0]).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Encoder_Decoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, transition=None, prior=None):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        if transition is None:\n",
    "            transition = PT_Transition()\n",
    "            \n",
    "        self.transition = transition\n",
    "        \n",
    "        if prior is None:\n",
    "            prior = NormalPrior(torch.zeros((encoder.d_latent)), torch.zeros((encoder.d_latent)), \n",
    "                                trainable=False)\n",
    "        \n",
    "        self.prior = prior\n",
    "        \n",
    "    def forward(self, x, decoder_input=None):\n",
    "        if decoder_input is None:\n",
    "            decoder_input = x\n",
    "            \n",
    "        z = self.encoder(x)\n",
    "        z = self.transition(x)\n",
    "        output = self.decoder(decoder_input, z)\n",
    "        return output\n",
    "    \n",
    "    def set_prior(self, prior):\n",
    "        self.prior = prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "class VAE(Encoder_Decoder):\n",
    "    def __init__(self, encoder, decoder, prior=None, bos_idx=0):\n",
    "        transition = VAE_Transition(encoder.d_latent)\n",
    "        super().__init__(encoder, decoder, transition, prior)\n",
    "            \n",
    "        self.bos_idx = bos_idx\n",
    "        \n",
    "    def forward(self, x, decoder_input=None):\n",
    "        \n",
    "        z = self.encoder(x)\n",
    "        z, kl_loss = self.transition(z)\n",
    "            \n",
    "        if decoder_input is None:\n",
    "            decoder_input = x\n",
    "            \n",
    "        output, hiddens = self.decoder(decoder_input, z)\n",
    "        return output, kl_loss\n",
    "    \n",
    "\n",
    "    def sample(self, bs, sl, z=None, temperature=1., multinomial=True):\n",
    "        \n",
    "        preds = idxs = to_device(torch.tensor([self.bos_idx]*bs).long().unsqueeze(-1))\n",
    "        lps = []\n",
    "        \n",
    "        if z is None:\n",
    "            z = to_device(self.prior.rsample([bs]))\n",
    "        \n",
    "        hiddens = None\n",
    "        \n",
    "        for i in range(sl):\n",
    "            x, hiddens = self.decoder(idxs, z, hiddens)\n",
    "            x.div_(temperature)\n",
    "            \n",
    "            idxs, lp = x_to_preds(x, multinomial=multinomial)\n",
    "            \n",
    "            lps.append(lp)            \n",
    "            preds = torch.cat([preds, idxs], -1)\n",
    "            \n",
    "        return preds[:, 1:], torch.cat(lps,-1)\n",
    "    \n",
    "    def sample_no_grad(self, bs, sl, z=None, temperature=1., multinomial=True):\n",
    "        with torch.no_grad():\n",
    "            return self.sample(bs, sl, z=z, temperature=temperature, multinomial=multinomial)\n",
    "        \n",
    "    def get_lps(self, x, y, temperature=1., z=None):\n",
    "\n",
    "        if type(x)==list:\n",
    "            z,_ = self.transition(self.encoder(x[0]))\n",
    "            x,_ = self.decoder(x[1], z)\n",
    "        else:\n",
    "            z,_ = self.transition(self.encoder(x))\n",
    "            x,_ = self.decoder(x,z)\n",
    "        \n",
    "        x.div_(temperature)\n",
    "        \n",
    "        lps = F.log_softmax(x, -1)\n",
    "        lps = lps.gather(2, y.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        if self.prior.trainable:\n",
    "            prior_lps = self.prior.log_prob(z).mean(-1, keepdim=True)\n",
    "            prior_lps = torch.zeros(prior_lps.shape).float() + prior_lps - prior_lps.detach()\n",
    "            lps += prior_lps\n",
    "        \n",
    "        return lps\n",
    "    \n",
    "    def set_prior_from_stats(self, mu, logvar, trainable=False):\n",
    "        mu = mu.detach()\n",
    "        logvar = logvar.detach()\n",
    "        self.prior = NormalPrior(mu, logvar, trainable)\n",
    "        \n",
    "    def set_prior_from_latent(self, z, trainable=False):\n",
    "        mu, logvar = self.transition.get_stats(z)\n",
    "        self.set_prior_from_stats(mu, logvar, trainable)\n",
    "        \n",
    "    def set_prior_from_encoder(self, x, trainable=False):\n",
    "        assert x.shape[0]==1, \"Must set prior from a single input\"\n",
    "        z = self.encoder(x)\n",
    "        z = z.squeeze(0)\n",
    "        self.set_prior_from_latent(z, trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LSTM_Encoder(32, 64, 128, 2, 128)\n",
    "decoder = Conditional_LSTM_Block(32, 64, 128, 64, 128, 2,\n",
    "                                condition_hidden=True, condition_output=True)\n",
    "vae = VAE(encoder, decoder)\n",
    "\n",
    "ints = torch.randint(0, 31, (8, 10))\n",
    "x = ints[:,:-1]\n",
    "y = ints[:,1:]\n",
    "\n",
    "_ = vae(x)\n",
    "\n",
    "decoder = Conditional_LSTM_Block(32, 64, 128, 64, 128, 2,\n",
    "                                condition_hidden=False, condition_output=True)\n",
    "vae = VAE(encoder, decoder)\n",
    "\n",
    "_ = vae(x)\n",
    "\n",
    "decoder = Conditional_LSTM_Block(32, 64, 128, 64, 128, 2,\n",
    "                                condition_hidden=True, condition_output=False)\n",
    "vae = VAE(encoder, decoder)\n",
    "\n",
    "_ = vae(x)\n",
    "\n",
    "_ = vae.sample(8, 16)\n",
    "\n",
    "z = vae.prior.sample([8])\n",
    "_ = vae.sample(8, 16, z=z)\n",
    "\n",
    "_ = vae.get_lps(x,y)\n",
    "\n",
    "vae.set_prior_from_encoder(x[0].unsqueeze(0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LSTM_VAE(VAE):\n",
    "    def __init__(self, d_vocab, d_embedding, d_hidden, n_layers, d_latent,\n",
    "                enc_drop=0., dec_drop=0., condition_hidden=True, condition_output=True,\n",
    "                prior=None, bos_idx=0):\n",
    "        \n",
    "        encoder = LSTM_Encoder(d_vocab, d_embedding, d_hidden, \n",
    "                               n_layers, d_latent, dropout=enc_drop)\n",
    "        \n",
    "        decoder = Conditional_LSTM_Block(d_vocab, d_embedding, d_hidden, d_embedding,\n",
    "                                d_latent, n_layers, lstm_drop=dec_drop, lin_drop=dec_drop, \n",
    "                                condition_hidden=condition_hidden, condition_output=condition_output)\n",
    "        \n",
    "        super().__init__(encoder, decoder, prior, bos_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = LSTM_VAE(32, 64, 128, 2, 128, condition_hidden=True, condition_output=True)\n",
    "\n",
    "ints = torch.randint(0, 31, (8, 10))\n",
    "x = ints[:,:-1]\n",
    "y = ints[:,1:]\n",
    "\n",
    "_ = vae(x)\n",
    "\n",
    "_ = vae.sample(8, 16)\n",
    "\n",
    "z = vae.prior.sample([8])\n",
    "_ = vae.sample(8, 16, z=z)\n",
    "\n",
    "_ = vae.get_lps(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Conv_VAE(VAE):\n",
    "    def __init__(self, d_vocab, d_embedding, conv_filters, kernel_sizes, strides, conv_drops,\n",
    "                 d_hidden, n_layers, d_latent, dec_drop=0., \n",
    "                 condition_hidden=True, condition_output=True,\n",
    "                 prior=None, bos_idx=0):\n",
    "        \n",
    "        encoder = Conv_Encoder(d_vocab, d_embedding, d_latent, \n",
    "                               conv_filters, kernel_sizes, strides, conv_drops)\n",
    "        \n",
    "        decoder = Conditional_LSTM_Block(d_vocab, d_embedding, d_hidden, d_embedding,\n",
    "                                d_latent, n_layers, lstm_drop=dec_drop, lin_drop=dec_drop, \n",
    "                                condition_hidden=condition_hidden, condition_output=condition_output)\n",
    "        \n",
    "        super().__init__(encoder, decoder, prior, bos_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = Conv_VAE(32, 64, [128, 256], [7,7], [2,2], [0.1,0.1], 128, 2, 128, \n",
    "               condition_hidden=False, condition_output=True)\n",
    "\n",
    "ints = torch.randint(0, 31, (8, 10))\n",
    "x = ints[:,:-1]\n",
    "y = ints[:,1:]\n",
    "\n",
    "_ = vae(x)\n",
    "\n",
    "_ = vae.sample(8, 16)\n",
    "\n",
    "z = vae.prior.sample([8])\n",
    "_ = vae.sample(8, 16, z=z)\n",
    "\n",
    "_ = vae.get_lps(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "        \n",
    "class MLP_VAE(VAE):\n",
    "    def __init__(self, d_vocab, d_embedding, encoder_d_in, encoder_dims, encoder_drops,\n",
    "                 d_hidden, n_layers, d_latent, dec_drop=0., \n",
    "                 condition_hidden=True, condition_output=True,\n",
    "                 prior=None, bos_idx=0):\n",
    "        \n",
    "        encoder = MLP_Encoder(encoder_d_in, encoder_dims, d_latent, encoder_drops)\n",
    "        \n",
    "        decoder = Conditional_LSTM_Block(d_vocab, d_embedding, d_hidden, d_embedding,\n",
    "                                d_latent, n_layers, lstm_drop=dec_drop, lin_drop=dec_drop, \n",
    "                                condition_hidden=condition_hidden, condition_output=condition_output)\n",
    "        \n",
    "        super().__init__(encoder, decoder, prior, bos_idx)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = MLP_VAE(32, 64, 128, [64, 32], [0.1, 0.1], 128, 2, 128, \n",
    "               condition_hidden=False, condition_output=True)\n",
    "\n",
    "ints = torch.randint(0, 31, (8, 10))\n",
    "x = ints[:,:-1]\n",
    "y = ints[:,1:]\n",
    "\n",
    "condition = torch.randn((8,128))\n",
    "\n",
    "\n",
    "_ = vae(condition, x)\n",
    "\n",
    "_ = vae.sample(8, 16)\n",
    "\n",
    "z = vae.prior.sample([8])\n",
    "_ = vae.sample(8, 16, z=z)\n",
    "\n",
    "_ = vae.get_lps([condition,x],y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Conditional_LSTM_LM(Encoder_Decoder):\n",
    "    def __init__(self, encoder, d_vocab, d_embedding, d_hidden, d_latent, n_layers,\n",
    "                 lstm_drop=0., lin_drop=0., bidir=False,\n",
    "                 condition_hidden=True, condition_output=False, bos_idx=0, prior=None):\n",
    "        \n",
    "        transition = Norm_Transition(d_latent)\n",
    "        \n",
    "        decoder = Conditional_LSTM_Block(d_vocab, d_embedding, d_hidden, d_embedding,\n",
    "                                d_latent, n_layers, lstm_drop=lstm_drop, lin_drop=lin_drop, \n",
    "                                condition_hidden=condition_hidden, condition_output=condition_output)\n",
    "        \n",
    "        if prior is None:\n",
    "            prior = SphericalPrior(torch.zeros((encoder.d_latent)), torch.zeros((encoder.d_latent)), \n",
    "                                trainable=False)\n",
    "        \n",
    "        super().__init__(encoder, decoder, transition, prior)\n",
    "        \n",
    "        self.bos_idx = bos_idx\n",
    "        \n",
    "    def forward(self, x, condition, hiddens=None):\n",
    "        z = self.encoder(condition)\n",
    "        z = self.transition(z)\n",
    "        x, hiddens = self.decoder(x, z, hiddens)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def sample(self, bs, sl, z=None, temperature=1., multinomial=True):\n",
    "        \n",
    "        if z is None:\n",
    "            if self.prior is not None:\n",
    "                z = to_device(self.prior.rsample([bs]))\n",
    "            else:\n",
    "                z = to_device(torch.randn((bs, self.encoder.d_latent)))\n",
    "                z = self.transition(z)\n",
    "        else:\n",
    "            bs = z.shape[0]\n",
    "        \n",
    "        preds = idxs = to_device(torch.tensor([self.bos_idx]*bs).long().unsqueeze(-1))\n",
    "        lps = []\n",
    "\n",
    "        hiddens = self.decoder.lstm.latent_to_hidden(z)\n",
    "        \n",
    "        for i in range(sl):\n",
    "            \n",
    "            x, hiddens = self.decoder(idxs,z,hiddens)\n",
    "            x.div_(temperature)\n",
    "            \n",
    "            idxs, lp = x_to_preds(x, multinomial=multinomial)\n",
    "            \n",
    "            lps.append(lp)            \n",
    "            preds = torch.cat([preds, idxs], -1)\n",
    "            \n",
    "        return preds[:, 1:], torch.cat(lps,-1)\n",
    "    \n",
    "    def sample_no_grad(self, bs, sl, z=None, temperature=1., multinomial=True):\n",
    "        with torch.no_grad():\n",
    "            return self.sample(bs, sl, z=z, temperature=temperature, multinomial=multinomial)\n",
    "        \n",
    "    def get_lps(self, x, y, temperature=1.):\n",
    "        x, c = x\n",
    "        z = self.transition(self.encoder(c))\n",
    "        x,_ = self.decoder(x, z)\n",
    "        \n",
    "        x.div_(temperature)\n",
    "        \n",
    "        lps = F.log_softmax(x, -1)\n",
    "        lps = lps.gather(2, y.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        if self.prior.trainable:\n",
    "            prior_lps = self.prior.log_prob(z).mean(-1, keepdim=True)\n",
    "            prior_lps = torch.zeros(prior_lps.shape).float() + prior_lps - prior_lps.detach()\n",
    "            lps += prior_lps\n",
    "        \n",
    "        return lps\n",
    "        \n",
    "    def set_prior_from_latent(self, z, logvar, trainable=False):\n",
    "        z = z.detach()\n",
    "        logvar = logvar.detach()\n",
    "        self.prior = SphericalPrior(z, logvar, trainable)\n",
    "        \n",
    "    def set_prior_from_encoder(self, condition, logvar, trainable=False):\n",
    "        assert condition.shape[0]==1\n",
    "        z = self.transition(self.encoder(condition))\n",
    "        z = z.squeeze(0)\n",
    "        self.set_prior_from_latent(z, logvar, trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = MLP_Encoder(128, [64, 32], 16, [0.1, 0.1])\n",
    "\n",
    "lm = Conditional_LSTM_LM(encoder, 32, 64, 128, 16, 2)\n",
    "\n",
    "ints = torch.randint(0, 31, (8, 10))\n",
    "x = ints[:,:-1]\n",
    "y = ints[:,1:]\n",
    "\n",
    "condition = torch.randn((8,128))\n",
    "\n",
    "_ = lm(x, condition)\n",
    "\n",
    "_ = lm.get_lps([x,condition],y)\n",
    "\n",
    "_ = lm.sample(3, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTM(nn.Module):\n",
    "#     def __init__(self, d_embedding, d_hidden, d_output, n_layers, \n",
    "#                  bidir=False, dropout=0., batch_first=True):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.d_embedding = d_embedding\n",
    "#         self.d_hidden = d_hidden\n",
    "#         self.d_output = d_output\n",
    "#         self.n_layers = n_layers\n",
    "#         self.bidir = bidir\n",
    "#         self.n_dir = 1 if not bidir else 2\n",
    "#         self.batch_first = batch_first\n",
    "        \n",
    "#         self.lstms = []\n",
    "#         self.hidden_sizes = []\n",
    "        \n",
    "#         for l in range(n_layers):\n",
    "#             input_size = d_embedding if l==0 else d_hidden\n",
    "#             output_size = d_output if l==n_layers-1 else d_hidden\n",
    "#             output_size = output_size // self.n_dir\n",
    "            \n",
    "#             hidden_size = (self.n_dir, 1, output_size)\n",
    "#             self.hidden_sizes.append(hidden_size)\n",
    "            \n",
    "#             lstm = nn.LSTM(input_size, output_size, 1, batch_first=batch_first, \n",
    "#                            dropout=dropout, bidirectional=bidir)\n",
    "#             self.lstms.append(lstm)\n",
    "            \n",
    "#         self.lstms = nn.ModuleList(self.lstms)\n",
    "        \n",
    "#     def forward(self, x, hiddens=None):\n",
    "        \n",
    "#         bs = x.shape[0] if self.batch_first else x.shape[1]\n",
    "        \n",
    "#         if hiddens is None:\n",
    "#             hiddens = self.get_new_hidden(bs)\n",
    "#             hiddens = to_device(hiddens, x.device)\n",
    "            \n",
    "#         new_hiddens = []\n",
    "#         for i, lstm in enumerate(self.lstms):\n",
    "#             x, (h,c) = lstm(x, hiddens[i])\n",
    "#             new_hiddens.append((h.detach(), c.detach()))\n",
    "            \n",
    "#         return x, new_hiddens\n",
    "            \n",
    "#     def get_new_hidden(self, bs):\n",
    "#         hiddens = []\n",
    "#         for hs in self.hidden_sizes:\n",
    "#             h = torch.zeros(hs).repeat(1,bs,1)\n",
    "#             c = torch.zeros(hs).repeat(1,bs,1)\n",
    "#             hiddens.append((h,c))\n",
    "        \n",
    "#         return hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Conditional_LSTM(nn.Module):\n",
    "#     def __init__(self, d_embedding, d_hidden, d_output, d_latent, n_layers,\n",
    "#                  condition_hidden=True, condition_output=True,\n",
    "#                  bidir=False, dropout=0., batch_first=True):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.d_embedding = d_embedding\n",
    "#         self.d_hidden = d_hidden\n",
    "#         self.d_output = d_output\n",
    "#         self.n_layers = n_layers\n",
    "#         self.bidir = bidir\n",
    "#         self.n_dir = 1 if not bidir else 2\n",
    "#         self.batch_first = batch_first\n",
    "#         self.condition_hidden = condition_hidden\n",
    "#         self.condition_output = condition_output\n",
    "        \n",
    "#         self.lstms = []\n",
    "#         self.hidden_sizes = []\n",
    "        \n",
    "#         for l in range(n_layers):\n",
    "#             if l==0:\n",
    "#                 input_size = d_embedding if not self.condition_output else d_embedding+d_latent\n",
    "#             else:\n",
    "#                 input_size = d_hidden\n",
    "                \n",
    "#             output_size = d_output if l==n_layers-1 else d_hidden\n",
    "#             output_size = output_size // self.n_dir\n",
    "            \n",
    "#             hidden_size = (self.n_dir, 1, output_size)\n",
    "#             self.hidden_sizes.append(hidden_size)\n",
    "            \n",
    "#             lstm = nn.LSTM(input_size, output_size, 1, batch_first=batch_first, \n",
    "#                            dropout=dropout, bidirectional=bidir)\n",
    "#             self.lstms.append(lstm)\n",
    "            \n",
    "#         self.lstms = nn.ModuleList(self.lstms)\n",
    "        \n",
    "#         if self.condition_hidden:\n",
    "#             to_hidden = []\n",
    "#             for size in self.hidden_sizes:\n",
    "#                 ndir, _, dim = size\n",
    "#                 to_hidden.append(Linear(d_latent, ndir*dim*2, act=False, bn=False))\n",
    "                \n",
    "#             self.to_hidden = nn.ModuleList(to_hidden)\n",
    "        \n",
    "#     def forward(self, x, z, hiddens=None):\n",
    "        \n",
    "#         bs = x.shape[0] if self.batch_first else x.shape[1]\n",
    "#         sl = x.shape[1] if self.batch_first else x.shape[0]\n",
    "        \n",
    "#         if self.condition_output:\n",
    "#             if self.batch_first:\n",
    "#                 z_ = z.unsqueeze(1).repeat(1,sl,1)\n",
    "#             else:\n",
    "#                 z_ = z.unsqueeze(0).repeat(sl,1,1)\n",
    "                \n",
    "#             x = torch.cat([x, z_], -1)\n",
    "\n",
    "#         if hiddens is None:\n",
    "#             if self.condition_hidden:\n",
    "#                 hiddens = self.latent_to_hidden(z)\n",
    "                \n",
    "#             else:\n",
    "#                 hiddens = self.get_new_hidden(bs)\n",
    "            \n",
    "#             hiddens = to_device(hiddens, x.device)\n",
    "            \n",
    "#         new_hiddens = []\n",
    "#         for i, lstm in enumerate(self.lstms):\n",
    "#             x, (h,c) = lstm(x, hiddens[i])\n",
    "#             new_hiddens.append((h.detach(), c.detach()))\n",
    "            \n",
    "#         return x, new_hiddens\n",
    "    \n",
    "#     def latent_to_hidden(self, z):\n",
    "#         hiddens = []\n",
    "#         for layer in self.to_hidden:\n",
    "#             h = layer(z)\n",
    "#             h,c = torch.chunk(h, 2, dim=-1)\n",
    "#             bs, _ = h.shape\n",
    "#             h = h.contiguous().reshape(bs, self.n_dir, -1).permute(1,0,2)\n",
    "#             c = c.contiguous().reshape(bs, self.n_dir, -1).permute(1,0,2)\n",
    "#             hiddens.append((h,c))\n",
    "            \n",
    "#         return hiddens\n",
    "            \n",
    "#     def get_new_hidden(self, bs):\n",
    "#         hiddens = []\n",
    "#         for hs in self.hidden_sizes:\n",
    "#             h = torch.zeros(hs).repeat(1,bs,1)\n",
    "#             c = torch.zeros(hs).repeat(1,bs,1)\n",
    "#             hiddens.append((h,c))\n",
    "        \n",
    "#         return hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTMLM(nn.Module):\n",
    "#     def __init__(self, d_vocab, d_embedding, d_hidden, n_layers, \n",
    "#                  lstm_drop=0., bos_idx=0, bidir=False):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.embedding = nn.Embedding(d_vocab, d_embedding)\n",
    "#         self.lstm = LSTM(d_embedding, d_hidden, d_embedding, n_layers, bidir=bidir, dropout=lstm_drop)\n",
    "#         self.head = Linear(d_embedding, d_vocab, act=False, bn=False, dropout=0.)\n",
    "#         self.bos_idx = bos_idx\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.embedding(x)\n",
    "#         x, hiddens = self.lstm(x)\n",
    "#         self.last_hidden = hiddens\n",
    "#         x = self.head(x)\n",
    "#         return x\n",
    "    \n",
    "#     def sample(self, bs, sl, temperature=1., multinomial=True):\n",
    "        \n",
    "#         preds = idxs = to_device(torch.tensor([self.bos_idx]*bs).long().unsqueeze(-1))\n",
    "#         lps = []\n",
    "\n",
    "#         hiddens = None\n",
    "        \n",
    "#         for i in range(sl):\n",
    "#             x = self.embedding(idxs)\n",
    "#             x, hiddens = self.lstm(x, hiddens)\n",
    "#             x = self.head(x)\n",
    "            \n",
    "#             x.div_(temperature)\n",
    "            \n",
    "#             log_probs = F.log_softmax(x, -1).squeeze(1)\n",
    "#             probs = log_probs.detach().exp()\n",
    "            \n",
    "#             if multinomial:\n",
    "#                 idxs = torch.multinomial(probs, 1)\n",
    "#             else:\n",
    "#                 idxs = x.argmax(-1)\n",
    "                \n",
    "#             lps.append(torch.gather(log_probs, 1, idxs))\n",
    "            \n",
    "#             preds = torch.cat([preds, idxs], -1)\n",
    "            \n",
    "#         return preds[:, 1:], torch.cat(lps,-1)\n",
    "    \n",
    "#     def sample_no_grad(self, bs, sl, temperature=1., multinomial=True):\n",
    "#         with torch.no_grad():\n",
    "#             return self.sample(bs, sl, temperature=temperature, multinomial=multinomial)\n",
    "        \n",
    "#     def get_lps(self, x, y, temperature=1.):\n",
    "#         x = self.forward(x)\n",
    "#         x.div_(temperature)\n",
    "        \n",
    "#         lps = F.log_softmax(x, -1)\n",
    "#         lps = lps.gather(2, y.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "#         return lps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Conditional_LSTMLM(nn.Module):\n",
    "#     def __init__(self, d_vocab, d_embedding, d_hidden, n_layers, mapping, d_latent,\n",
    "#                  lstm_drop=0., lin_drop=0., bos_idx=0, bidir=False,\n",
    "#                  condition_hidden=True, condition_output=False, norm_latent=True):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.mapping = mapping\n",
    "#         self.d_latent = d_latent\n",
    "        \n",
    "#         self.embedding = nn.Embedding(d_vocab, d_embedding)\n",
    "#         self.lstm = Conditional_LSTM(d_embedding, d_hidden, d_embedding, d_latent, n_layers,\n",
    "#                                     condition_hidden=condition_hidden, condition_output=condition_output,\n",
    "#                                      bidir=bidir, dropout=lstm_drop)\n",
    "        \n",
    "#         self.head = Linear(d_embedding, d_vocab, act=False, bn=False, dropout=0.)\n",
    "#         self.bos_idx = bos_idx\n",
    "#         self.norm_latent = norm_latent\n",
    "        \n",
    "#     def forward(self, x, condition):\n",
    "        \n",
    "#         z = self.mapping(condition)\n",
    "#         if self.norm_latent:\n",
    "#             z = F.normalize(z, p=2, dim=-1)\n",
    "        \n",
    "#         x = self.embedding(x)\n",
    "#         x, hiddens = self.lstm(x,z)\n",
    "#         self.last_hidden = hiddens\n",
    "#         x = self.head(x)\n",
    "#         return x\n",
    "    \n",
    "#     def sample(self, bs, sl, z=None, temperature=1., multinomial=True):\n",
    "        \n",
    "#         if z is None:\n",
    "#             z = to_device(F.normalize(torch.randn((bs, self.d_latent)), p=2, dim=-1))\n",
    "#         else:\n",
    "#             bs = z.shape[0]\n",
    "        \n",
    "#         preds = idxs = to_device(torch.tensor([self.bos_idx]*bs).long().unsqueeze(-1))\n",
    "#         lps = []\n",
    "\n",
    "#         hiddens = None\n",
    "        \n",
    "#         for i in range(sl):\n",
    "#             x = self.embedding(idxs)\n",
    "#             x, hiddens = self.lstm(x, z, hiddens)\n",
    "#             x = self.head(x)\n",
    "            \n",
    "#             x.div_(temperature)\n",
    "            \n",
    "#             log_probs = F.log_softmax(x, -1).squeeze(1)\n",
    "#             probs = log_probs.detach().exp()\n",
    "            \n",
    "#             if multinomial:\n",
    "#                 idxs = torch.multinomial(probs, 1)\n",
    "#             else:\n",
    "#                 idxs = x.argmax(-1)\n",
    "                \n",
    "#             lps.append(torch.gather(log_probs, 1, idxs))\n",
    "            \n",
    "#             preds = torch.cat([preds, idxs], -1)\n",
    "            \n",
    "#         return preds[:, 1:], torch.cat(lps,-1)\n",
    "    \n",
    "#     def sample_no_grad(self, bs, sl, z=None, temperature=1., multinomial=True):\n",
    "#         with torch.no_grad():\n",
    "#             return self.sample(bs, sl, z=z, temperature=temperature, multinomial=multinomial)\n",
    "        \n",
    "#     def get_lps(self, x, y, temperature=1.):\n",
    "#         x = self.forward(x[0], x[1])\n",
    "#         x.div_(temperature)\n",
    "        \n",
    "#         lps = F.log_softmax(x, -1)\n",
    "#         lps = lps.gather(2, y.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "#         return lps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VAEEncoder(nn.Module):\n",
    "#     def __init__(self, d_latent):\n",
    "#         super().__init__()\n",
    "#         self.d_latent = d_latent\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         raise NotImplementedError\n",
    "        \n",
    "#     def get_latent(self, mu, logvar, z_scale=1.):\n",
    "#         z = z_scale*torch.randn(mu.shape).to(mu.device)\n",
    "#         z = mu + z*torch.exp(0.5*logvar)\n",
    "#         kl_loss = 0.5 * (logvar.exp() + mu.pow(2) - 1 - logvar).sum(1).mean()\n",
    "#         return z, kl_loss\n",
    "        \n",
    "# class VAELSTMEncoder(VAEEncoder):\n",
    "#     def __init__(self, d_vocab, d_embedding, d_hidden, n_layers, d_latent, dropout=0.):\n",
    "#         super().__init__(d_latent)\n",
    "        \n",
    "#         self.embedding = nn.Embedding(d_vocab, d_embedding)\n",
    "#         self.lstm_encoder = LSTM(d_embedding, d_hidden, d_hidden, n_layers, \n",
    "#                                  bidir=True, batch_first=True, dropout=dropout)\n",
    "#         self.transition = nn.Linear(d_hidden*2, d_latent*2)\n",
    "        \n",
    "        \n",
    "#     def forward(self, x, z_scale=1.):\n",
    "#         x = self.embedding(x)\n",
    "#         x, hiddens = self.lstm_encoder(x)\n",
    "#         hidden = torch.cat(list(torch.cat(hiddens[-1], -1)), -1) # concatenate hidden/cell states of last layer\n",
    "        \n",
    "#         mu, logvar = torch.chunk(self.transition(hidden), 2, dim=-1)\n",
    "#         z, kl_loss = self.get_latent(mu, logvar, z_scale)\n",
    "        \n",
    "#         return z, kl_loss\n",
    "              \n",
    "# class VAEConvEncoder(VAEEncoder):\n",
    "#     def __init__(self, d_vocab, d_embedding, kernel_size, n_layers, d_latent, dropout=0.):\n",
    "#         super().__init__(d_latent)\n",
    "    \n",
    "#         self.embedding = nn.Embedding(d_vocab, d_embedding)\n",
    "\n",
    "#         convs = []\n",
    "#         input_size = d_embedding\n",
    "#         for i in range(n_layers):\n",
    "#             convs.append(Conv1d(input_size, input_size*2, ks=kernel_size, stride=2, \n",
    "#                                 act=True, bn=True, dropout=dropout))\n",
    "#             input_size = input_size*2\n",
    "\n",
    "#         self.convs = nn.Sequential(*convs)\n",
    "#         self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "#         self.transition = nn.Linear(input_size, d_latent*2)\n",
    "    \n",
    "#     def forward(self, x, z_scale=1.):\n",
    "#         x = self.embedding(x)\n",
    "#         x = x.permute(0,2,1)\n",
    "#         x = self.convs(x)\n",
    "#         x = self.pool(x).squeeze(-1)\n",
    "        \n",
    "#         mu, logvar = torch.chunk(self.transition(x), 2, dim=-1)\n",
    "#         z, kl_loss = self.get_latent(mu, logvar, z_scale)\n",
    "        \n",
    "#         return z, kl_loss\n",
    "              \n",
    "# class VAELinEncoder(VAEEncoder):\n",
    "#     def __init__(self, d_input, n_layers, d_latent, dropout=0.):\n",
    "#         super().__init__(d_latent)\n",
    "    \n",
    "#         lins = []\n",
    "#         input_size = d_input\n",
    "#         for i in range(n_layers):\n",
    "#             lins.append(Linear(input_size, input_size//2, act=True, bn=True, dropout=dropout))\n",
    "#             input_size = input_size//2\n",
    "            \n",
    "#         self.layers = nn.Sequential(*lins)\n",
    "#         self.transition = nn.Linear(input_size, d_latent*2)\n",
    "    \n",
    "#     def forward(self, x, z_scale=1.):\n",
    "#         x = self.layers(x)\n",
    "        \n",
    "#         mu, logvar = torch.chunk(self.transition(x), 2, dim=-1)\n",
    "#         z, kl_loss = self.get_latent(mu, logvar, z_scale)\n",
    "        \n",
    "#         return z, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VAEDecoder(nn.Module):\n",
    "#     def __init__(self, d_vocab, d_embedding, d_hidden, n_layers, d_latent,\n",
    "#                 condition_hidden=True, condition_output=True):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.embedding = nn.Embedding(d_vocab, d_embedding)\n",
    "#         self.decoder = Conditional_LSTM(d_embedding, d_hidden, d_embedding, d_latent, 3, \n",
    "#                                     condition_hidden=condition_hidden, condition_output=condition_output, \n",
    "#                                     bidir=False, batch_first=True)\n",
    "        \n",
    "#         self.head = Linear(d_embedding, d_vocab, act=False, bn=False, dropout=0.)\n",
    "        \n",
    "#     def forward(self, x, z, hiddens=None):\n",
    "#         bs, sl = x.shape\n",
    "#         x = self.embedding(x)\n",
    "        \n",
    "#         decoded, hiddens = self.decoder(x, z, hiddens)\n",
    "#         output = self.head(decoded)\n",
    "        \n",
    "#         return output, hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VAE(nn.Module):\n",
    "#     def __init__(self, encoder, decoder, prior=None, bos_idx=0):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.encoder = encoder\n",
    "#         self.decoder = decoder\n",
    "#         if prior is None:\n",
    "#             prior = Normal(torch.zeros((encoder.d_latent)), torch.ones((encoder.d_latent)))\n",
    "#         self.prior = prior\n",
    "#         self.bos_idx = bos_idx\n",
    "        \n",
    "#     def forward(self, x, decoder_input=None):\n",
    "#         z, kl_loss = self.encoder(x)\n",
    "        \n",
    "#         if decoder_input is None:\n",
    "#             decoder_input = x\n",
    "            \n",
    "#         output, hiddens = self.decoder(decoder_input, z)\n",
    "#         return output, kl_loss\n",
    "    \n",
    "#     def sample(self, bs, sl, z=None, temperature=1., multinomial=True):\n",
    "        \n",
    "#         preds = idxs = to_device(torch.tensor([self.bos_idx]*bs).long().unsqueeze(-1))\n",
    "#         lps = []\n",
    "        \n",
    "#         if z is None:\n",
    "#             z = to_device(self.prior.sample([bs]))\n",
    "        \n",
    "#         hiddens = None\n",
    "        \n",
    "#         for i in range(sl):\n",
    "#             x, hiddens = self.decoder(idxs, z, hiddens)\n",
    "#             x.div_(temperature)\n",
    "            \n",
    "#             log_probs = F.log_softmax(x, -1).squeeze(1)\n",
    "#             probs = log_probs.detach().exp()\n",
    "            \n",
    "#             if multinomial:\n",
    "#                 idxs = torch.multinomial(probs, 1)\n",
    "#             else:\n",
    "#                 idxs = x.argmax(-1)\n",
    "                \n",
    "#             lps.append(torch.gather(log_probs, 1, idxs))\n",
    "            \n",
    "#             preds = torch.cat([preds, idxs], -1)\n",
    "            \n",
    "#         return preds[:, 1:], torch.cat(lps,-1)\n",
    "    \n",
    "#     def sample_no_grad(self, bs, sl, z=None, temperature=1., multinomial=True):\n",
    "#         with torch.no_grad():\n",
    "#             return self.sample(bs, sl, z=z, temperature=temperature, multinomial=multinomial)\n",
    "        \n",
    "#     def get_lps(self, x, y, temperature=1., z=None):\n",
    "\n",
    "#         if type(x)==list:\n",
    "#             x,_ = self.forward(x[0], decoder_input=x[1])\n",
    "#         else:\n",
    "#             x,_ = self.forward(x)\n",
    "        \n",
    "#         x.div_(temperature)\n",
    "        \n",
    "#         lps = F.log_softmax(x, -1)\n",
    "#         lps = lps.gather(2, y.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "#         return lps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_chem.ipynb.\n",
      "Converted 02_template.filters.ipynb.\n",
      "Converted 03_template.template.ipynb.\n",
      "Converted 04_template.blocks.ipynb.\n",
      "Converted 05_torch_core.ipynb.\n",
      "Converted 06_layers.ipynb.\n",
      "Converted 07_dataloaders.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted template.overview.ipynb.\n",
      "Converted tutorials.ipynb.\n",
      "Converted tutorials.structure_enumeration.ipynb.\n",
      "Converted tutorials.template.advanced.ipynb.\n",
      "Converted tutorials.template.beginner.ipynb.\n",
      "Converted tutorials.template.intermediate.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
