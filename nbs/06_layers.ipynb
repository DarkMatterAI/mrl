{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp layers\n",
    "# all_slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "\n",
    "> Pytorch model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from mrl.imports import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, d_in, d_out, act=True, bn=False, dropout=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = [nn.Linear(d_in, d_out)]\n",
    "        \n",
    "        if bn:\n",
    "            layers.append(nn.BatchNorm1d(d_out))\n",
    "            \n",
    "        if act:\n",
    "            layers.append(nn.ReLU())\n",
    "            \n",
    "        if dropout>0.:\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, d_embedding, d_hidden, d_output, n_layers, \n",
    "                 bidir=False, dropout=0., batch_first=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_embedding = d_embedding\n",
    "        self.d_hidden = d_hidden\n",
    "        self.d_output = d_output\n",
    "        self.n_layers = n_layers\n",
    "        self.bidir = bidir\n",
    "        self.n_dir = 1 if not bidir else 2\n",
    "        self.batch_first = batch_first\n",
    "        \n",
    "        self.lstms = []\n",
    "        self.hidden_sizes = []\n",
    "        \n",
    "        for l in range(n_layers):\n",
    "            input_size = d_embedding if l==0 else d_hidden\n",
    "            output_size = d_output if l==n_layers-1 else d_hidden\n",
    "            \n",
    "            hidden_size = (self.n_dir, 1, output_size)\n",
    "            self.hidden_sizes.append(hidden_size)\n",
    "            \n",
    "            lstm = nn.LSTM(input_size, output_size, 1, batch_first=batch_first, \n",
    "                           dropout=dropout, bidirectional=bidir)\n",
    "            self.lstms.append(lstm)\n",
    "            \n",
    "        self.lstms = nn.ModuleList(self.lstms)\n",
    "        \n",
    "    def forward(self, x, hiddens=None):\n",
    "        \n",
    "        bs = x.shape[0] if self.batch_first else x.shape[1]\n",
    "        \n",
    "        if hiddens is None:\n",
    "            hiddens = self.get_new_hidden(bs)\n",
    "            hiddens = to_device(hiddens, x.device)\n",
    "            \n",
    "        new_hiddens = []\n",
    "        for i, lstm in enumerate(self.lstms):\n",
    "            x, (h,c) = lstm(x, hiddens[i])\n",
    "            new_hiddens.append((h.detach(), c.detach()))\n",
    "            \n",
    "        return x, new_hiddens\n",
    "            \n",
    "    def get_new_hidden(self, bs):\n",
    "        hiddens = []\n",
    "        for hs in self.hidden_sizes:\n",
    "            h = torch.zeros(hs).repeat(1,bs,1)\n",
    "            c = torch.zeros(hs).repeat(1,bs,1)\n",
    "            hiddens.append((h,c))\n",
    "        \n",
    "        return hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LSTMLM(nn.Module):\n",
    "    def __init__(self, d_vocab, d_embedding, d_hidden, n_layers, pad_idx, \n",
    "                 lstm_drop=0., bos_idx=0, bidir=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(d_vocab, d_embedding)\n",
    "        self.lstm = LSTM(d_embedding, d_hidden, d_embedding, n_layers, bidir=bidir, dropout=lstm_drop)\n",
    "        self.head = Linear(d_embedding, d_vocab, act=False, bn=False, dropout=0.)\n",
    "        self.bos_idx = bos_idx\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, hiddens = self.lstm(x)\n",
    "        self.last_hidden = hiddens\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "    \n",
    "    def sample(self, bs, sl, temperature=1., multinomial=True):\n",
    "        \n",
    "        preds = idxs = to_device(torch.tensor([self.bos_idx]*bs).long().unsqueeze(-1))\n",
    "        lps = []\n",
    "\n",
    "        hiddens = to_device(self.lstm.get_new_hidden(bs))\n",
    "        \n",
    "        for i in range(sl):\n",
    "            x = self.embedding(idxs)\n",
    "            x, hiddens = self.lstm(x, hiddens)\n",
    "            x = self.head(x)\n",
    "            \n",
    "            x.div_(temperature)\n",
    "            \n",
    "            log_probs = F.log_softmax(x, -1).squeeze(1)\n",
    "            probs = log_probs.detach().exp()\n",
    "            \n",
    "            if multinomial:\n",
    "                idxs = torch.multinomial(probs, 1)\n",
    "            else:\n",
    "                idxs = x.argmax(-1)\n",
    "                \n",
    "            lps.append(torch.gather(log_probs, 1, idxs))\n",
    "            \n",
    "            preds = torch.cat([preds, idxs], -1)\n",
    "            \n",
    "        return preds[:, 1:], torch.cat(lps,-1)\n",
    "    \n",
    "    def sample_no_grad(self, bs, sl, temperature=1., multinomial=True):\n",
    "        with torch.no_grad():\n",
    "            return self.sample(bs, sl, temperature=temperature, multinomial=multinomial)\n",
    "        \n",
    "    def get_lps(self, x, y, temperature=1.):\n",
    "        x = self.forward(x)\n",
    "        x.div_(temperature)\n",
    "        \n",
    "        lps = F.log_softmax(x, -1)\n",
    "        lps = lps.gather(2, y.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        return lps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/envs/mrl/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for boost::shared_ptr<RDKit::FilterCatalogEntry const> already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from mrl.dataloaders import *\n",
    "from mrl.chem import *\n",
    "from mrl.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = CharacterVocab(SMILES_CHAR_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMLM(len(vocab.itos), 256, 1024, 3, vocab.stoi['pad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('untracked_files/smiles_lm.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrl.templates import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = Template([ValidityFilter(), SingleCompoundFilter()],\n",
    "                    [QEDFilter(None, None, score=PassThroughScore())], fail_score=-1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, lps = model.sample_no_grad(32,90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = [vocab.reconstruct(i) for i in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CCc1cnccc1C(=O)NCCN(C)Cc1cccc(C(F)F)c1'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = TextDataset(smiles, vocab)\n",
    "x,y = ds.collate_function([ds.__getitem__(i) for i in range(len(ds))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_chem.ipynb.\n",
      "Converted 02_template.filters.ipynb.\n",
      "Converted 03_template.template.ipynb.\n",
      "Converted 04_template.blocks.ipynb.\n",
      "Converted 05_torch_core.ipynb.\n",
      "Converted 06_layers.ipynb.\n",
      "Converted 07_dataloaders.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted template.overview.ipynb.\n",
      "Converted tutorials.ipynb.\n",
      "Converted tutorials.structure_enumeration.ipynb.\n",
      "Converted tutorials.template.advanced.ipynb.\n",
      "Converted tutorials.template.beginner.ipynb.\n",
      "Converted tutorials.template.intermediate.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
