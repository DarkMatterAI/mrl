{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "\n",
    "> Pytorch model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from mrl.imports import *\n",
    "from mrl.torch_imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, d_in, d_out, act=True, bn=False, dropout=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = [nn.Linear(d_in, d_out)]\n",
    "        \n",
    "        if bn:\n",
    "            layers.append(nn.BatchNorm1d(d_out))\n",
    "            \n",
    "        if act:\n",
    "            layers.append(nn.ReLU())\n",
    "            \n",
    "        if dropout>0.:\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LSTMBase(nn.Module):\n",
    "    def __init__(self, d_vocab, d_embedding, d_hidden, n_layers, pad_idx):\n",
    "        super().__init()\n",
    "        \n",
    "        self.d_vocab = d_vocab\n",
    "        self.d_embedding = d_embedding\n",
    "        self.d_hidden = d_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        self.embedding = nn.Embedding(d_vocab, d_embedding, padding_idx=pad_idx)\n",
    "        \n",
    "        self.lstms = []\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            input_size = d_embedding if i==0 else d_hidden\n",
    "            output_side = d_embedding if i==n_layers-1 else d_hidden\n",
    "                \n",
    "            lstm = nn.LSTM(input_size, output_size, 1, batch_first=True)\n",
    "            self.lstms.append(lstm)\n",
    "            \n",
    "        self.lstms = nn.ModuleList(self.lstms)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        hiddens = []\n",
    "        for i, lstm in enumerate(self.lstms):\n",
    "            x, (h,c) = lstm(x)\n",
    "            hiddens.append((h.detach(), c.detach()))\n",
    "        \n",
    "        self.last_hiddens = hiddens\n",
    "\n",
    "        return x\n",
    "    \n",
    "class LMHead(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer = nn.Linear(d_in, d_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
