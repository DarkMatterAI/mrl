{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders\n",
    "\n",
    "> Pytorch datasets, dataloaders, collate functions and vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from mrl.imports import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *\n",
    "from mrl.vocab import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collate Functions\n",
    "\n",
    "Collate functions are used to batch `Dataset` outputs into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def batch_sequences(sequences, pad_idx):\n",
    "    'Packs `sequences` into a dense tensor, using `pad_idx` for padding'\n",
    "    max_len = max([len(i) for i in sequences])+1\n",
    "    bs = len(sequences)\n",
    "    \n",
    "    batch_tensor = torch.zeros((bs, max_len)).long() + pad_idx\n",
    "    \n",
    "    for i,item in enumerate(sequences):\n",
    "        batch_tensor[i,:item.shape[0]] = item\n",
    "        \n",
    "    return batch_tensor\n",
    "    \n",
    "    \n",
    "def lm_collate(batch, pad_idx, batch_first=True):\n",
    "    '''\n",
    "    Collate function for language models. Returns packed \n",
    "    batch for next-token prediction\n",
    "    '''\n",
    "    \n",
    "    x_tensor = batch_sequences([i[0] for i in batch], pad_idx)\n",
    "    \n",
    "    if isinstance(batch[0][1], torch.Tensor):\n",
    "        y_tensor = batch_sequences([i[1] for i in batch], pad_idx)\n",
    "    else:\n",
    "        y_tensor = x_tensor\n",
    "        \n",
    "    if batch_first:\n",
    "        output = (x_tensor[:,:-1], y_tensor[:,1:])\n",
    "    else:\n",
    "        x_tensor = x_tensor.T\n",
    "        y_tensor = y_tensor.T\n",
    "        output = (x_tensor[:-1,:], y_tensor[1:,:])\n",
    "        \n",
    "    return output\n",
    "\n",
    "def sequence_prediction_collate(batch, pad_idx, batch_first=True):\n",
    "    '''\n",
    "    Collate function for predicting some y value from a sequence\n",
    "    '''\n",
    "    batch_tensor = batch_sequences([i[0] for i in batch], pad_idx)\n",
    "    y_vals = torch.stack([i[1] for i in batch])\n",
    "    y_vals = y_vals.squeeze(-1)\n",
    "\n",
    "    if not batch_first:\n",
    "        batch_tensor = batch_tensor.T\n",
    "        \n",
    "    return (batch_tensor, y_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def vector_collate(batch):\n",
    "    '''\n",
    "    Collate function for vectors\n",
    "    '''\n",
    "    fps = torch.stack(batch)\n",
    "    return fps\n",
    "\n",
    "def vec_to_text_collate(batch, pad_idx, batch_first=True):\n",
    "    '''\n",
    "    Collate function for predicting a sequence from an input vector where \n",
    "    `batch_tensor` is needed for input (ie predict SMILES from properties)\n",
    "    '''\n",
    "    fps = torch.stack([i[0] for i in batch])\n",
    "    batch_tensor = batch_sequences([i[1] for i in batch], pad_idx)\n",
    "    \n",
    "    if batch_first:\n",
    "        output = ((batch_tensor[:,:-1], fps), batch_tensor[:,1:])\n",
    "    else:\n",
    "        batch_tensor = batch_tensor.T\n",
    "        output = ((batch_tensor[:-1,:], fps), batch_tensor[1:,:])\n",
    "        \n",
    "    return output\n",
    "\n",
    "def vector_prediction_collate(batch):\n",
    "    '''\n",
    "    Collate function for predicting some y value from a vector\n",
    "    '''\n",
    "    fps = torch.stack([i[0] for i in batch])\n",
    "    y_vals = torch.stack([i[1] for i in batch])\n",
    "    y_vals = y_vals.squeeze(-1)\n",
    "    return (fps, y_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "Datasets subclass the Pytorch `Dataset` class. MRL datasets add a collate function and the `Base_Dataset.dataloader` function to easily generate Pytorch dataloaders from the same class\n",
    "\n",
    "Like all Pytorch datasets, subclass datasets must contain a valid `__len__` and `__getitem__` method. MRL datasets should aalso include a `new` method. \n",
    "\n",
    "The purpose of `new` is to create a new dataset from new data using the same input arguments and collate function as the current dataset. This is used during generative training to process and batch generated samples to ensure they are processed and batched the same as training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Base_Dataset(Dataset):\n",
    "    '''\n",
    "    BaseDataset - base dataset\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `collate_function Callable`: batch collate function for the particular dataset class\n",
    "    '''\n",
    "    def __init__(self, collate_function):\n",
    "        self.collate_function = collate_function\n",
    "        \n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def dataloader(self, bs, num_workers=-1, **dl_kwargs):\n",
    "        if num_workers==-1:\n",
    "            if 'ncpus' in os.environ.keys():\n",
    "                num_workers = int(os.environ['ncpus'])\n",
    "            else:\n",
    "                num_workers=os.cpu_count()\n",
    "                \n",
    "        return DataLoader(self, batch_size=bs, num_workers=num_workers, \n",
    "                          collate_fn=self.collate_function, **dl_kwargs)\n",
    "    \n",
    "    def new(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def split(self, percent_valid, seed=0):\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        idxs = torch.randperm(self.__len__()).numpy()\n",
    "        train_length = int(self.__len__()*(1-percent_valid))\n",
    "        \n",
    "        train_idxs = idxs[:train_length]\n",
    "        valid_idxs = idxs[train_length:]\n",
    "        \n",
    "        return self.split_on_idxs(train_idxs, valid_idxs)\n",
    "        \n",
    "    def split_on_idxs(self, train_idxs, valid_idxs):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Datasets\n",
    "\n",
    "Text datasets deal with tokenizing and numericalizing text data, like SMILES strings. \n",
    "\n",
    "`Text_Dataset` returns numericalized SMILES for language modeling.\n",
    "\n",
    "`Text_Prediction_Dataset` returns numericaized SMILES along with some `y_val` output value, for tasks like property prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Text_Dataset(Base_Dataset):\n",
    "    '''\n",
    "    Text_Dataset - base dataset for language modes\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `sequences [list[str], list[tuple]]`: list of text sequences or text tuples (source, target)\n",
    "        \n",
    "    - `vocab Vocab`: vocabuary for tokenization/numericaization\n",
    "        \n",
    "    - `collate_function Callable`: batch collate function. If None, defauts to `lm_collate`\n",
    "        \n",
    "    If `sequences` is a list of strings, `__getitem__` returns a tuple of `(sequence_ints, None)`. \n",
    "    This is suitable for language modeling where the goal is to predict the input sequence.\n",
    "    \n",
    "    If `sequences` is a list of tuples, `__getitem__` returns a tuple of \n",
    "    `(input_sequence_ints, output_sequence_ints)`. This is suitable for seq-to-seq tasks where \n",
    "    the predicted sequence is different from the input sequence\n",
    "    '''\n",
    "    def __init__(self, sequences, vocab, collate_function=None):\n",
    "        self.sequences = sequences\n",
    "        self.vocab = vocab\n",
    "        if collate_function is None:\n",
    "            collate_function = partial(lm_collate, pad_idx=self.vocab.stoi['pad'])\n",
    "        \n",
    "        super().__init__(collate_function)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def numericalize(self, sequence):\n",
    "        tokens = self.vocab.tokenize(sequence)\n",
    "        ints = self.vocab.numericalize(tokens)\n",
    "        ints = torch.LongTensor(ints)\n",
    "        return ints\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        \n",
    "        if type(sequence)==tuple:\n",
    "            outputs = (self.numericalize(sequence[0]),\n",
    "                       self.numericalize(sequence[1]))\n",
    "        else:\n",
    "            outputs = (self.numericalize(sequence), None)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def new(self, sequences):\n",
    "        return self.__class__(sequences, self.vocab, self.collate_function)\n",
    "    \n",
    "    def split_on_idxs(self, train_idxs, valid_idxs):\n",
    "        \n",
    "        train_ds = self.new([self.sequences[i] for i in train_idxs])\n",
    "        valid_ds = self.new([self.sequences[i] for i in valid_idxs])\n",
    "        return (train_ds, valid_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('files/smiles.csv')\n",
    "vocab = CharacterVocab(SMILES_CHAR_VOCAB)\n",
    "\n",
    "ds = Text_Dataset(df.smiles.values, vocab)\n",
    "dl = ds.dataloader(16, num_workers=0)\n",
    "x,y = next(iter(dl))\n",
    "\n",
    "assert (x[:,1:] == y[:,:-1]).all()\n",
    "\n",
    "assert sum([len(i) for i in ds.split(0.2)]) == len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Text_Dataset([(i,i) for i in df.smiles.values], vocab)\n",
    "dl = ds.dataloader(16, num_workers=0)\n",
    "x,y = next(iter(dl))\n",
    "\n",
    "assert (x[:,1:] == y[:,:-1]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Text_Prediction_Dataset(Text_Dataset):\n",
    "    '''\n",
    "    Text_Prediction_Dataset - base dataset for predicting from text strings\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `sequences list[str]`: list of text sequences\n",
    "\n",
    "    - `y_vals list[int, float]`: list of paired output values\n",
    "\n",
    "    - `vocab Vocab`: vocabuary for tokenization/numericaization\n",
    "\n",
    "    - `collate_function Callable`: batch collate function. If None, defauts to `sequence_prediction_collate`\n",
    "        \n",
    "    `__getitem__` returns a tuple of `(sequence_ints, y_vals)` suitable for predicting \n",
    "    regressions or classifications from the sequence\n",
    "    '''\n",
    "    def __init__(self, sequences, y_vals, vocab, collate_function=None):\n",
    "        \n",
    "        if collate_function is None:\n",
    "            collate_function = partial(sequence_prediction_collate, pad_idx=vocab.stoi['pad'])\n",
    "        \n",
    "        super().__init__(sequences, vocab, collate_function)\n",
    "        \n",
    "        self.y_vals = y_vals\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        ints = super().__getitem__(idx)[0]\n",
    "        y_val = torch.Tensor([self.y_vals[idx]]).float()\n",
    "        return (ints, y_val)\n",
    "    \n",
    "    def new(self, sequences, y_vals):\n",
    "        return self.__class__(sequences, y_vals, self.vocab, self.collate_function)\n",
    "    \n",
    "    def split_on_idxs(self, train_idxs, valid_idxs):\n",
    "        \n",
    "        train_ds = self.new([self.sequences[i] for i in train_idxs],\n",
    "                            [self.y_vals[i] for i in train_idxs])\n",
    "        valid_ds = self.new([self.sequences[i] for i in valid_idxs],\n",
    "                            [self.y_vals[i] for i in valid_idxs])\n",
    "        \n",
    "        return (train_ds, valid_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Text_Prediction_Dataset(df.smiles.values, [0]*len(df.smiles.values), vocab)\n",
    "dl = ds.dataloader(16, num_workers=0)\n",
    "x,y = next(iter(dl))\n",
    "assert (y == torch.zeros(y.shape).float()).all()\n",
    "assert sum([len(i) for i in ds.split(0.2)]) == len(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Datasets\n",
    "\n",
    "Another common dataset framework is where we are dealing with vectors derived from a molecule. This could be a vector of properties, fingerprints, or any task where a molecule-derived vector is needed.\n",
    "\n",
    "`Vector_Dataset` is a base dataset that simply returns the molecule derived vector\n",
    "\n",
    "`Vec_Recon_Dataset` returns the molecule derived vector and tokenized SMILES strings. This is used for tasks like generating compounds based on an input vector or fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Vector_Dataset(Base_Dataset):\n",
    "    '''\n",
    "    Vector_Dataset - base dataset for molecule-derived vectors\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `sequences list[str]`: list of text sequences\n",
    "\n",
    "    - `vec_function Callable`: function to convert sequence to a vector\n",
    "\n",
    "    - `collate_function Callable`: batch collate function. If None, defauts to `vector_collate`\n",
    "    '''\n",
    "    def __init__(self, sequences, vec_function, collate_function=None):\n",
    "        if collate_function is None:\n",
    "            collate_function = vector_collate\n",
    "        super().__init__(collate_function)\n",
    "        \n",
    "        self.sequences = sequences\n",
    "        self.vec_function = vec_function\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        vec = self.vec_function(sequence)\n",
    "        vec = torch.FloatTensor(vec)\n",
    "        return vec\n",
    "    \n",
    "    def new(self, sequences):\n",
    "        return self.__class__(sequences, self.vec_function, self.collate_function)\n",
    "    \n",
    "    def split_on_idxs(self, train_idxs, valid_idxs):\n",
    "        \n",
    "        train_ds = self.new([self.sequences[i] for i in train_idxs])\n",
    "        valid_ds = self.new([self.sequences[i] for i in valid_idxs])\n",
    "        \n",
    "        return (train_ds, valid_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmai/miniconda3/envs/mrl/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for boost::shared_ptr<RDKit::FilterCatalogEntry const> already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# rdkit\n",
    "\n",
    "from mrl.chem import ECFP6\n",
    "\n",
    "df = pd.read_csv('files/smiles.csv')\n",
    "ds = Vector_Dataset(df.smiles.values, ECFP6)\n",
    "dl = ds.dataloader(16, num_workers=0)\n",
    "batch = next(iter(dl))\n",
    "new1, new2 = ds.split(0.1)\n",
    "\n",
    "x = next(iter(dl))\n",
    "assert x.shape==(16,2048)\n",
    "assert sum([len(i) for i in ds.split(0.2)]) == len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Vec_To_Text_Dataset(Vector_Dataset):\n",
    "    '''\n",
    "    Vec_To_Text_Dataset - base dataset for predicting text sequences from vectors\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `sequences [list[str], list[tuple]]`: list of text sequences or text tuples (source, target)\n",
    "\n",
    "    - `vocab Vocab`: vocabuary for tokenization/numericaization\n",
    "\n",
    "    - `vec_function Callable`: function to convert a sequence to a vector\n",
    "\n",
    "    - `collate_function Callable`: batch collate function. If None, defauts to `vec_to_text_collate`\n",
    "        \n",
    "    `__getitem__` returns a tuple of `(sequence_vector, sequence_ints)`. \n",
    "    \n",
    "    If `sequences` is a list of strings, both `sequence_vector` and `sequence_ints` \n",
    "    will be derived from the same sequence.\n",
    "    \n",
    "    If `sequences` is a list of tuples, `sequence_vector` will be derived from the first sequence \n",
    "    and `sequence_ints` will be derived from the second sequence\n",
    "    '''\n",
    "    def __init__(self, sequences, vocab, vec_function, collate_function=None):\n",
    "        \n",
    "        if collate_function is None:\n",
    "            collate_function = partial(vec_to_text_collate, pad_idx=vocab.stoi['pad'])\n",
    "            \n",
    "        super().__init__(sequences, vec_function, collate_function)\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        \n",
    "        if type(sequence)==tuple:\n",
    "            source_sequence = sequence[0]\n",
    "            target_sequence = sequence[1]\n",
    "        else:\n",
    "            source_sequence = sequence\n",
    "            target_sequence = sequence\n",
    "        \n",
    "        vec = self.vec_function(source_sequence)\n",
    "        vec = torch.FloatTensor(vec)\n",
    "        \n",
    "        tokens = self.vocab.tokenize(target_sequence)\n",
    "        ints = self.vocab.numericalize(tokens)\n",
    "        ints = torch.LongTensor(ints)\n",
    "        \n",
    "        return (vec, ints)\n",
    "    \n",
    "    def new(self, sequences):\n",
    "        return self.__class__(sequences, self.vocab, self.vec_function, self.collate_function)\n",
    "    \n",
    "    def split_on_idxs(self, train_idxs, valid_idxs):\n",
    "        \n",
    "        train_ds = self.new([self.sequences[i] for i in train_idxs])\n",
    "        valid_ds = self.new([self.sequences[i] for i in valid_idxs])\n",
    "        \n",
    "        return (train_ds, valid_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdkit\n",
    "\n",
    "from mrl.chem import ECFP6\n",
    "\n",
    "ds = Vec_To_Text_Dataset(df.smiles.values, vocab, ECFP6)\n",
    "dl = ds.dataloader(16, num_workers=0)\n",
    "x,y = next(iter(dl))\n",
    "assert len(x)==2\n",
    "assert (x[0][:,1:] == y[:,:-1]).all()\n",
    "assert sum([len(i) for i in ds.split(0.2)]) == len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdkit\n",
    "\n",
    "from mrl.chem import ECFP6\n",
    "\n",
    "ds = Vec_To_Text_Dataset(\n",
    "    [(df.smiles.values[i],df.smiles.values[i+1]) for i in range(len(df.smiles.values)-1)], \n",
    "    vocab, ECFP6)\n",
    "dl = ds.dataloader(16, num_workers=0)\n",
    "x,y = next(iter(dl))\n",
    "assert len(x)==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Vec_Prediction_Dataset(Vector_Dataset):\n",
    "    '''\n",
    "    Vec_Prediction_Dataset - base dataset for predicting y_vals from vectors\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `sequences list[str]`: list of text sequences\n",
    "\n",
    "    - `y_vals list[int, float]`: list of paired output values\n",
    "\n",
    "    - `vec_function Callable`: function to convert a sequence to a vector\n",
    "\n",
    "    - `collate_function Callable`: batch collate function. If None, defauts to `vector_prediction_collate`\n",
    "    '''\n",
    "    def __init__(self, sequences, y_vals, vec_function, collate_function=None):\n",
    "        if collate_function is None:\n",
    "            collate_function = vector_prediction_collate\n",
    "        super().__init__(sequences, vec_function, collate_function)\n",
    "        \n",
    "        self.y_vals = y_vals\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fp = super().__getitem__(idx)\n",
    "        y_val = torch.FloatTensor([self.y_vals[idx]])\n",
    "        return (fp, y_val)\n",
    "    \n",
    "    def new(self, sequences, y_vals):\n",
    "        return self.__class__(sequences, y_vals, self.vec_function, self.collate_function)\n",
    "    \n",
    "    \n",
    "    def split_on_idxs(self, train_idxs, valid_idxs):\n",
    "        \n",
    "        train_ds = self.new([self.sequences[i] for i in train_idxs],\n",
    "                            [self.y_vals[i] for i in train_idxs])\n",
    "        valid_ds = self.new([self.sequences[i] for i in valid_idxs],\n",
    "                            [self.y_vals[i] for i in valid_idxs])\n",
    "        \n",
    "        return (train_ds, valid_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdkit\n",
    "\n",
    "from mrl.chem import ECFP6\n",
    "\n",
    "ds = Vec_Prediction_Dataset(df.smiles.values, [0 for i in df.smiles.values], ECFP6)\n",
    "dl = ds.dataloader(16, num_workers=0)\n",
    "x,y = next(iter(dl))\n",
    "assert sum([len(i) for i in ds.split(0.2)]) == len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mrl)",
   "language": "python",
   "name": "mrl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
