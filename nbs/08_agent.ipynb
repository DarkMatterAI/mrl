{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1060f0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aec834",
   "metadata": {},
   "source": [
    "# Agent\n",
    "\n",
    "> Base Agent class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300b5c54",
   "metadata": {},
   "source": [
    "Agent holds a model, a reference model, vocab and dataset. agent handles getting log probs, reconstructing model outputs, and supervised training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699053c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c4b472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from mrl.imports import *\n",
    "from mrl.core import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f1594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, model, vocab, loss_function, dataset):\n",
    "        self.model = model\n",
    "        self.base_model = copy.deepcopy(model)\n",
    "        to_device(self.model)\n",
    "        to_device(self.base_model)\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.dataset = dataset\n",
    "        self.opt = self.get_opt()\n",
    "        self.loss_function = loss_function\n",
    "        \n",
    "    def get_opt(self, **optim_kwargs):\n",
    "        return optim.Adam(self.model.parameters(), **optim_kwargs)\n",
    "    \n",
    "    def train_supervised(self, bs, epochs, lr, percent_valid=0.05):\n",
    "        \n",
    "        train_ds, valid_ds = self.dataset.split(percent_valid)\n",
    "        \n",
    "        train_dl = train_ds.dataloader(bs)\n",
    "        valid_dl = valid_ds.dataloader(bs)\n",
    "        \n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(self.opt, max_lr=lr,\n",
    "                                                 steps_per_epoch=len(train_dl), epochs=10)\n",
    "        \n",
    "        print('Epoch\\tTrain\\tValid')\n",
    "        for epoch in range(epochs):\n",
    "            train_losses = []\n",
    "            for i, batch in enumerate(train_dl):\n",
    "                x,y = batch\n",
    "                if not type(x)==list:\n",
    "                    x = [x]\n",
    "                    \n",
    "                output = self.model(*x)\n",
    "                self.opt.zero_grad()\n",
    "                loss = self.loss_function(output, y)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                scheduler.step()\n",
    "                train_losses.append(loss.detach().cpu())\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                valid_losses = []\n",
    "                for i, batch in enumerate(valid_dl):\n",
    "                    x,y = batch\n",
    "                    if not type(x)==list:\n",
    "                        x = [x]\n",
    "                        \n",
    "                    output = self.model(*x)\n",
    "                    loss = self.loss_function(output, y)\n",
    "                    valid_losses.append(loss.detach().cpu())\n",
    "                    \n",
    "            train_loss = smooth_batches(train_losses)\n",
    "            valid_loss = smooth_batches(valid_losses)\n",
    "                    \n",
    "            print(f'{epoch}\\t{train_loss:.2f}\\t{valid_loss:.2f}')\n",
    "                \n",
    "    \n",
    "    def update_dataset(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    def update_dataset_from_inputs(self, *dataset_inputs):\n",
    "        dataset = self.dataset.new(*dataset_inputs)\n",
    "        self.update_dataset(dataset)\n",
    "    \n",
    "    def reconstruct(self, preds):\n",
    "        return maybe_parallel(self.vocab.reconstruct, preds)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4781a7bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
