{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#skip\n",
    "! [ -e /content ] && pip install -Uqq mrl-pypi  # upgrade mrl on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model_zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Zoo\n",
    "\n",
    "> Standard Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmai/miniconda3/envs/mrl/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for boost::shared_ptr<RDKit::FilterCatalogEntry const> already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "from mrl.imports import *\n",
    "from mrl.core import *\n",
    "from mrl.chem import *\n",
    "from mrl.torch_imports import *\n",
    "from mrl.torch_core import *\n",
    "from mrl.layers import *\n",
    "from mrl.dataloaders import *\n",
    "from mrl.vocab import *\n",
    "from mrl.g_models.all import *\n",
    "from mrl.train.agent import *\n",
    "\n",
    "from torch.utils.model_zoo import load_url\n",
    "from torch.hub import download_url_to_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Zoo\n",
    "\n",
    "This module contains a set of standard models with pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "S3_PREFIX = 'https://dmai-mrl.s3.amazonaws.com/mrl_public'\n",
    "\n",
    "def model_from_url(weight_filename):\n",
    "    location = f'{S3_PREFIX}/{weight_filename}'\n",
    "    return load_url(location, map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class PretrainedGenerativeAgent(GenerativeAgent):\n",
    "    '''\n",
    "    PretrainedGenerativeAgent - base `GenerativeAgent` \n",
    "    variant for pretrained models\n",
    "    \n",
    "    Inputs:\n",
    "\n",
    "    - `weight_filename str`: filename to grab from S3\n",
    "\n",
    "    - `model nn.Module`: model\n",
    "\n",
    "    - `vocab Vocab`: vocabulary\n",
    "\n",
    "    - `loss_function Callable`: loss function for supervised training. Should\n",
    "    function as `loss = loss_function(model_output, y)`\n",
    "\n",
    "    - `dataset Base_Dataset`: dataset\n",
    "\n",
    "    - `base_update float`: update fraction for the baseline model. Updates\n",
    "    the base model following `base_model = base_update*base_model + (1-base_update)*model`\n",
    "\n",
    "    - `base_update_iter int`: update frequency for baseline model\n",
    "\n",
    "    - `base_model bool`: if False, baseline model will not be created\n",
    "\n",
    "    - `opt_kwargs dict`: dictionary of keyword arguments passed to `optim.Adam`\n",
    "\n",
    "    - `clip float`: gradient clipping\n",
    "\n",
    "    - `name str`: agent name\n",
    "    \n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 weight_filename,\n",
    "                 model,\n",
    "                 vocab,\n",
    "                 loss_function,\n",
    "                 dataset,\n",
    "                 base_update=0.97,\n",
    "                 base_update_iter=5,\n",
    "                 base_model=True,\n",
    "                 opt_kwargs={},\n",
    "                 clip=1.,\n",
    "                 name='pretrained_model'\n",
    "                ):\n",
    "        \n",
    "        self.weight_filename = weight_filename\n",
    "        location = f'{S3_PREFIX}/{weight_filename}'\n",
    "        model.load_state_dict(load_url(location, map_location='cpu'))\n",
    "        self.weight_filepath = f\"{torch.hub.get_dir()}/checkpoints/{self.weight_filename}\"\n",
    "        \n",
    "        super().__init__(model,\n",
    "                         vocab, \n",
    "                         loss_function,\n",
    "                         dataset,\n",
    "                         base_update=base_update,\n",
    "                         base_update_iter=base_update_iter,\n",
    "                         base_model=base_model,\n",
    "                         opt_kwargs=opt_kwargs,\n",
    "                         clip=clip,\n",
    "                         name=name\n",
    "                         )\n",
    "        \n",
    "    def reload_weights(self):\n",
    "        self.model.load_state_dict(\n",
    "            torch.load(self.weight_filepath))\n",
    "\n",
    "        if isinstance(self.base_model, nn.Module):\n",
    "            self.base_model.load_state_dict(\n",
    "                torch.load(self.weight_filepath))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM LM\n",
    "\n",
    "Models based on `LSTM_LM` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def lstm_lm_small(vocab, drop_scale=1.):\n",
    "    '''\n",
    "    lstm_lm_small - small LSTM_LM model\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `vocab Vocab`: vocab to use\n",
    "    \n",
    "    - `drop_scale float`: scale dropout values\n",
    "    '''\n",
    "    \n",
    "    d_vocab = len(vocab.itos)\n",
    "    bos_idx = vocab.stoi['bos']\n",
    "    d_embedding = 256\n",
    "    d_hidden = 1024\n",
    "    n_layers = 3\n",
    "    bidir = False\n",
    "    tie_weights = True\n",
    "    \n",
    "    input_dropout = 0.3*drop_scale\n",
    "    lstm_dropout = 0.3*drop_scale\n",
    "    \n",
    "    model = LSTM_LM(d_vocab, \n",
    "                    d_embedding,\n",
    "                    d_hidden, \n",
    "                    n_layers,\n",
    "                    input_dropout,\n",
    "                    lstm_dropout,\n",
    "                    bos_idx, \n",
    "                    bidir, \n",
    "                    tie_weights)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def lstm_lm_large(vocab, drop_scale=1.):\n",
    "    '''\n",
    "    lstm_lm_large - large LSTM_LM model\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `vocab Vocab`: vocab to use\n",
    "    \n",
    "    - `drop_scale float`: scale dropout values\n",
    "    '''\n",
    "    \n",
    "    d_vocab = len(vocab.itos)\n",
    "    bos_idx = vocab.stoi['bos']\n",
    "    d_embedding = 400\n",
    "    d_hidden = 1552\n",
    "    n_layers = 5\n",
    "    bidir = False\n",
    "    tie_weights = True\n",
    "    \n",
    "    \n",
    "    input_dropout = 0.3*drop_scale\n",
    "    lstm_dropout = 0.3*drop_scale\n",
    "    \n",
    "    model = LSTM_LM(d_vocab, \n",
    "                    d_embedding,\n",
    "                    d_hidden, \n",
    "                    n_layers,\n",
    "                    input_dropout,\n",
    "                    lstm_dropout,\n",
    "                    bos_idx, \n",
    "                    bidir, \n",
    "                    tie_weights)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = CharacterVocab(SMILES_CHAR_VOCAB)\n",
    "model = lstm_lm_small(vocab)\n",
    "assert isinstance(model, nn.Module)\n",
    "model = lstm_lm_large(vocab)\n",
    "assert isinstance(model, nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LSTM_LM_Small_ZINC(PretrainedGenerativeAgent):\n",
    "    '''\n",
    "    LSTM_LM_Small_ZINC - small `LSTM_LM` model \n",
    "    trained on a chunk of the ZINC library\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `drop_scale float`: dropout scale\n",
    "    \n",
    "    - `base_update float`: update fraction for the baseline model. Updates\n",
    "    the base model following `base_model = base_update*base_model + (1-base_update)*model`\n",
    "    \n",
    "    - `base_update_iter int`: update frequency for baseline model\n",
    "\n",
    "    - `base_model bool`: if False, baseline model will not be created\n",
    "\n",
    "    - `opt_kwargs dict`: dictionary of keyword arguments passed to `optim.Adam`\n",
    "\n",
    "    - `clip float`: gradient clipping\n",
    "\n",
    "    - `name str`: agent name\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 drop_scale=1.,\n",
    "                 base_update=0.97, \n",
    "                 base_update_iter=5,\n",
    "                 base_model=True,\n",
    "                 opt_kwargs={},\n",
    "                 clip=1.,\n",
    "                 name = 'lstmlm_small_zinc'\n",
    "                ):\n",
    "        vocab = CharacterVocab(SMILES_CHAR_VOCAB)\n",
    "        model = lstm_lm_small(vocab, drop_scale=drop_scale)\n",
    "        weight_filename = 'lstmlm_small_zinc.pt'\n",
    "        \n",
    "        dataset = Text_Dataset(['C'], vocab)\n",
    "        loss_function = CrossEntropy()\n",
    "        \n",
    "        \n",
    "        super().__init__(weight_filename,\n",
    "                         model,\n",
    "                         vocab, \n",
    "                         loss_function,\n",
    "                         dataset,\n",
    "                         base_update=base_update,\n",
    "                         base_update_iter=base_update_iter,\n",
    "                         base_model=base_model,\n",
    "                         opt_kwargs=opt_kwargs,\n",
    "                         clip=clip,\n",
    "                         name=name\n",
    "                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "agent = LSTM_LM_Small_ZINC()\n",
    "\n",
    "preds, _ = agent.model.sample_no_grad(100, 100)\n",
    "smiles = agent.reconstruct(preds)\n",
    "mols = to_mols(smiles)\n",
    "mols = [i for i in mols if i is not None]\n",
    "assert len(mols)>80\n",
    "\n",
    "agent.reload_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LSTM_LM_Small_Chembl(PretrainedGenerativeAgent):\n",
    "    '''\n",
    "    LSTM_LM_Small_Chembl - small `LSTM_LM` model \n",
    "    trained on a chunk of the Chembl library\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `drop_scale float`: dropout scale\n",
    "    \n",
    "    - `base_update float`: update fraction for the baseline model. Updates\n",
    "    the base model following `base_model = base_update*base_model + (1-base_update)*model`\n",
    "    \n",
    "    - `base_update_iter int`: update frequency for baseline model\n",
    "\n",
    "    - `base_model bool`: if False, baseline model will not be created\n",
    "\n",
    "    - `opt_kwargs dict`: dictionary of keyword arguments passed to `optim.Adam`\n",
    "\n",
    "    - `clip float`: gradient clipping\n",
    "\n",
    "    - `name str`: agent name\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 drop_scale=1.,\n",
    "                 base_update=0.97, \n",
    "                 base_update_iter=5,\n",
    "                 base_model=True,\n",
    "                 opt_kwargs={},\n",
    "                 clip=1.,\n",
    "                 name = 'lstmlm_small_chembl'\n",
    "                ):\n",
    "        \n",
    "        vocab = CharacterVocab(SMILES_CHAR_VOCAB)\n",
    "        model = lstm_lm_small(vocab, drop_scale=drop_scale)\n",
    "        weight_filename = 'lstmlm_small_chembl.pt'\n",
    "        \n",
    "        loss_function = CrossEntropy()\n",
    "        dataset = Text_Dataset(['C'], vocab)\n",
    "        \n",
    "        \n",
    "        super().__init__(weight_filename,\n",
    "                         model,\n",
    "                         vocab, \n",
    "                         loss_function,\n",
    "                         dataset,\n",
    "                         base_update=base_update,\n",
    "                         base_update_iter=base_update_iter,\n",
    "                         base_model=base_model,\n",
    "                         opt_kwargs=opt_kwargs,\n",
    "                         clip=clip,\n",
    "                         name=name\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "agent = LSTM_LM_Small_Chembl()\n",
    "\n",
    "preds, _ = agent.model.sample_no_grad(100, 100)\n",
    "smiles = agent.reconstruct(preds)\n",
    "mols = to_mols(smiles)\n",
    "mols = [i for i in mols if i is not None]\n",
    "assert len(mols)>80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LSTM_LM_Small_ZINC_NC(PretrainedGenerativeAgent):\n",
    "    '''\n",
    "    LSTM_LM_Small_ZINC_NC - small `LSTM_LM` model \n",
    "    trained on a chunk of the ZINC library with \n",
    "    no chirality markers (ie no '@' symbols)\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `drop_scale float`: dropout scale\n",
    "    \n",
    "    - `base_update float`: update fraction for the baseline model. Updates\n",
    "    the base model following `base_model = base_update*base_model + (1-base_update)*model`\n",
    "    \n",
    "    - `base_update_iter int`: update frequency for baseline model\n",
    "\n",
    "    - `base_model bool`: if False, baseline model will not be created\n",
    "\n",
    "    - `opt_kwargs dict`: dictionary of keyword arguments passed to `optim.Adam`\n",
    "\n",
    "    - `clip float`: gradient clipping\n",
    "\n",
    "    - `name str`: agent name\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 drop_scale=1.,\n",
    "                 base_update=0.97, \n",
    "                 base_update_iter=5,\n",
    "                 base_model=True,\n",
    "                 opt_kwargs={},\n",
    "                 clip=1.,\n",
    "                 name = 'lstmlm_small_zinc_nc'\n",
    "                ):\n",
    "        \n",
    "        vocab = CharacterVocab(SMILES_CHAR_VOCAB, prefunc=remove_stereo, postfunc=remove_stereo)\n",
    "        model = lstm_lm_small(vocab, drop_scale=drop_scale)\n",
    "        weight_filename = 'lstmlm_small_zinc_nc.pt'\n",
    "        \n",
    "        loss_function = CrossEntropy()\n",
    "        dataset = Text_Dataset(['C'], vocab)\n",
    "        \n",
    "        super().__init__(weight_filename,\n",
    "                         model,\n",
    "                         vocab, \n",
    "                         loss_function,\n",
    "                         dataset,\n",
    "                         base_update=base_update,\n",
    "                         base_update_iter=base_update_iter,\n",
    "                         base_model=base_model,\n",
    "                         opt_kwargs=opt_kwargs,\n",
    "                         clip=clip,\n",
    "                         name=name\n",
    "                         )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "agent = LSTM_LM_Small_ZINC_NC()\n",
    "\n",
    "preds, _ = agent.model.sample_no_grad(100, 100)\n",
    "smiles = agent.reconstruct(preds)\n",
    "mols = to_mols(smiles)\n",
    "mols = [i for i in mols if i is not None]\n",
    "assert len(mols)>80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LSTM_LM_Small_Chembl_NC(PretrainedGenerativeAgent):\n",
    "    '''\n",
    "    LSTM_LM_Small_Chembl_NC - small `LSTM_LM` model \n",
    "    trained on a chunk of the Chembl library with \n",
    "    no chirality markers (ie no '@' symbols)\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `drop_scale float`: dropout scale\n",
    "    \n",
    "    - `base_update float`: update fraction for the baseline model. Updates\n",
    "    the base model following `base_model = base_update*base_model + (1-base_update)*model`\n",
    "    \n",
    "    - `base_update_iter int`: update frequency for baseline model\n",
    "\n",
    "    - `base_model bool`: if False, baseline model will not be created\n",
    "\n",
    "    - `opt_kwargs dict`: dictionary of keyword arguments passed to `optim.Adam`\n",
    "\n",
    "    - `clip float`: gradient clipping\n",
    "\n",
    "    - `name str`: agent name\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 drop_scale=1.,\n",
    "                 base_update=0.97, \n",
    "                 base_update_iter=5,\n",
    "                 base_model=True,\n",
    "                 opt_kwargs={},\n",
    "                 clip=1.,\n",
    "                 name = 'lstmlm_small_chembl_nc'\n",
    "                ):\n",
    "        \n",
    "        vocab = CharacterVocab(SMILES_CHAR_VOCAB, prefunc=remove_stereo, postfunc=remove_stereo)\n",
    "        model = lstm_lm_small(vocab, drop_scale=drop_scale)\n",
    "        weight_filename = 'lstmlm_small_chembl_nc.pt'\n",
    "        \n",
    "        loss_function = CrossEntropy()\n",
    "        dataset = Text_Dataset(['C'], vocab)\n",
    "        \n",
    "        super().__init__(weight_filename,\n",
    "                         model,\n",
    "                         vocab, \n",
    "                         loss_function,\n",
    "                         dataset,\n",
    "                         base_update=base_update,\n",
    "                         base_update_iter=base_update_iter,\n",
    "                         base_model=base_model,\n",
    "                         opt_kwargs=opt_kwargs,\n",
    "                         clip=clip,\n",
    "                         name=name\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "agent = LSTM_LM_Small_Chembl_NC()\n",
    "\n",
    "preds, _ = agent.model.sample_no_grad(100, 100)\n",
    "smiles = agent.reconstruct(preds)\n",
    "mols = to_mols(smiles)\n",
    "mols = [i for i in mols if i is not None]\n",
    "assert len(mols)>80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "        \n",
    "class LSTM_LM_Small_ZINC_Selfies(PretrainedGenerativeAgent):\n",
    "    '''\n",
    "    LSTM_LM_Small_ZINC_Selfies - small `LSTM_LM` model \n",
    "    trained on a chunk of the ZINC library using \n",
    "    SELFIES-type tokenization\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `drop_scale float`: dropout scale\n",
    "    \n",
    "    - `base_update float`: update fraction for the baseline model. Updates\n",
    "    the base model following `base_model = base_update*base_model + (1-base_update)*model`\n",
    "    \n",
    "    - `base_update_iter int`: update frequency for baseline model\n",
    "\n",
    "    - `base_model bool`: if False, baseline model will not be created\n",
    "\n",
    "    - `opt_kwargs dict`: dictionary of keyword arguments passed to `optim.Adam`\n",
    "\n",
    "    - `clip float`: gradient clipping\n",
    "\n",
    "    - `name str`: agent name\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 drop_scale=1.,\n",
    "                 base_update=0.97, \n",
    "                 base_update_iter=5,\n",
    "                 base_model=True,\n",
    "                 opt_kwargs={},\n",
    "                 clip=1.,\n",
    "                 name = 'lstmlm_small_zinc_selfies'\n",
    "                ):\n",
    "        \n",
    "        vocab = FuncVocab(SELFIES_VOCAB, split_selfie, \n",
    "                  prefunc=smile_to_selfie, postfunc=selfie_to_smile)\n",
    "        model = lstm_lm_small(vocab, drop_scale=drop_scale)\n",
    "        weight_filename = 'lstmlm_small_zinc_selfies.pt'\n",
    "        \n",
    "        loss_function = CrossEntropy()\n",
    "        dataset = Text_Dataset(['C'], vocab)\n",
    "        \n",
    "        \n",
    "        super().__init__(weight_filename,\n",
    "                         model,\n",
    "                         vocab, \n",
    "                         loss_function,\n",
    "                         dataset,\n",
    "                         base_update=base_update,\n",
    "                         base_update_iter=base_update_iter,\n",
    "                         base_model=base_model,\n",
    "                         opt_kwargs=opt_kwargs,\n",
    "                         clip=clip,\n",
    "                         name=name\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "agent = LSTM_LM_Small_ZINC_Selfies()\n",
    "\n",
    "preds, _ = agent.model.sample_no_grad(100, 100)\n",
    "smiles = agent.reconstruct(preds)\n",
    "mols = to_mols(smiles)\n",
    "mols = [i for i in mols if i is not None]\n",
    "assert len(mols)>80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "        \n",
    "class LSTM_LM_Small_Chembl_Selfies(PretrainedGenerativeAgent):\n",
    "    '''\n",
    "    LSTM_LM_Small_Chembl_Selfies - small `LSTM_LM` model \n",
    "    trained on a chunk of the Chembl library using \n",
    "    SELFIES-type tokenization\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `drop_scale float`: dropout scale\n",
    "    \n",
    "    - `base_update float`: update fraction for the baseline model. Updates\n",
    "    the base model following `base_model = base_update*base_model + (1-base_update)*model`\n",
    "    \n",
    "    - `base_update_iter int`: update frequency for baseline model\n",
    "\n",
    "    - `base_model bool`: if False, baseline model will not be created\n",
    "\n",
    "    - `opt_kwargs dict`: dictionary of keyword arguments passed to `optim.Adam`\n",
    "\n",
    "    - `clip float`: gradient clipping\n",
    "\n",
    "    - `name str`: agent name\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 drop_scale=1.,\n",
    "                 base_update=0.97, \n",
    "                 base_update_iter=5,\n",
    "                 base_model=True,\n",
    "                 opt_kwargs={},\n",
    "                 clip=1.,\n",
    "                 name = 'lstmlm_small_chembl_selfies'\n",
    "                ):\n",
    "        \n",
    "        vocab = FuncVocab(SELFIES_VOCAB, split_selfie, \n",
    "                  prefunc=smile_to_selfie, postfunc=selfie_to_smile)\n",
    "        model = lstm_lm_small(vocab, drop_scale=drop_scale)\n",
    "        weight_filename = 'lstmlm_small_chembl_selfies.pt'\n",
    "        \n",
    "        loss_function = CrossEntropy()\n",
    "        dataset = Text_Dataset(['C'], vocab)\n",
    "        \n",
    "        \n",
    "        super().__init__(weight_filename,\n",
    "                         model,\n",
    "                         vocab, \n",
    "                         loss_function,\n",
    "                         dataset,\n",
    "                         base_update=base_update,\n",
    "                         base_update_iter=base_update_iter,\n",
    "                         base_model=base_model,\n",
    "                         opt_kwargs=opt_kwargs,\n",
    "                         clip=clip,\n",
    "                         name=name\n",
    "                         )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "agent = LSTM_LM_Small_Chembl_Selfies()\n",
    "\n",
    "preds, _ = agent.model.sample_no_grad(100, 100)\n",
    "smiles = agent.reconstruct(preds)\n",
    "mols = to_mols(smiles)\n",
    "mols = [i for i in mols if i is not None]\n",
    "assert len(mols)>80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LSTM_LM_Small_Rgroup(PretrainedGenerativeAgent):\n",
    "    '''\n",
    "    LSTM_LM_Small_Rgroup - small `LSTM_LM` model \n",
    "    trained on R-groups. R groups are SMILES with \n",
    "    the format `*R`\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `drop_scale float`: dropout scale\n",
    "    \n",
    "    - `base_update float`: update fraction for the baseline model. Updates\n",
    "    the base model following `base_model = base_update*base_model + (1-base_update)*model`\n",
    "    \n",
    "    - `base_update_iter int`: update frequency for baseline model\n",
    "\n",
    "    - `base_model bool`: if False, baseline model will not be created\n",
    "\n",
    "    - `opt_kwargs dict`: dictionary of keyword arguments passed to `optim.Adam`\n",
    "\n",
    "    - `clip float`: gradient clipping\n",
    "\n",
    "    - `name str`: agent name\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 drop_scale=1.,\n",
    "                 base_update=0.97, \n",
    "                 base_update_iter=5,\n",
    "                 base_model=True,\n",
    "                 opt_kwargs={},\n",
    "                 clip=1.,\n",
    "                 name = 'lstmlm_small_rgroup'\n",
    "                ):\n",
    "        \n",
    "        vocab = CharacterVocab(SMILES_CHAR_VOCAB)\n",
    "        model = lstm_lm_small(vocab, drop_scale=drop_scale)\n",
    "        weight_filename = 'lstmlm_small_rgroup.pt'\n",
    "        \n",
    "        loss_function = CrossEntropy()\n",
    "        dataset = Text_Dataset(['C'], vocab)\n",
    "        \n",
    "        \n",
    "        super().__init__(weight_filename,\n",
    "                         model,\n",
    "                         vocab, \n",
    "                         loss_function,\n",
    "                         dataset,\n",
    "                         base_update=base_update,\n",
    "                         base_update_iter=base_update_iter,\n",
    "                         base_model=base_model,\n",
    "                         opt_kwargs=opt_kwargs,\n",
    "                         clip=clip,\n",
    "                         name=name\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "agent = LSTM_LM_Small_Rgroup()\n",
    "\n",
    "preds, _ = agent.model.sample_no_grad(100, 100)\n",
    "smiles = agent.reconstruct(preds)\n",
    "mols = to_mols(smiles)\n",
    "mols = [i for i in mols if i is not None]\n",
    "assert len(mols)>80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LSTM_LM_Small_Linkers(PretrainedGenerativeAgent):\n",
    "    '''\n",
    "    LSTM_LM_Small_Linkers - small `LSTM_LM` model \n",
    "    trained on linkers. Linkes are SMILES with \n",
    "    the format `*R*`\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `drop_scale float`: dropout scale\n",
    "    \n",
    "    - `base_update float`: update fraction for the baseline model. Updates\n",
    "    the base model following `base_model = base_update*base_model + (1-base_update)*model`\n",
    "    \n",
    "    - `base_update_iter int`: update frequency for baseline model\n",
    "\n",
    "    - `base_model bool`: if False, baseline model will not be created\n",
    "\n",
    "    - `opt_kwargs dict`: dictionary of keyword arguments passed to `optim.Adam`\n",
    "\n",
    "    - `clip float`: gradient clipping\n",
    "\n",
    "    - `name str`: agent name\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 drop_scale=1.,\n",
    "                 base_update=0.97, \n",
    "                 base_update_iter=5,\n",
    "                 base_model=True,\n",
    "                 opt_kwargs={},\n",
    "                 clip=1.,\n",
    "                 name = 'lstmlm_small_linkers'\n",
    "                ):\n",
    "        \n",
    "        vocab = CharacterVocab(SMILES_CHAR_VOCAB, prefunc=remove_stereo)\n",
    "        model = lstm_lm_small(vocab, drop_scale=drop_scale)\n",
    "        weight_filename = 'lstmlm_small_linkers.pt'\n",
    "        \n",
    "        loss_function = CrossEntropy()\n",
    "        dataset = Text_Dataset(['C'], vocab)\n",
    "        \n",
    "        super().__init__(weight_filename,\n",
    "                         model,\n",
    "                         vocab, \n",
    "                         loss_function,\n",
    "                         dataset,\n",
    "                         base_update=base_update,\n",
    "                         base_update_iter=base_update_iter,\n",
    "                         base_model=base_model,\n",
    "                         opt_kwargs=opt_kwargs,\n",
    "                         clip=clip,\n",
    "                         name=name\n",
    "                         )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "agent = LSTM_LM_Small_Linkers()\n",
    "\n",
    "preds, _ = agent.model.sample_no_grad(100, 100)\n",
    "smiles = agent.reconstruct(preds)\n",
    "mols = to_mols(smiles)\n",
    "mols = [i for i in mols if i is not None]\n",
    "assert len(mols)>80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LSTM_LM_Small_Linkers_Mapped(PretrainedGenerativeAgent):\n",
    "    '''\n",
    "    LSTM_LM_Small_Linkers_Mapped - small `LSTM_LM` \n",
    "    model trained on linkers with mapping tokens for \n",
    "    compatibility with `LinkerBlockTemplate`. Linkers \n",
    "    are SMILES with the format `*R*`\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `drop_scale float`: dropout scale\n",
    "    \n",
    "    - `base_update float`: update fraction for the baseline model. Updates\n",
    "    the base model following `base_model = base_update*base_model + (1-base_update)*model`\n",
    "    \n",
    "    - `base_update_iter int`: update frequency for baseline model\n",
    "\n",
    "    - `base_model bool`: if False, baseline model will not be created\n",
    "\n",
    "    - `opt_kwargs dict`: dictionary of keyword arguments passed to `optim.Adam`\n",
    "\n",
    "    - `clip float`: gradient clipping\n",
    "\n",
    "    - `name str`: agent name\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 drop_scale=1.,\n",
    "                 base_update=0.97, \n",
    "                 base_update_iter=5,\n",
    "                 base_model=True,\n",
    "                 opt_kwargs={},\n",
    "                 clip=1.,\n",
    "                 name = 'lstmlm_small_linkers_mapped'\n",
    "                ):\n",
    "        \n",
    "        vocab = CharacterReplaceVocab(SMILES_CHAR_VOCAB, \n",
    "                                replace_dict={'[2*:1]':'X', '[2*:2]':'Y'},\n",
    "                                prefunc=remove_stereo)\n",
    "        model = lstm_lm_small(vocab, drop_scale=drop_scale)\n",
    "        weight_filename = 'lstmlm_small_linkers_mapped.pt'\n",
    "        \n",
    "        loss_function = CrossEntropy()\n",
    "        dataset = Text_Dataset(['C'], vocab)\n",
    "        \n",
    "        \n",
    "        super().__init__(weight_filename,\n",
    "                         model,\n",
    "                         vocab, \n",
    "                         loss_function,\n",
    "                         dataset,\n",
    "                         base_update=base_update,\n",
    "                         base_update_iter=base_update_iter,\n",
    "                         base_model=base_model,\n",
    "                         opt_kwargs=opt_kwargs,\n",
    "                         clip=clip,\n",
    "                         name=name\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "agent = LSTM_LM_Small_Linkers_Mapped()\n",
    "\n",
    "preds, _ = agent.model.sample_no_grad(100, 100)\n",
    "smiles = agent.reconstruct(preds)\n",
    "mols = to_mols(smiles)\n",
    "mols = [i for i in mols if i is not None]\n",
    "assert len(mols)>80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LSTM_LM_Small_Swissprot(PretrainedGenerativeAgent):\n",
    "    '''\n",
    "    LSTM_LM_Small_Swissprot - small `LSTM_LM` \n",
    "    model trained on the Swissprot protein dataset. \n",
    "    This model was trained on protein sequences \n",
    "    of 650 amino acids or fewer\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `drop_scale float`: dropout scale\n",
    "    \n",
    "    - `base_update float`: update fraction for the baseline model. Updates\n",
    "    the base model following `base_model = base_update*base_model + (1-base_update)*model`\n",
    "    \n",
    "    - `base_update_iter int`: update frequency for baseline model\n",
    "\n",
    "    - `base_model bool`: if False, baseline model will not be created\n",
    "\n",
    "    - `opt_kwargs dict`: dictionary of keyword arguments passed to `optim.Adam`\n",
    "\n",
    "    - `clip float`: gradient clipping\n",
    "\n",
    "    - `name str`: agent name\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 drop_scale=1.,\n",
    "                 base_update=0.97, \n",
    "                 base_update_iter=5,\n",
    "                 base_model=True,\n",
    "                 opt_kwargs={},\n",
    "                 clip=1.,\n",
    "                 name = 'lstmlm_small_swissprot'\n",
    "                ):\n",
    "        \n",
    "        vocab = CharacterVocab(AMINO_ACID_VOCAB)\n",
    "        \n",
    "        model = lstm_lm_small(vocab, drop_scale=drop_scale)\n",
    "        weight_filename = 'lstmlm_small_swissprot.pt'\n",
    "        \n",
    "        loss_function = CrossEntropy()\n",
    "        dataset = Text_Dataset(['C'], vocab)\n",
    "        \n",
    "        \n",
    "        super().__init__(weight_filename,\n",
    "                         model,\n",
    "                         vocab, \n",
    "                         loss_function,\n",
    "                         dataset,\n",
    "                         base_update=base_update,\n",
    "                         base_update_iter=base_update_iter,\n",
    "                         base_model=base_model,\n",
    "                         opt_kwargs=opt_kwargs,\n",
    "                         clip=clip,\n",
    "                         name=name\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "agent = LSTM_LM_Small_Swissprot()\n",
    "\n",
    "preds, _ = agent.model.sample_no_grad(100, 100)\n",
    "smiles = agent.reconstruct(preds)\n",
    "proteins = to_protein(smiles)\n",
    "proteins = [i for i in proteins if i is not None]\n",
    "assert len(proteins)>80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LSTM_LM_Small_PI1M(PretrainedGenerativeAgent):\n",
    "    '''\n",
    "    LSTM_LM_Small_PI1M - small `LSTM_LM` model \n",
    "    trained on a chunk of the PI1M polymer library\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `drop_scale float`: dropout scale\n",
    "    \n",
    "    - `base_update float`: update fraction for the baseline model. Updates\n",
    "    the base model following `base_model = base_update*base_model + (1-base_update)*model`\n",
    "    \n",
    "    - `base_update_iter int`: update frequency for baseline model\n",
    "\n",
    "    - `base_model bool`: if False, baseline model will not be created\n",
    "\n",
    "    - `opt_kwargs dict`: dictionary of keyword arguments passed to `optim.Adam`\n",
    "\n",
    "    - `clip float`: gradient clipping\n",
    "\n",
    "    - `name str`: agent name\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 drop_scale=1.,\n",
    "                 base_update=0.97, \n",
    "                 base_update_iter=5,\n",
    "                 base_model=True,\n",
    "                 opt_kwargs={},\n",
    "                 clip=1.,\n",
    "                 name = 'lstmlm_small_pi1m'\n",
    "                ):\n",
    "        \n",
    "        vocab = CharacterVocab(SMILES_CHAR_VOCAB)\n",
    "        model = lstm_lm_small(vocab, drop_scale=drop_scale)\n",
    "        weight_filename = 'lstmlm_small_pi1m.pt'\n",
    "        \n",
    "        loss_function = CrossEntropy()\n",
    "        dataset = Text_Dataset(['*C*'], vocab)\n",
    "        \n",
    "        \n",
    "        super().__init__(weight_filename,\n",
    "                         model,\n",
    "                         vocab, \n",
    "                         loss_function,\n",
    "                         dataset,\n",
    "                         base_update=base_update,\n",
    "                         base_update_iter=base_update_iter,\n",
    "                         base_model=base_model,\n",
    "                         opt_kwargs=opt_kwargs,\n",
    "                         clip=clip,\n",
    "                         name=name\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "agent = LSTM_LM_Small_PI1M()\n",
    "\n",
    "preds, _ = agent.model.sample_no_grad(100, 100)\n",
    "smiles = agent.reconstruct(preds)\n",
    "mols = to_mols(smiles)\n",
    "mols = [i for i in mols if i is not None]\n",
    "assert len(mols)>80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LSTM_LM_Small_HGenome(PretrainedGenerativeAgent):\n",
    "    '''\n",
    "    LSTM_LM_Small_HGenome - small `LSTM_LM` model \n",
    "    trained on 400bp chunks of the human genome\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `drop_scale float`: dropout scale\n",
    "    \n",
    "    - `base_update float`: update fraction for the baseline model. Updates\n",
    "    the base model following `base_model = base_update*base_model + (1-base_update)*model`\n",
    "    \n",
    "    - `base_update_iter int`: update frequency for baseline model\n",
    "\n",
    "    - `base_model bool`: if False, baseline model will not be created\n",
    "\n",
    "    - `opt_kwargs dict`: dictionary of keyword arguments passed to `optim.Adam`\n",
    "\n",
    "    - `clip float`: gradient clipping\n",
    "\n",
    "    - `name str`: agent name\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 drop_scale=1.,\n",
    "                 base_update=0.97, \n",
    "                 base_update_iter=5,\n",
    "                 base_model=True,\n",
    "                 opt_kwargs={},\n",
    "                 clip=1.,\n",
    "                 name = 'lstmlm_small_hgenome'\n",
    "                ):\n",
    "        \n",
    "        vocab = CharacterVocab(NUCLEIC_ACID_VOCAB)\n",
    "        model = lstm_lm_small(vocab, drop_scale=drop_scale)\n",
    "        weight_filename = 'lstmlm_small_hgenome.pt'\n",
    "        \n",
    "        loss_function = CrossEntropy()\n",
    "        dataset = Text_Dataset(['C'], vocab)\n",
    "        \n",
    "        \n",
    "        super().__init__(weight_filename,\n",
    "                         model,\n",
    "                         vocab, \n",
    "                         loss_function,\n",
    "                         dataset,\n",
    "                         base_update=base_update,\n",
    "                         base_update_iter=base_update_iter,\n",
    "                         base_model=base_model,\n",
    "                         opt_kwargs=opt_kwargs,\n",
    "                         clip=clip,\n",
    "                         name=name\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "agent = LSTM_LM_Small_HGenome()\n",
    "\n",
    "preds, _ = agent.model.sample_no_grad(100, 100)\n",
    "smiles = agent.reconstruct(preds)\n",
    "mols = to_dnas(smiles)\n",
    "mols = [i for i in mols if i is not None]\n",
    "assert len(mols)>80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LSTM_LM_Small_HGenome_3Mer(PretrainedGenerativeAgent):\n",
    "    '''\n",
    "    LSTM_LM_Small_HGenome_3Mer - small `LSTM_LM` model \n",
    "    trained on 400bp chunks of the human genome with \n",
    "    3-mer tokenization\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `drop_scale float`: dropout scale\n",
    "    \n",
    "    - `base_update float`: update fraction for the baseline model. Updates\n",
    "    the base model following `base_model = base_update*base_model + (1-base_update)*model`\n",
    "    \n",
    "    - `base_update_iter int`: update frequency for baseline model\n",
    "\n",
    "    - `base_model bool`: if False, baseline model will not be created\n",
    "\n",
    "    - `opt_kwargs dict`: dictionary of keyword arguments passed to `optim.Adam`\n",
    "\n",
    "    - `clip float`: gradient clipping\n",
    "\n",
    "    - `name str`: agent name\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 drop_scale=1.,\n",
    "                 base_update=0.97, \n",
    "                 base_update_iter=5,\n",
    "                 base_model=True,\n",
    "                 opt_kwargs={},\n",
    "                 clip=1.,\n",
    "                 name = 'lstmlm_small_hgenome_3mer'\n",
    "                ):\n",
    "        \n",
    "        vocab = KmerVocab(DNA_TRIMERS, 3)\n",
    "        model = lstm_lm_small(vocab, drop_scale=drop_scale)\n",
    "        weight_filename = 'lstmlm_small_hgenome_3mer.pt'\n",
    "        \n",
    "        loss_function = CrossEntropy()\n",
    "        dataset = Text_Dataset(['C'], vocab)\n",
    "        \n",
    "        \n",
    "        super().__init__(weight_filename,\n",
    "                         model,\n",
    "                         vocab, \n",
    "                         loss_function,\n",
    "                         dataset,\n",
    "                         base_update=base_update,\n",
    "                         base_update_iter=base_update_iter,\n",
    "                         base_model=base_model,\n",
    "                         opt_kwargs=opt_kwargs,\n",
    "                         clip=clip,\n",
    "                         name=name\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "agent = LSTM_LM_Small_HGenome_3Mer()\n",
    "\n",
    "preds, _ = agent.model.sample_no_grad(100, 33)\n",
    "smiles = agent.reconstruct(preds)\n",
    "mols = to_dnas(smiles)\n",
    "mols = [i for i in mols if i is not None]\n",
    "assert len(mols)>80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional LSTM LM\n",
    "\n",
    "Models based on `Conditional_LSTM_LM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def cond_lstm_small(vocab, encoder, drop_scale=1.):\n",
    "    '''\n",
    "    cond_lstm_small - small conditional LSTM_LM model\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `vocab Vocab`: vocab to use\n",
    "    \n",
    "    - `encoder nn.Module`: encoder module\n",
    "    \n",
    "    - `drop_scale float`: scale dropout values\n",
    "    '''\n",
    "    \n",
    "    d_vocab = len(vocab.itos)\n",
    "    bos_idx = vocab.stoi['bos']\n",
    "    \n",
    "    d_latent = 512\n",
    "    d_embedding = 256\n",
    "    d_hidden = 1024\n",
    "    n_layers = 3\n",
    "    bidir = False\n",
    "    tie_weights = True\n",
    "    condition_hidden = True\n",
    "    condition_output = False\n",
    "    norm_latent = True\n",
    "    \n",
    "    input_dropout = 0.3*drop_scale\n",
    "    lstm_dropout = 0.3*drop_scale\n",
    "    \n",
    "    model = Conditional_LSTM_LM(encoder, \n",
    "                                d_vocab, \n",
    "                                d_embedding, \n",
    "                                d_hidden, \n",
    "                                d_latent, \n",
    "                                n_layers,\n",
    "                                input_dropout, \n",
    "                                lstm_dropout, \n",
    "                                norm_latent, \n",
    "                                condition_hidden, \n",
    "                                condition_output, \n",
    "                                bos_idx)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def cond_lstm_large(vocab, encoder, drop_scale=1.):\n",
    "    '''\n",
    "    cond_lstm_large - large conditional LSTM_LM model\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `vocab Vocab`: vocab to use\n",
    "    \n",
    "    - `encoder nn.Module`: encoder module\n",
    "    \n",
    "    - `drop_scale float`: scale dropout values\n",
    "    '''\n",
    "\n",
    "    d_vocab = len(vocab.itos)\n",
    "    bos_idx = vocab.stoi['bos']\n",
    "    \n",
    "    d_latent = 512\n",
    "    d_embedding = 400\n",
    "    d_hidden = 1552\n",
    "    n_layers = 5\n",
    "    bidir = False\n",
    "    tie_weights = True\n",
    "    condition_hidden = True\n",
    "    condition_output = False\n",
    "    norm_latent = True\n",
    "    \n",
    "    \n",
    "    input_dropout = 0.3*drop_scale\n",
    "    lstm_dropout = 0.3*drop_scale\n",
    "    \n",
    "    model = Conditional_LSTM_LM(encoder, \n",
    "                                d_vocab, \n",
    "                                d_embedding, \n",
    "                                d_hidden, \n",
    "                                d_latent, \n",
    "                                n_layers,\n",
    "                                input_dropout, \n",
    "                                lstm_dropout, \n",
    "                                norm_latent, \n",
    "                                condition_hidden, \n",
    "                                condition_output, \n",
    "                                bos_idx)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def mlp_cond_lstm_small(vocab, drop_scale=1.):\n",
    "    '''\n",
    "    mlp_cond_lstm_small - small conditional \n",
    "    LSTM_LM model with MLP encoder\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `vocab Vocab`: vocab to use\n",
    "        \n",
    "    - `drop_scale float`: scale dropout values\n",
    "    '''\n",
    "    enc_drops = [0.1*drop_scale, 0.1*drop_scale]\n",
    "        \n",
    "    encoder = MLP_Encoder(2048, [1024, 512], 512, enc_drops)\n",
    "    return cond_lstm_small(vocab, encoder, drop_scale=drop_scale)\n",
    "\n",
    "def mlp_cond_lstm_large(vocab, drop_scale=1.):\n",
    "    '''\n",
    "    mlp_cond_lstm_large - large conditional \n",
    "    LSTM_LM model with MLP encoder\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `vocab Vocab`: vocab to use\n",
    "        \n",
    "    - `drop_scale float`: scale dropout values\n",
    "    '''\n",
    "    enc_drops = [0.2*drop_scale, \n",
    "                 0.2*drop_scale, \n",
    "                 0.2*drop_scale, \n",
    "                 0.2*drop_scale]\n",
    "    \n",
    "    encoder = MLP_Encoder(2048, [1024, 512, 512, 512], 512, [0.2, 0.2, 0.2, 0.2])\n",
    "    return cond_lstm_small(vocab, encoder, drop_scale=drop_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = CharacterVocab(SMILES_CHAR_VOCAB)\n",
    "model = mlp_cond_lstm_small(vocab)\n",
    "assert isinstance(model, nn.Module)\n",
    "model = mlp_cond_lstm_large(vocab)\n",
    "assert isinstance(model, nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class FP_Cond_LSTM_LM_Small_ZINC(PretrainedGenerativeAgent):\n",
    "    '''\n",
    "    FP_Cond_LSTM_LM_Small_ZINC - small \n",
    "    `Conditional_LSTM_LM` model trained to \n",
    "    reconstruct SMILES from a ECFP6 fingerprint \n",
    "    using a chunk of the ZINC database\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `drop_scale float`: dropout scale\n",
    "    \n",
    "    - `base_update float`: update fraction for the baseline model. Updates\n",
    "    the base model following `base_model = base_update*base_model + (1-base_update)*model`\n",
    "    \n",
    "    - `base_update_iter int`: update frequency for baseline model\n",
    "\n",
    "    - `base_model bool`: if False, baseline model will not be created\n",
    "\n",
    "    - `opt_kwargs dict`: dictionary of keyword arguments passed to `optim.Adam`\n",
    "\n",
    "    - `clip float`: gradient clipping\n",
    "\n",
    "    - `name str`: agent name\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 drop_scale=1.,\n",
    "                 base_update=0.97, \n",
    "                 base_update_iter=5,\n",
    "                 base_model=True,\n",
    "                 opt_kwargs={},\n",
    "                 clip=1.,\n",
    "                 name = 'fp_cond_lstmlm_small_zinc'\n",
    "                ):\n",
    "        \n",
    "        vocab = CharacterVocab(SMILES_CHAR_VOCAB)\n",
    "        model = mlp_cond_lstm_small(vocab, drop_scale=drop_scale)\n",
    "        weight_filename = 'fp_cond_lstmlm_small_zinc.pt'\n",
    "        \n",
    "        loss_function = CrossEntropy()\n",
    "        fp_function = partial(failsafe_fp, fp_function=ECFP6)\n",
    "        dataset = Vec_To_Text_Dataset(['C'], vocab, fp_function)\n",
    "        \n",
    "        \n",
    "        super().__init__(weight_filename,\n",
    "                         model,\n",
    "                         vocab, \n",
    "                         loss_function,\n",
    "                         dataset,\n",
    "                         base_update=base_update,\n",
    "                         base_update_iter=base_update_iter,\n",
    "                         base_model=base_model,\n",
    "                         opt_kwargs=opt_kwargs,\n",
    "                         clip=clip,\n",
    "                         name=name\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "agent = FP_Cond_LSTM_LM_Small_ZINC()\n",
    "\n",
    "preds, _ = agent.model.sample_no_grad(100, 100)\n",
    "smiles = agent.reconstruct(preds)\n",
    "mols = to_mols(smiles)\n",
    "mols = [i for i in mols if i is not None]\n",
    "assert len(mols)>80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class FP_Cond_LSTM_LM_Small_ZINC_Selfies(PretrainedGenerativeAgent):\n",
    "    '''\n",
    "    FP_Cond_LSTM_LM_Small_ZINC_Selfies - small \n",
    "    `Conditional_LSTM_LM` model trained to \n",
    "    reconstruct SMILES from a ECFP6 fingerprint \n",
    "    using a chunk of the ZINC database with \n",
    "    SELFIES encoding\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `drop_scale float`: dropout scale\n",
    "    \n",
    "    - `base_update float`: update fraction for the baseline model. Updates\n",
    "    the base model following `base_model = base_update*base_model + (1-base_update)*model`\n",
    "    \n",
    "    - `base_update_iter int`: update frequency for baseline model\n",
    "\n",
    "    - `base_model bool`: if False, baseline model will not be created\n",
    "\n",
    "    - `opt_kwargs dict`: dictionary of keyword arguments passed to `optim.Adam`\n",
    "\n",
    "    - `clip float`: gradient clipping\n",
    "\n",
    "    - `name str`: agent name\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 drop_scale=1.,\n",
    "                 base_update=0.97, \n",
    "                 base_update_iter=5,\n",
    "                 base_model=True,\n",
    "                 opt_kwargs={},\n",
    "                 clip=1.,\n",
    "                 name = 'fp_cond_lstmlm_small_zinc_selfies'\n",
    "                ):\n",
    "        \n",
    "        vocab = FuncVocab(SELFIES_VOCAB, split_selfie, \n",
    "                    prefunc=smile_to_selfie, postfunc=selfie_to_smile)\n",
    "        model = mlp_cond_lstm_small(vocab, drop_scale=drop_scale)\n",
    "        weight_filename = 'fp_cond_lstmlm_small_zinc_selfies.pt'\n",
    "        \n",
    "        loss_function = CrossEntropy()\n",
    "        fp_function = partial(failsafe_fp, fp_function=ECFP6)\n",
    "        dataset = Vec_To_Text_Dataset(['C'], vocab, fp_function)\n",
    "        \n",
    "        super().__init__(weight_filename,\n",
    "                         model,\n",
    "                         vocab, \n",
    "                         loss_function,\n",
    "                         dataset,\n",
    "                         base_update=base_update,\n",
    "                         base_update_iter=base_update_iter,\n",
    "                         base_model=base_model,\n",
    "                         opt_kwargs=opt_kwargs,\n",
    "                         clip=clip,\n",
    "                         name=name\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "agent = FP_Cond_LSTM_LM_Small_ZINC_Selfies()\n",
    "\n",
    "preds, _ = agent.model.sample_no_grad(100, 100)\n",
    "smiles = agent.reconstruct(preds)\n",
    "mols = to_mols(smiles)\n",
    "mols = [i for i in mols if i is not None]\n",
    "assert len(mols)>80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class FP_Cond_LSTM_LM_Small_Chembl(PretrainedGenerativeAgent):\n",
    "    '''\n",
    "    FP_Cond_LSTM_LM_Small_Chembl - small \n",
    "    `Conditional_LSTM_LM` model trained to \n",
    "    reconstruct SMILES from a ECFP6 fingerprint \n",
    "    using the Chembl database\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `drop_scale float`: dropout scale\n",
    "    \n",
    "    - `base_update float`: update fraction for the baseline model. Updates\n",
    "    the base model following `base_model = base_update*base_model + (1-base_update)*model`\n",
    "    \n",
    "    - `base_update_iter int`: update frequency for baseline model\n",
    "\n",
    "    - `base_model bool`: if False, baseline model will not be created\n",
    "\n",
    "    - `opt_kwargs dict`: dictionary of keyword arguments passed to `optim.Adam`\n",
    "\n",
    "    - `clip float`: gradient clipping\n",
    "\n",
    "    - `name str`: agent name\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 drop_scale=1.,\n",
    "                 base_update=0.97, \n",
    "                 base_update_iter=5,\n",
    "                 base_model=True,\n",
    "                 opt_kwargs={},\n",
    "                 clip=1.,\n",
    "                 name = 'fp_cond_lstmlm_small_chembl'\n",
    "                ):\n",
    "        \n",
    "        vocab = CharacterVocab(SMILES_CHAR_VOCAB)\n",
    "        model = mlp_cond_lstm_small(vocab, drop_scale=drop_scale)\n",
    "        weight_filename = 'fp_cond_lstmlm_small_chembl.pt'\n",
    "        \n",
    "        loss_function = CrossEntropy()\n",
    "        fp_function = partial(failsafe_fp, fp_function=ECFP6)\n",
    "        dataset = Vec_To_Text_Dataset(['C'], vocab, fp_function)\n",
    "        \n",
    "        super().__init__(weight_filename,\n",
    "                         model,\n",
    "                         vocab, \n",
    "                         loss_function,\n",
    "                         dataset,\n",
    "                         base_update=base_update,\n",
    "                         base_update_iter=base_update_iter,\n",
    "                         base_model=base_model,\n",
    "                         opt_kwargs=opt_kwargs,\n",
    "                         clip=clip,\n",
    "                         name=name\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "agent = FP_Cond_LSTM_LM_Small_Chembl()\n",
    "\n",
    "preds, _ = agent.model.sample_no_grad(100, 100)\n",
    "smiles = agent.reconstruct(preds)\n",
    "mols = to_mols(smiles)\n",
    "mols = [i for i in mols if i is not None]\n",
    "assert len(mols)>80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE\n",
    "\n",
    "Models based on `VAE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def mlp_vae(vocab, drop_scale=1.):\n",
    "    '''\n",
    "    mlp_vae - VAE with MLP encoder\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `vocab Vocab`: vocab to use\n",
    "        \n",
    "    - `drop_scale float`: scale dropout values\n",
    "    '''\n",
    "    \n",
    "    d_vocab = len(vocab.itos)\n",
    "    bos_idx = vocab.stoi['bos']\n",
    "    \n",
    "    d_embedding = 256\n",
    "    encoder_d_in = 2048\n",
    "    encoder_dims = [1024, 512]\n",
    "    d_hidden = 1024\n",
    "    n_layers = 3\n",
    "    d_latent =512\n",
    "    condition_hidden=True\n",
    "    condition_output=True\n",
    "    \n",
    "    encoder_drops = [0.2*drop_scale, 0.2*drop_scale]\n",
    "    input_dropout=0.3*drop_scale\n",
    "    lstm_dropout=0.3*drop_scale\n",
    "\n",
    "    model = MLP_VAE(\n",
    "                d_vocab,\n",
    "                d_embedding,\n",
    "                encoder_d_in,\n",
    "                encoder_dims,\n",
    "                encoder_drops,\n",
    "                d_hidden,\n",
    "                n_layers,\n",
    "                d_latent,\n",
    "                input_dropout=input_dropout,\n",
    "                lstm_dropout=lstm_dropout,\n",
    "                condition_hidden=condition_hidden,\n",
    "                condition_output=condition_output,\n",
    "                bos_idx=bos_idx,\n",
    "            )\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def conv_vae(vocab, drop_scale=1.):\n",
    "    '''\n",
    "    mlp_vae - VAE with convolutional encoder\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `vocab Vocab`: vocab to use\n",
    "        \n",
    "    - `drop_scale float`: scale dropout values\n",
    "    '''\n",
    "    \n",
    "    d_vocab = len(vocab.itos)\n",
    "    bos_idx = vocab.stoi['bos']\n",
    "    \n",
    "    d_embedding = 256\n",
    "    conv_filters = [256, 512, 512]\n",
    "    kernel_sizes = [7, 7, 7]\n",
    "    strides = [2, 2, 2]\n",
    "    d_hidden = 1024\n",
    "    n_layers = 3\n",
    "    d_latent = 512\n",
    "    condition_hidden=True\n",
    "    condition_output=True\n",
    "    \n",
    "    conv_drops = [0.2*drop_scale, 0.2*drop_scale, 0.2*drop_scale]\n",
    "    input_dropout=0.3*drop_scale\n",
    "    lstm_dropout=0.3*drop_scale\n",
    "    \n",
    "    model = Conv_VAE(\n",
    "                    d_vocab,\n",
    "                    d_embedding,\n",
    "                    conv_filters,\n",
    "                    kernel_sizes,\n",
    "                    strides,\n",
    "                    conv_drops,\n",
    "                    d_hidden,\n",
    "                    n_layers,\n",
    "                    d_latent,\n",
    "                    input_dropout=input_dropout,\n",
    "                    lstm_dropout=lstm_dropout,\n",
    "                    condition_hidden=condition_hidden,\n",
    "                    condition_output=condition_output,\n",
    "                    bos_idx=bos_idx)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def lstm_vae(vocab, drop_scale=1.):\n",
    "    '''\n",
    "    mlp_vae - VAE with LSTM encoder\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `vocab Vocab`: vocab to use\n",
    "        \n",
    "    - `drop_scale float`: scale dropout values\n",
    "    '''\n",
    "    \n",
    "    d_vocab = len(vocab.itos)\n",
    "    bos_idx = vocab.stoi['bos']\n",
    "    \n",
    "    d_embedding = 256\n",
    "    d_hidden = 1024\n",
    "    n_layers = 3\n",
    "    d_latent = 512\n",
    "    condition_hidden=True\n",
    "    condition_output=True\n",
    "    \n",
    "    input_dropout=0.3*drop_scale\n",
    "    lstm_dropout=0.3*drop_scale\n",
    "    \n",
    "    model = LSTM_VAE(\n",
    "                    d_vocab,\n",
    "                    d_embedding,\n",
    "                    d_hidden,\n",
    "                    n_layers,\n",
    "                    d_latent,\n",
    "                    input_dropout=input_dropout,\n",
    "                    lstm_dropout=lstm_dropout,\n",
    "                    condition_hidden=condition_hidden,\n",
    "                    condition_output=condition_output,\n",
    "                    bos_idx=bos_idx,\n",
    "                )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = CharacterVocab(SMILES_CHAR_VOCAB)\n",
    "model = mlp_vae(vocab)\n",
    "assert isinstance(model, nn.Module)\n",
    "model = conv_vae(vocab)\n",
    "assert isinstance(model, nn.Module)\n",
    "model = lstm_vae(vocab)\n",
    "assert isinstance(model, nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class FP_VAE_ZINC(PretrainedGenerativeAgent):\n",
    "    '''\n",
    "    FP_VAE_ZINC - MLP-to-LSTM VAE trained to \n",
    "    reconstruct SMILES from a ECFP6 fingerprint \n",
    "    using the ZINC database\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `drop_scale float`: dropout scale\n",
    "    \n",
    "    - `base_update float`: update fraction for the baseline model. Updates\n",
    "    the base model following `base_model = base_update*base_model + (1-base_update)*model`\n",
    "    \n",
    "    - `base_update_iter int`: update frequency for baseline model\n",
    "\n",
    "    - `base_model bool`: if False, baseline model will not be created\n",
    "\n",
    "    - `opt_kwargs dict`: dictionary of keyword arguments passed to `optim.Adam`\n",
    "\n",
    "    - `clip float`: gradient clipping\n",
    "\n",
    "    - `name str`: agent name\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 drop_scale=1.,\n",
    "                 base_update=0.97, \n",
    "                 base_update_iter=5,\n",
    "                 base_model=True,\n",
    "                 opt_kwargs={},\n",
    "                 clip=1.,\n",
    "                 name = 'fp_vae_zinc'\n",
    "                ):\n",
    "        \n",
    "        vocab = CharacterVocab(SMILES_CHAR_VOCAB)\n",
    "        model = mlp_vae(vocab, drop_scale=drop_scale)\n",
    "        weight_filename = 'fp_vae_zinc.pt'\n",
    "        \n",
    "        loss_function = CrossEntropy()\n",
    "        fp_function = partial(failsafe_fp, fp_function=ECFP6)\n",
    "        dataset = Vec_To_Text_Dataset(['C'], vocab, fp_function)\n",
    "        \n",
    "        \n",
    "        super().__init__(weight_filename,\n",
    "                         model,\n",
    "                         vocab, \n",
    "                         loss_function,\n",
    "                         dataset,\n",
    "                         base_update=base_update,\n",
    "                         base_update_iter=base_update_iter,\n",
    "                         base_model=base_model,\n",
    "                         opt_kwargs=opt_kwargs,\n",
    "                         clip=clip,\n",
    "                         name=name\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "agent = FP_VAE_ZINC()\n",
    "\n",
    "preds, _ = agent.model.sample_no_grad(100, 100)\n",
    "smiles = agent.reconstruct(preds)\n",
    "mols = to_mols(smiles)\n",
    "mols = [i for i in mols if i is not None]\n",
    "assert len(mols)>80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class FP_VAE_Chembl(PretrainedGenerativeAgent):\n",
    "    '''\n",
    "    FP_VAE_Chembl - MLP-to-LSTM VAE trained to \n",
    "    reconstruct SMILES from a ECFP6 fingerprint \n",
    "    using the Chembl database\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `drop_scale float`: dropout scale\n",
    "    \n",
    "    - `base_update float`: update fraction for the baseline model. Updates\n",
    "    the base model following `base_model = base_update*base_model + (1-base_update)*model`\n",
    "    \n",
    "    - `base_update_iter int`: update frequency for baseline model\n",
    "\n",
    "    - `base_model bool`: if False, baseline model will not be created\n",
    "\n",
    "    - `opt_kwargs dict`: dictionary of keyword arguments passed to `optim.Adam`\n",
    "\n",
    "    - `clip float`: gradient clipping\n",
    "\n",
    "    - `name str`: agent name\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 drop_scale=1.,\n",
    "                 base_update=0.97, \n",
    "                 base_update_iter=5,\n",
    "                 base_model=True,\n",
    "                 opt_kwargs={},\n",
    "                 clip=1.,\n",
    "                 name = 'fp_vae_chembl'\n",
    "                ):\n",
    "        \n",
    "        vocab = CharacterVocab(SMILES_CHAR_VOCAB)\n",
    "        model = mlp_vae(vocab, drop_scale=drop_scale)\n",
    "        weight_filename = 'fp_vae_chembl.pt'\n",
    "        \n",
    "        loss_function = CrossEntropy()\n",
    "        fp_function = partial(failsafe_fp, fp_function=ECFP6)\n",
    "        dataset = Vec_To_Text_Dataset(['C'], vocab, fp_function)\n",
    "        \n",
    "        super().__init__(weight_filename,\n",
    "                         model,\n",
    "                         vocab, \n",
    "                         loss_function,\n",
    "                         dataset,\n",
    "                         base_update=base_update,\n",
    "                         base_update_iter=base_update_iter,\n",
    "                         base_model=base_model,\n",
    "                         opt_kwargs=opt_kwargs,\n",
    "                         clip=clip,\n",
    "                         name=name\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "agent = FP_VAE_Chembl()\n",
    "\n",
    "preds, _ = agent.model.sample_no_grad(100, 100)\n",
    "smiles = agent.reconstruct(preds)\n",
    "mols = to_mols(smiles)\n",
    "mols = [i for i in mols if i is not None]\n",
    "assert len(mols)>80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class FP_VAE_ZINC_Selfies(PretrainedGenerativeAgent):\n",
    "    '''\n",
    "    FP_VAE_ZINC_Selfies - MLP-to-LSTM VAE trained to \n",
    "    reconstruct SMILES from a ECFP6 fingerprint \n",
    "    using the ZINC database with SELFIES encoding\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    - `drop_scale float`: dropout scale\n",
    "    \n",
    "    - `base_update float`: update fraction for the baseline model. Updates\n",
    "    the base model following `base_model = base_update*base_model + (1-base_update)*model`\n",
    "    \n",
    "    - `base_update_iter int`: update frequency for baseline model\n",
    "\n",
    "    - `base_model bool`: if False, baseline model will not be created\n",
    "\n",
    "    - `opt_kwargs dict`: dictionary of keyword arguments passed to `optim.Adam`\n",
    "\n",
    "    - `clip float`: gradient clipping\n",
    "\n",
    "    - `name str`: agent name\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 drop_scale=1.,\n",
    "                 base_update=0.97, \n",
    "                 base_update_iter=5,\n",
    "                 base_model=True,\n",
    "                 opt_kwargs={},\n",
    "                 clip=1.,\n",
    "                 name = 'fp_vae_zinc_selfies'\n",
    "                ):\n",
    "        \n",
    "        vocab = FuncVocab(SELFIES_VOCAB, split_selfie, \n",
    "                    prefunc=smile_to_selfie, postfunc=selfie_to_smile)\n",
    "        \n",
    "        model = mlp_vae(vocab, drop_scale=drop_scale)\n",
    "        weight_filename = 'fp_vae_zinc_selfies.pt'\n",
    "        \n",
    "        loss_function = CrossEntropy()\n",
    "        fp_function = partial(failsafe_fp, fp_function=ECFP6)\n",
    "        dataset = Vec_To_Text_Dataset(['C'], vocab, fp_function)\n",
    "        \n",
    "        \n",
    "        super().__init__(weight_filename,\n",
    "                         model,\n",
    "                         vocab, \n",
    "                         loss_function,\n",
    "                         dataset,\n",
    "                         base_update=base_update,\n",
    "                         base_update_iter=base_update_iter,\n",
    "                         base_model=base_model,\n",
    "                         opt_kwargs=opt_kwargs,\n",
    "                         clip=clip,\n",
    "                         name=name\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "agent = FP_VAE_ZINC_Selfies()\n",
    "\n",
    "preds, _ = agent.model.sample_no_grad(100, 100)\n",
    "smiles = agent.reconstruct(preds)\n",
    "mols = to_mols(smiles)\n",
    "mols = [i for i in mols if i is not None]\n",
    "assert len(mols)>80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
