# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/07_dataloaders.ipynb (unless otherwise specified).

__all__ = ['tokenize_by_character', 'tokenize_with_replacements', 'regex_tokenize', 'Vocab', 'CharacterVocab',
           'CharacterReplaceVocab', 'RegexVocab', 'test_reconstruction', 'SmilesDataset', 'SmilesPredictionDataset',
           'FPDataset', 'batch_sequences', 'lm_collate', 'sequence_prediction_collate', 'fp_collate']

# Cell
from .imports import *
from .torch_imports import *

# Cell

def tokenize_by_character(smile):
    return [i for i in smile]

def tokenize_with_replacements(smile, replacement_dict):
    for k,v in replacement_dict.items():
        smile = smile.replace(k,v)
    return [i for i in smile]

def regex_tokenize(smile, regex):
    tokens = [token for token in regex.findall(smile)]
    return tokens

# Cell

class Vocab():
    def __init__(self, itos):
        self.special_tokens = ['bos', 'eos', 'pad', 'unk']

        self.itos = self.special_tokens + [i for i in itos if not i in self.special_tokens]
        self.stoi = {self.itos[i]:i for i in range(len(self.itos))}
        self.unks = []

    def tokenize(self, input):
        raise NotImplementedError

    def numericalize(self, input):
        output = []
        for tok in input:
            if tok in self.stoi.keys():
                output.append(self.stoi[tok])
            else:
                output.append(self.stoi['unk'])
                self.unks.append(tok)
        return output

    def reconstruct(self, input):

        output = []
        for item in input:
            item = self.itos[item]
            if item=='eos':
                break

            if not item=='bos':
                output.append(item)

        return ''.join(output)

    def update_vocab(self):
        unks = list(set(self.unks))
        self.itos += unks
        self.stoi = {self.itos[i]:i for i in range(len(self.itos))}
        self.unks = []

    def update_vocab_from_data(self, inputs):
        _ = [self.numericalize(self.tokenize(i)) for i in inputs]
        self.update_vocab()


class CharacterVocab(Vocab):
    def tokenize(self, smile):
        toks = tokenize_by_character(smile)
        toks = ['bos'] + toks + ['eos']
        return toks


class CharacterReplaceVocab(Vocab):
    def __init__(self, itos, replace_dict):
        self.replace_dict = replace_dict
        self.reverse_dict = {v:k for k,v in replace_dict.items()}
        for rep in self.reverse_dict.keys():
            if not rep in itos:
                itos.append(rep)
        super().__init__(itos)

    def tokenize(self, smile):
        toks = tokenize_with_replacements(smile, self.replace_dict)
        toks = ['bos'] + toks + ['eos']
        return toks

    def reconstruct(self, input):
        output = []
        for item in input:
            item = self.itos[item]
            if item=='eos':
                break

            if not item=='bos':
                if item in self.reverse_dict.keys():
                    item = self.reverse_dict[item]

                output.append(item)

        return ''.join(output)


class RegexVocab(Vocab):
    def __init__(self, itos, pattern):
        super().__init__(itos)

        self.pattern = pattern
        self.regex = re.compile(self.pattern)

    def tokenize(self, smile):
        toks = regex_tokenize(smile, self.regex)
        toks = ['bos'] + toks + ['eos']
        return toks

# Cell

def test_reconstruction(vocab, inputs):
    fails = []
    for item in inputs:
        recon = vocab.reconstruct(vocab.numericalize(vocab.tokenize(item)))
        if not item==recon:
            fails.append((item, recon))

    return fails

# Cell

class SmilesDataset(Dataset):
    def __init__(self, smiles, vocab):
        self.smiles = smiles
        self.vocab = vocab

    def __len__(self):
        return len(self.smiles)

    def __getitem__(self, idx):
        smile = self.smiles[idx]
        tokens = self.vocab.tokenize(smile)
        ints = self.vocab.numericalize(tokens)
        ints = torch.LongTensor(ints)
        return ints


class SmilesPredictionDataset(SmilesDataset):
    def __init__(self, smiles, y_vals, vocab):
        super().__init__(smiles, vocab)
        self.y_vals = y_vals

    def __getitem__(self, idx):
        ints = super().__getitem__(idx)
        y_val = torch.FloatTensor(self.y_vals[i])
        return (ints, y_val)

# Cell

class FPDataset(Dataset):
    def __init__(self, smiles, y_vals, vocab, fp_function):
        self.smiles = smiles
        self.vocab = vocab
        self.y_vals = y_vals
        self.fp_function = fp_function

    def __len__(self):
        return len(self.smiles)

    def __getitem__(self, idx):
        smile = self.smiles[idx]
        fp = fp_function(smile)
        fp = torch.FloatTensor(fp)
        y_val = torch.FloatTensor(self.y_vals[idx])
        return (fp, y_val)

# Cell

def batch_sequences(sequences, pad_idx):

    max_len = max([len(i) for i in sequences])+1
    bs = len(sequences)

    batch_tensor = torch.zeros((bs, max_len)).long() + pad_idx

    for i,item in enumerate(sequences):
        batch_tensor[i,:item.shape[0]] = item

    return batch_tensor


def lm_collate(batch, pad_idx, batch_first=True):

    batch_tensor = batch_sequences(batch, pad_idx)

    if batch_first:
        output = (batch_tensor[:,:-1], batch_tensor[:,1:])
    else:
        batch_tensor = batch_tensor.T
        output = (batch_tensor[:-1,:], batch_tensor[1:,:])

    return output

def sequence_prediction_collate(batch, pad_idx, batch_first=True):

    batch_tensor = batch_sequences([i[0] for i in batch])
    y_vals = torch.stack([i[1] for i in batch])

    if not batch_first:
        batch_tensor = batch_tensor.T

    return (batch_tensor, y_vals)

def fp_collate(batch):
    fps = torch.stack([i[0] for i in batch])
    y_vals = torch.stack([i[1] for i in batch])
    return (fps, y_vals)