# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/06_layers.ipynb (unless otherwise specified).

__all__ = ['Linear', 'LSTMBase', 'LSTMLM']

# Cell
from .imports import *
from .torch_imports import *

# Cell

class Linear(nn.Module):
    def __init__(self, d_in, d_out, act=True, bn=False, dropout=0.):
        super().__init__()

        layers = [nn.Linear(d_in, d_out)]

        if bn:
            layers.append(nn.BatchNorm1d(d_out))

        if act:
            layers.append(nn.ReLU())

        if dropout>0.:
            layers.append(nn.Dropout(p=dropout))

        self.layers = nn.Sequential(*layers)

    def forward(self, x):
        return self.layers(x)

# Cell

class LSTMBase(nn.Module):
    def __init__(self, d_vocab, d_embedding, d_hidden, n_layers, pad_idx,
                 lstm_drop=0.):
        super().__init__()

        self.d_vocab = d_vocab
        self.d_embedding = d_embedding
        self.d_hidden = d_hidden
        self.n_layers = n_layers
        self.pad_idx = pad_idx

        self.embedding = nn.Embedding(d_vocab, d_embedding, padding_idx=pad_idx)

        self.lstms = []

        for i in range(n_layers):
            input_size = d_embedding if i==0 else d_hidden
            output_size = d_embedding if i==n_layers-1 else d_hidden

            lstm = nn.LSTM(input_size, output_size, 1, batch_first=True, dropout=lstm_drop)
            self.lstms.append(lstm)

        self.lstms = nn.ModuleList(self.lstms)

    def forward(self, x):
        x = self.embedding(x)

        hiddens = []
        for i, lstm in enumerate(self.lstms):
            x, (h,c) = lstm(x)
            hiddens.append((h.detach(), c.detach()))

        self.last_hiddens = hiddens

        return x

# class LMHead(nn.Module):
#     def __init__(self, d_in, d_out):
#         super().__init__()

#         self.layer = nn.Linear(d_in, d_out)

#     def forward(self, x):
#         return self.layer(x)

# Cell

class LSTMLM(nn.Module):
    def __init__(self, d_vocab, d_embedding, d_hidden, n_layers, pad_idx, lstm_drop=0., bos_idx=0):
        super().__init__()
        self.base = LSTMBase(d_vocab, d_embedding, d_hidden, n_layers, pad_idx, lstm_drop)
        self.head = Linear(d_embedding, d_vocab, act=False, bn=False, dropout=0.)
        self.bos_idx = bos_idx

    def forward(self, x):
        x = self.base(x)
        x = self.head(x)
        return x

    def sample(self, bs, sl, multinomial=True):

        preds = idxs = torch.tensor([vocab.stoi['bos']]*bs).long().unsqueeze(-1) # todo - cuda
#         lps = []

        hiddens = []

        for l in range(self.base.n_layers):
            if l==self.base.n_layers-1:
                d_hidden = self.base.d_embedding
            else:
                d_hidden = self.base.d_hidden

            h = torch.zeros((1, bs, d_hidden)).cuda()
            c = torch.zeros((1, bs, d_hidden)).cuda()
            hiddens.append((h,c))

        for i in range(sl):
            x = self.base.embedding(idxs)
            new_hidden = []
            for i, lstm in enumerate(self.base.lstms):
                x, (h,c) = lstm(x, hiddens[i])
                new_hidden.append((h.detach(), c.detach()))

            hiddens = new_hidden

            x = self.head(x)
            log_probs = F.log_softmax(x, -1).squeeze(1)
            probs = log_probs.detach().exp()

            if multinomial:
                idxs = torch.multinomial(probs, 1)
            else:
                idxs = x.argmax(-1)


            preds = torch.cat([preds, idxs], -1)

        return preds[:, 1:]