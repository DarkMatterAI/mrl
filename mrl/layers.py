# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/06_layers.ipynb (unless otherwise specified).

__all__ = ['Linear']

# Cell
from .imports import *
from .torch_imports import *

# Cell

class Linear(nn.Module):
    def __init__(self, d_in, d_out, act=True, bn=False, dropout=0.):
        super().__init__()

        layers = [nn.Linear(d_in, d_out)]

        if bn:
            layers.append(nn.BatchNorm1d(d_out))

        if act:
            layers.append(nn.ReLU())

        if dropout>0.:
            layers.append(nn.Dropout(p=dropout))

        self.layers = nn.Sequential(*layers)

    def forward(self, x):
        return self.layers(x)