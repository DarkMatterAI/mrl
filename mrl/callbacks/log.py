# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/13_logging.ipynb (unless otherwise specified).

__all__ = ['log_to_df', 'Log', 'StatsCallback', 'MaxCallback', 'PercentileCallback']

# Cell

from ..imports import *
from ..core import *
from .core import *
from ..torch_imports import *

# Cell

def log_to_df(log, keys=None):
    batch = 0
    output_dict = defaultdict(list)

    if keys is None:
        keys = list(log.keys())

    items = log[keys[0]]
    for item in items:
        output_dict['batch'] += [batch]*len(item)
        batch += 1

    for key in keys:
        output_dict[key] = flatten_list_of_lists(log[key])

    return pd.DataFrame(output_dict)

# Cell

class Log(Callback):
    def __init__(self):
        super().__init__(name='log', order=100)

        self.pbar = None
        self.iterations = 0
        self.metrics = {}

        self.batch_log = {}
        self.timelog = defaultdict(list)

        self.report = 1
        self.unique_samples = {}

        self.log_df = None

        self.add_metric('rewards')
        self.add_metric('rewards_final')
        self.add_metric('new')
        self.add_metric('diversity')
        self.add_metric('bs')

        self.add_log('samples')
        self.add_log('sources')
        self.add_log('rewards')
        self.add_log('rewards_final')

    def setup(self):
        self.df = pd.DataFrame(self.batch_log)

    def before_train(self):
        cols = ['iterations'] + list(self.metrics.keys())
        if self.pbar is None:
            print('\t'.join(cols))
        else:
            self.pbar.write(cols, table=True)

    def add_metric(self, name):
        if not name in self.metrics.keys():
            self.metrics[name] = []

    def add_log(self, name):
        if not name in self.batch_log.keys():
            self.batch_log[name] = []

    def update_metric(self, name, value):
        self.metrics[name].append(value)

    def after_sample(self):
        env = self.environment
        batch_state = env.batch_state
        samples = batch_state.samples

        new = np.array([not i in self.unique_samples for i in samples])

        self.update_metric('new', new.mean())

        diversity = len(set(samples))/len(samples)
        self.environment.log.update_metric('diversity', diversity)

        self.environment.log.update_metric('bs', len(batch_state.samples))

    def after_compute_reward(self):
        env = self.environment
        batch_state = env.batch_state
        samples = batch_state.samples
        rewards = batch_state.rewards
        batch_state.rewards_final = rewards.clone().detach()

        rewards = rewards.detach().cpu().numpy()

        self.update_metric('rewards', rewards.mean())

        for i in range(len(samples)):
            if not samples[i] in self.unique_samples:
                self.unique_samples[samples[i]] = rewards[i]

    def after_reward_modification(self):
        env = self.environment
        rewards = env.batch_state.rewards_final.detach().cpu().numpy()
        self.update_metric('rewards_final', rewards.mean())


    def update_log(self):
        env = self.environment
        batch_state = env.batch_state
        samples = batch_state.samples
        update_dict = {}

        for key in self.batch_log.keys():
            items = batch_state[key]
            if isinstance(items, torch.Tensor):
                items = items.detach().cpu().numpy()
            self.batch_log[key].append(items)
            update_dict[key] = items

        new_df = pd.DataFrame(update_dict)
        repeats = new_df.samples.isin(self.df.samples)
        new_df = new_df[~repeats]

        self.df = self.df.append(new_df, ignore_index=True)

        if self.iterations%10==0 and self.iterations>0:
            self.df.drop_duplicates(subset='samples', inplace=True)

    def report_batch(self):
        outputs = [f'{self.iterations}']
        if self.iterations%self.report==0:

            for k,v in self.metrics.items():
                val = v[-1]

                if type(val)==int:
                    val = f'{val}'
                elif type(val)==str:
                    val = val
                else:
                    val = f'{val:.3f}'

                outputs.append(val)

            if self.pbar is None:
                print('\t'.join(outputs))
            else:
                self.pbar.write(outputs, table=True)

        self.iterations += 1

    def after_batch(self):
        self.update_log()
        self.report_batch()

    def get_df(self):
        return log_to_df(self.batch_log)

    def plot_metrics(self, cols=4, smooth=True):
        self.plot_dict(self.metrics, cols=cols, smooth=smooth)

    def plot_timelog(self, cols=4, smooth=True):
        self.plot_dict(self.timelog, cols=cols, smooth=smooth)


# Cell

class StatsCallback(Callback):
    # grabs from batch_state based on name
    def __init__(self, batch_attribute, grabname=None, name='stats', order=20):
        super().__init__(name=name, order=order)
        self.grabname = grabname
        self.batch_attribute = batch_attribute

    def get_values(self):
        batch_state = self.environment.batch_state
        sources = np.array(batch_state.sources)
        values = batch_state[self.batch_attribute]

        if self.grabname is not None:
            source_mask = sources==self.grabname
            values = values[source_mask]

        if isinstance(values, torch.Tensor):
            values = values.detach().cpu().numpy()

        return values

class MaxCallback(StatsCallback):
    def __init__(self, batch_attribute, grabname, order=20):

        if grabname is None:
            name = f'{batch_attribute}_max'
        else:
            name = f'{batch_attribute}_{grabname}_max'

        super().__init__(batch_attribute, grabname, name=name)


    def setup(self):
        log = self.environment.log
        log.add_metric(self.name)

    def after_compute_reward(self):

        values = self.get_values()
        self.environment.log.update_metric(self.name, values.max())

class PercentileCallback(StatsCallback):
    def __init__(self, batch_attribute, grabname, percentile, order=20):

        if grabname is None:
            name = f'{batch_attribute}_p{percentile}'
        else:
            name = f'{batch_attribute}_{grabname}_p{percentile}'

        super().__init__(batch_attribute, grabname, name=name)
        self.percentile = percentile

    def setup(self):
        log = self.environment.log
        log.add_metric(self.name)

    def after_compute_reward(self):

        values = self.get_values()
        self.environment.log.update_metric(self.name, np.percentile(values, self.percentile))
