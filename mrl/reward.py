# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/21_reward.ipynb (unless otherwise specified).

__all__ = ['Reward', 'ContrastiveReward', 'FunctionReward', 'NoveltyReward']

# Cell

from .imports import *
from .core import *
from .callbacks import *
from .torch_imports import *
from .torch_core import *

# Cell

class Reward(Callback):
    def __init__(self, name, sample_name='samples',
                 weight=1., bs=None,
                 order=10, track=True, log=True):
        super().__init__(name=name, order=order)

        self.sample_name = sample_name
        self.weight = weight
        self.bs = bs
        self.track = track
        self.log = log
        self.score_log = {}

    def load_data(self, samples, values):
        for i in range(len(samples)):
            self.score_log[samples[i]] = values[i]

    def setup(self):
        log = self.environment.log
        log.add_log(self.name)
        if self.track:
            log.add_metric(self.name)

    def _compute_reward(self, samples):
        return [0. for i in samples]

    def compute_batched_reward(self, samples):
        if self.bs is not None:
            sample_chunks = chunk_list(samples, self.bs)
            rewards = []
            for chunk in sample_chunks:
                rewards_iter = self._compute_reward(chunk)
                if isinstance(rewards_iter, torch.Tensor):
                    rewards_iter = rewards_iter.detach().cpu()

                rewards += list(rewards_iter)

        else:
            rewards = self._compute_reward(samples)
            if isinstance(rewards, torch.Tensor):
                rewards = rewards.detach().cpu()

        return rewards

    def compute_reward(self):
        env = self.environment
        batch_state = env.batch_state
        samples = batch_state[self.sample_name]
        prescored = batch_state.prescored

        rewards = np.array([0. for i in samples])

        to_score = []
        to_score_idxs = []

        for i in range(len(samples)):
            if not prescored[i]:

#                 if self.log:
#                     if samples[i] in self.score_log:
#                         rewards[i] = self.score_log[samples[i]]
#                     else:
#                         to_score.append(samples[i])
#                         to_score_idxs.append(i)

#                 else:
#                     to_score.append(samples[i])
#                     to_score_idxs.append(i)

                if (self.log and (not samples[i] in self.score_log)) or (not self.log):
                    to_score.append(samples[i])
                    to_score_idxs.append(i)

        new_rewards = self.compute_batched_reward(to_score)

        for i in range(len(to_score)):
            batch_idx = to_score_idxs[i]
            reward = new_rewards[i]
            rewards[batch_idx] = reward

            if self.log:
                self.score_log[to_score[i]] = reward

        rewards = to_device(torch.tensor(rewards).float()).squeeze()
        rewards = rewards * self.weight

        batch_state.rewards += rewards
        batch_state[self.name] = rewards

        if self.track:
            env.log.update_metric(self.name, rewards.mean().detach().cpu().numpy())


#     def compute_reward(self):
#         env = self.environment
#         batch_state = env.batch_state
#         samples = batch_state[self.sample_name]

#         if self.log:
#             to_score = [i for i in samples if not i in self.score_log.keys()]
#             rewards = self.compute_batched_reward(to_score)

#             for i in range(len(to_score)):
#                 self.score_log[to_score[i]] = rewards[i]

#             rewards = [self.score_log[i] for i in samples]

#         else:
#             rewards = self.compute_batched_reward(samples)

#         rewards = to_device(torch.tensor(rewards).float()).squeeze()
#         rewards = rewards * self.weight

#         batch_state.rewards += rewards
#         batch_state[self.name] = rewards

#         if self.track:
#             env.log.update_metric(self.name, rewards.mean().detach().cpu().numpy())



# Cell

class ContrastiveReward(Reward):
    def __init__(self, base_reward, max_score=None):
        super().__init__(name = base_reward.name,
                         sample_name = base_reward.sample_name,
                         weight = base_reward.weight,
                         bs = base_reward.bs,
                         order = base_reward.order,
                         track = base_reward.track,
                         log = base_reward.log)

        self.base_reward = base_reward
        self.max_score = max_score


    def __call__(self, event_name):

        event = getattr(self, event_name, None)

        if event is not None:
            output = event()
        else:
            output = None

        if not event_name=='compute_reward':
            _ = self.base_reward(event_name)

        return output

    def setup(self):
        self.base_reward.environment = self.environment

    def compute_and_clean(self, samples):
        rewards = self.base_reward._compute_reward(samples)
        if isinstance(rewards, torch.Tensor):
            rewards = rewards.detach().cpu()

        rewards = np.array(rewards)
        return rewards

    def _compute_reward(self, samples):
        source_samples = [i[0] for i in samples]
        target_samples = [i[1] for i in samples]

        source_rewards = self.compute_and_clean(source_samples)
        target_rewards = self.compute_and_clean(target_samples)

        rewards = target_rewards - source_rewards
        if self.max_score is not None:
            rewards = rewards / (self.max_score-source_rewards)

        return list(rewards)

# Cell

class FunctionReward(Reward):
    def __init__(self, reward_function, name, sample_name='samples',
                 weight=1., bs=None,
                 order=10, track=True, log=True):
        super().__init__(name=name,
                         sample_name=sample_name,
                         weight=weight,
                         bs=bs,
                         order=order,
                         track=track,
                         log=log)

        self.reward_function = reward_function


    def _compute_reward(self, samples):
        rewards = []
        if samples:
            rewards = self.reward_function(samples)
        return rewards




# Cell

class NoveltyReward(Reward):
    def __init__(self, weight=1., track=True):
        super().__init__(name='novel',
                         sample_name='samples',
                         weight=weight,
                         order=10,
                         track=track,
                         log=False)

    def _compute_reward(self, samples):
        log = self.environment.log
        old = log.unique_samples
        new = [not i in old for i in samples]
        new = [float(i) for i in new]
        return new