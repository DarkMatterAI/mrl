# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/12_reward.ipynb (unless otherwise specified).

__all__ = ['Reward', 'trajectory_wrapper', 'RewardModule', 'MolReward', 'MLReward', 'FPModelReward',
           'SequenceModelReward']

# Cell

from .imports import *
from .torch_imports import *
from .torch_core import *
from .chem import *
from .templates import *
from .agent import *

# Cell

class Reward():
    def __init__(self, template=None, reward_modules=[], trajectory_modules=[]):

        if template == None:
            template = Template([])

        self.template = template
        self.reward_modules = reward_modules
        self.trajectory_modules = trajectory_modules
        self.mean_reward = None

    def __call__(self, model_output):

        template_passes = np.array(np.array(self.template(model_output['sequences'])))
        template_rewards = np.array(self.template.eval_mols(model_output['sequences']))

        rewards = self.compute_rewards(model_output, template_passes)
        trajectory_rewards = self.compute_trajectory_reward(model_output, template_passes)

        rewards = template_rewards + rewards

        if self.mean_reward is None:
            self.mean_reward = rewards.mean()
        else:
            self.mean_reward = (1-reward_decay)*rewards.mean() + reward_decay*self.mean_reward

        rewards_scaled = rewards - self.mean_rewards

        model_output['rewards'] = rewards
        model_output['rewards_scaled'] = rewards_scaled
        model_output['trajectory_rewards'] = trajectory_rewards

        return model_output

    def compute_trajectory_reward(self, model_output, template_passes):

        all_rewards = []

        for rm in self.trajectory_modules:
            all_rewards.append(rm(model_output, template_passes))

        all_rewards = np.stack(all_rewards, -1)
        all_rewards = all_rewards.sum(-1)
        return all_rewards

    def compute_rewards(self, model_output, template_passes):

        all_rewards = []

        for rm in self.reward_modules:
            all_rewards.append(rm(model_output, template_passes))

        all_rewards = np.stack(all_rewards, -1)
        all_rewards = all_rewards.sum(-1)
        return all_rewards

# Cell

def trajectory_wrapper(inputs, function):
    return np.array([function(i) for i in inputs])

# Cell

class RewardModule():

    def __call__(self, model_output, template_passes=None):

        reward_inputs = self.prepare_reward_inputs(model_output, template_passes)
        reward_outputs = self.reward_function(reward_inputs)
        final_reward = self.aggregate_reward(reward_outputs, model_output, template_passes)
        return final_reward

    def aggregate_reward(self, reward_outputs, model_output, template_passes=None):
        pass

    def prepare_reward_inputs(self, model_output, template_passes=None):
        pass

    def reward_function(self, inputs):
        pass

class MolReward(RewardModule):
    def __init__(self, mol_function, trajectory=False):
        self.mol_function = mol_function
        self.trajectory = trajectory

    def aggregate_reward(self, reward_outputs, model_output, template_passes=None):

        if template_passes is not None:
            passed_idxs = np.array([i for i in range(len(template_passes)) if template_passes[i]])
            bs = len(template_passes)
        else:
            passed_idxs = np.arange(len(reward_outputs))
            bs = len(reward_outputs)

        if self.trajectory:
            outputs = np.zeros((bs, model_output['sl']))

            for i, idx in enumerate(passed_idxs):
                traj = reward_outputs[i]
                traj_len = len(traj)
                outputs[idx, :traj_len] = traj

        else:
            outputs = np.zeros((bs))
            outputs[passed_idxs] = reward_outputs

        return outputs

    def prepare_reward_inputs(self, model_output, template_passes=None):

        if self.trajectory:
            inputs = model_output['sequence_trajectories']
        else:
            inputs = model_output['sequences']

        output = np.zeros((len(inputs)))

        if template_passes is not None:
            inputs = [inputs[i] for i in range(len(inputs)) if template_passes[i]]

        return inputs

    def reward_function(self, inputs):
        if self.trajectory:
            func = partial(trajectory_wrapper, function=self.mol_function)
        else:
            func = self.mol_function

        return maybe_parallel(func, inputs)

class MLReward():
    def __init__(self, model, trajectory=False):
        self.model = model
        self.trajectory = trajectory

    def reward_function(self, inputs):
        if not type(inputs)==list:
            inputs = [inputs]
        return np.array(self.model(*inputs).detach().cpu())

    def prepare_reward_inputs(self, model_output, template_passes=None):
        raise NotImplementedError

    def aggregate_reward(self, reward_outputs, model_output, template_passes=None):
        if template_passes is not None:
            passed_idxs = np.array([i for i in range(len(template_passes)) if template_passes[i]])
            bs = len(template_passes)
        else:
            passed_idxs = np.arange(len(reward_outputs))
            bs = len(reward_outputs)

        if reward_outputs.ndim==2:
            output = np.zeros((bs, reward_outputs.shape[-1]))
            output[passed_idxs] = reward_output

        else:
            output = np.zeros((bs,))
            output[passed_idxs] = reward_outputs

        return output

class FPModelReward(MLReward):
    def __init__(self, model, fp_func, trajectory=False):
        super().__init__(model, trajectory)
        self.fp_func = fp_func

    def prepare_reward_inputs(self, model_output, template_passes=None):

        smiles = model_output['sequences']
        fps = np.stack(maybe_parallel(self.fp_func, smiles))
        fps = to_device(torch.from_numpy(fps).float())
        return fps

class SequenceModelReward(MLReward):

    def prepare_reward_inputs(self, model_output, template_passes=None):

        return model_output['x']