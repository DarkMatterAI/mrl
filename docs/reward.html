---

title: Reward

keywords: fastai
sidebar: home_sidebar

summary: "Rewards - non-differentiable scores for samples"
description: "Rewards - non-differentiable scores for samples"
nb_path: "nbs/21_reward.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/21_reward.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Reward" class="doc_header"><code>class</code> <code>Reward</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/reward.py#L15" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Reward</code>(<strong><code>reward_function</code></strong>, <strong><code>weight</code></strong>=<em><code>1</code></em>, <strong><code>bs</code></strong>=<em><code>None</code></em>, <strong><code>log</code></strong>=<em><code>True</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="RewardCallback" class="doc_header"><code>class</code> <code>RewardCallback</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/reward.py#L87" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>RewardCallback</code>(<strong><code>reward</code></strong>, <strong><code>name</code></strong>, <strong><code>sample_name</code></strong>=<em><code>'samples'</code></em>, <strong><code>order</code></strong>=<em><code>10</code></em>, <strong><code>track</code></strong>=<em><code>True</code></em>) :: <a href="/mrl/callback_core#Callback"><code>Callback</code></a></p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="RewardModification" class="doc_header"><code>class</code> <code>RewardModification</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/reward.py#L120" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>RewardModification</code>(<strong><code>reward</code></strong>, <strong><code>name</code></strong>, <strong><code>sample_name</code></strong>=<em><code>'samples'</code></em>, <strong><code>order</code></strong>=<em><code>10</code></em>, <strong><code>track</code></strong>=<em><code>True</code></em>) :: <a href="/mrl/callback_core#Callback"><code>Callback</code></a></p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="NoveltyReward" class="doc_header"><code>class</code> <code>NoveltyReward</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/reward.py#L153" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>NoveltyReward</code>(<strong><code>weight</code></strong>=<em><code>1.0</code></em>, <strong><code>track</code></strong>=<em><code>True</code></em>) :: <a href="/mrl/callback_core#Callback"><code>Callback</code></a></p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ContrastiveReward" class="doc_header"><code>class</code> <code>ContrastiveReward</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/reward.py#L185" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ContrastiveReward</code>(<strong><code>base_reward</code></strong>, <strong><code>max_score</code></strong>=<em><code>None</code></em>) :: <a href="/mrl/reward#RewardCallback"><code>RewardCallback</code></a></p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># class Reward(Callback):</span>
<span class="c1">#     def __init__(self, name, sample_name=&#39;samples&#39;, </span>
<span class="c1">#                  weight=1., bs=None,</span>
<span class="c1">#                  order=10, track=True, log=True):</span>
<span class="c1">#         super().__init__(name=name, order=order)</span>
        
<span class="c1">#         self.sample_name = sample_name</span>
<span class="c1">#         self.weight = weight</span>
<span class="c1">#         self.bs = bs</span>
<span class="c1">#         self.track = track</span>
<span class="c1">#         self.log = log</span>
<span class="c1">#         self.score_log = {}</span>
        
<span class="c1">#     def load_data(self, samples, values):</span>
<span class="c1">#         for i in range(len(samples)):</span>
<span class="c1">#             self.score_log[samples[i]] = values[i]</span>
        
<span class="c1">#     def setup(self):</span>
<span class="c1">#         log = self.environment.log</span>
<span class="c1">#         log.add_log(self.name)</span>
<span class="c1">#         if self.track:</span>
<span class="c1">#             log.add_metric(self.name)</span>
            
<span class="c1">#     def _compute_reward(self, samples):</span>
<span class="c1">#         return [0. for i in samples]</span>
    
<span class="c1">#     def compute_batched_reward(self, samples):</span>
<span class="c1">#         if self.bs is not None:</span>
<span class="c1">#             sample_chunks = chunk_list(samples, self.bs)</span>
<span class="c1">#             rewards = []</span>
<span class="c1">#             for chunk in sample_chunks:</span>
<span class="c1">#                 rewards_iter = self._compute_reward(chunk)</span>
<span class="c1">#                 if isinstance(rewards_iter, torch.Tensor):</span>
<span class="c1">#                     rewards_iter = rewards_iter.detach().cpu()</span>
                    
<span class="c1">#                 rewards += list(rewards_iter)</span>
            
<span class="c1">#         else:</span>
<span class="c1">#             rewards = self._compute_reward(samples)</span>
<span class="c1">#             if isinstance(rewards, torch.Tensor):</span>
<span class="c1">#                 rewards = rewards.detach().cpu()</span>
            
<span class="c1">#         return rewards</span>
    
<span class="c1">#     def compute_reward(self):</span>
<span class="c1">#         env = self.environment</span>
<span class="c1">#         batch_state = env.batch_state</span>
<span class="c1">#         samples = batch_state[self.sample_name]</span>
        
<span class="c1">#         rewards = np.array([0. for i in samples])</span>
        
<span class="c1">#         to_score = []</span>
<span class="c1">#         to_score_idxs = []</span>
        
<span class="c1">#         for i in range(len(samples)):</span>
                
<span class="c1">#             if self.log:</span>
<span class="c1">#                 if samples[i] in self.score_log:</span>
<span class="c1">#                     rewards[i] = self.score_log[samples[i]]</span>
<span class="c1">#                 else:</span>
<span class="c1">#                     to_score.append(samples[i])</span>
<span class="c1">#                     to_score_idxs.append(i)</span>

<span class="c1">#             else:</span>
<span class="c1">#                 to_score.append(samples[i])</span>
<span class="c1">#                 to_score_idxs.append(i)</span>
                    
<span class="c1">#         new_rewards = self.compute_batched_reward(to_score)</span>
        
<span class="c1">#         for i in range(len(to_score)):</span>
<span class="c1">#             batch_idx = to_score_idxs[i]</span>
<span class="c1">#             reward = new_rewards[i]</span>
<span class="c1">#             rewards[batch_idx] = reward</span>
            
<span class="c1">#             if self.log:</span>
<span class="c1">#                 self.score_log[to_score[i]] = reward</span>
                
<span class="c1">#         rewards = to_device(torch.tensor(rewards).float()).squeeze()</span>
<span class="c1">#         rewards = rewards * self.weight</span>
        
<span class="c1">#         batch_state.rewards += rewards</span>
<span class="c1">#         batch_state[self.name] = rewards</span>
        
<span class="c1">#         if self.track:</span>
<span class="c1">#             env.log.update_metric(self.name, rewards.mean().detach().cpu().numpy())</span>
        
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># class RewardModification(Callback):</span>
<span class="c1">#     def __init__(self, name, sample_name=&#39;samples&#39;, </span>
<span class="c1">#                  weight=1., bs=None,</span>
<span class="c1">#                  order=10, track=True):</span>
<span class="c1">#         super().__init__(name=name, order=order)</span>
        
<span class="c1">#         self.sample_name = sample_name</span>
<span class="c1">#         self.weight = weight</span>
<span class="c1">#         self.bs = bs</span>
<span class="c1">#         self.track = track</span>
        
<span class="c1">#     def setup(self):</span>
<span class="c1">#         log = self.environment.log</span>
<span class="c1">#         log.add_log(self.name)</span>
<span class="c1">#         if self.track:</span>
<span class="c1">#             log.add_metric(self.name)</span>
            
<span class="c1">#     def _compute_reward_modification(self, samples):</span>
<span class="c1">#         return [0. for i in samples]</span>
    
<span class="c1">#     def compute_batched_reward_modification(self, samples):</span>
<span class="c1">#         if self.bs is not None:</span>
<span class="c1">#             sample_chunks = chunk_list(samples, self.bs)</span>
<span class="c1">#             rewards = []</span>
<span class="c1">#             for chunk in sample_chunks:</span>
<span class="c1">#                 rewards_iter = self._compute_reward_modification(chunk)</span>
<span class="c1">#                 if isinstance(rewards_iter, torch.Tensor):</span>
<span class="c1">#                     rewards_iter = rewards_iter.detach().cpu()</span>
                    
<span class="c1">#                 rewards += list(rewards_iter)</span>
            
<span class="c1">#         else:</span>
<span class="c1">#             rewards = self._compute_reward_modification(samples)</span>
<span class="c1">#             if isinstance(rewards, torch.Tensor):</span>
<span class="c1">#                 rewards = rewards.detach().cpu()</span>
            
<span class="c1">#         return rewards</span>
    
<span class="c1">#     def reward_modification(self):</span>
<span class="c1">#         env = self.environment</span>
<span class="c1">#         batch_state = env.batch_state</span>
<span class="c1">#         samples = batch_state[self.sample_name]</span>
        
<span class="c1">#         rewards = self.compute_batched_reward_modification(samples)</span>
                
<span class="c1">#         rewards = to_device(torch.tensor(rewards).float()).squeeze()</span>
<span class="c1">#         rewards = rewards * self.weight</span>
        
<span class="c1">#         batch_state.rewards_final += rewards</span>
<span class="c1">#         batch_state[self.name] = rewards</span>
        
<span class="c1">#         if self.track:</span>
<span class="c1">#             env.log.update_metric(self.name, rewards.mean().detach().cpu().numpy())</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># class ContrastiveReward(Reward):</span>
<span class="c1">#     def __init__(self, base_reward, max_score=None):</span>
<span class="c1">#         super().__init__(name = base_reward.name,</span>
<span class="c1">#                          sample_name = base_reward.sample_name,</span>
<span class="c1">#                          weight = base_reward.weight,</span>
<span class="c1">#                          bs = base_reward.bs,</span>
<span class="c1">#                          order = base_reward.order,</span>
<span class="c1">#                          track = base_reward.track,</span>
<span class="c1">#                          log = base_reward.log)</span>
        
<span class="c1">#         self.base_reward = base_reward</span>
<span class="c1">#         self.max_score = max_score</span>
        
        
<span class="c1">#     def __call__(self, event_name):</span>
        
<span class="c1">#         event = getattr(self, event_name, None)</span>
        
<span class="c1">#         if event is not None:</span>
<span class="c1">#             output = event()</span>
<span class="c1">#         else:</span>
<span class="c1">#             output = None</span>
            
<span class="c1">#         if not event_name==&#39;compute_reward&#39;:</span>
<span class="c1">#             _ = self.base_reward(event_name)</span>
            
<span class="c1">#         return output</span>
        
<span class="c1">#     def compute_and_clean(self, samples):</span>
<span class="c1">#         rewards = self.base_reward._compute_reward(samples)</span>
<span class="c1">#         if isinstance(rewards, torch.Tensor):</span>
<span class="c1">#             rewards = rewards.detach().cpu()</span>
            
<span class="c1">#         rewards = np.array(rewards)</span>
<span class="c1">#         return rewards</span>
        
<span class="c1">#     def _compute_reward(self, samples):</span>
<span class="c1">#         source_samples = [i[0] for i in samples]</span>
<span class="c1">#         target_samples = [i[1] for i in samples]</span>
        
<span class="c1">#         source_rewards = self.compute_and_clean(source_samples)</span>
<span class="c1">#         target_rewards = self.compute_and_clean(target_samples)</span>
        
<span class="c1">#         rewards = target_rewards - source_rewards</span>
<span class="c1">#         if self.max_score is not None:</span>
<span class="c1">#             rewards = rewards / (self.max_score-source_rewards)</span>
            
<span class="c1">#         return list(rewards)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># class FunctionReward(Reward):</span>
<span class="c1">#     def __init__(self, reward_function, name, sample_name=&#39;samples&#39;, </span>
<span class="c1">#                  weight=1., bs=None,</span>
<span class="c1">#                  order=10, track=True, log=True):</span>
<span class="c1">#         super().__init__(name=name, </span>
<span class="c1">#                          sample_name=sample_name, </span>
<span class="c1">#                          weight=weight,</span>
<span class="c1">#                          bs=bs,</span>
<span class="c1">#                          order=order, </span>
<span class="c1">#                          track=track, </span>
<span class="c1">#                          log=log)</span>
        
<span class="c1">#         self.reward_function = reward_function</span>
        
        
<span class="c1">#     def _compute_reward(self, samples):</span>
<span class="c1">#         rewards = []</span>
<span class="c1">#         if samples:</span>
<span class="c1">#             rewards = self.reward_function(samples)</span>
<span class="c1">#         return rewards</span>
        
        
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># class NoveltyReward(RewardModification):</span>
<span class="c1">#     def __init__(self, weight=1., track=True):</span>
<span class="c1">#         super().__init__(name=&#39;novel&#39;, </span>
<span class="c1">#                          sample_name=&#39;samples&#39;,</span>
<span class="c1">#                          weight=weight, </span>
<span class="c1">#                          order=10, </span>
<span class="c1">#                          track=track)</span>
        
        
<span class="c1">#     def _compute_reward_modification(self, samples):</span>
        
<span class="c1">#         df = self.environment.log.df</span>
        
<span class="c1">#         new = (~pd.Series(samples).isin(df.samples)).values</span>

<span class="c1">#         new = [float(i) for i in new]</span>
<span class="c1">#         return new</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

</div>
 

