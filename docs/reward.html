---

title: Reward

keywords: fastai
sidebar: home_sidebar

summary: "Rewards - non-differentiable scores for samples"
description: "Rewards - non-differentiable scores for samples"
nb_path: "nbs/21_reward.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/21_reward.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Reward" class="doc_header"><code>class</code> <code>Reward</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/reward.py#L15" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Reward</code>(<strong><code>reward_function</code></strong>, <strong><code>weight</code></strong>=<em><code>1</code></em>, <strong><code>bs</code></strong>=<em><code>None</code></em>, <strong><code>log</code></strong>=<em><code>True</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="RewardCallback" class="doc_header"><code>class</code> <code>RewardCallback</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/reward.py#L88" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>RewardCallback</code>(<strong><code>reward</code></strong>, <strong><code>name</code></strong>, <strong><code>sample_name</code></strong>=<em><code>'samples'</code></em>, <strong><code>order</code></strong>=<em><code>10</code></em>, <strong><code>track</code></strong>=<em><code>True</code></em>) :: <a href="/mrl/callback_core#Callback"><code>Callback</code></a></p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="RewardModification" class="doc_header"><code>class</code> <code>RewardModification</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/callbacks/core.py#L252" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>RewardModification</code>() :: <a href="/mrl/callback_core#Event"><code>Event</code></a></p>
</blockquote>
<p>RewardModification</p>
<p>This event is used to modify rewards before they
are used to compute the model's loss. Reward modifications
encompass changes to rewards in the context of the current
training cycle. These are things like "give a score bonus
to new samples that havent't been seen before" or "penalize
the score of samples that have occurred in the last 5 batches".</p>
<p>These types of modifications are kept separate from the core
reward for logging purposes. Samples are logged with their
respective rewards. These logged scores are referenced later
when samples are drawn from the log. This means we need the
logged score to be independent from "batch context" type scores</p>
<p>All reward modifications should be
applied to <code>self.environmemnt.batch_state.rewards</code></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="NoveltyReward" class="doc_header"><code>class</code> <code>NoveltyReward</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/reward.py#L154" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>NoveltyReward</code>(<strong><code>weight</code></strong>=<em><code>1.0</code></em>, <strong><code>track</code></strong>=<em><code>True</code></em>) :: <a href="/mrl/callback_core#Callback"><code>Callback</code></a></p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ContrastiveReward" class="doc_header"><code>class</code> <code>ContrastiveReward</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/reward.py#L186" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ContrastiveReward</code>(<strong><code>base_reward</code></strong>, <strong><code>max_score</code></strong>=<em><code>None</code></em>) :: <a href="/mrl/reward#RewardCallback"><code>RewardCallback</code></a></p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

