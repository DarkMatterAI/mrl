---

title: Layers

keywords: fastai
sidebar: home_sidebar

summary: "Pytorch model layers"
description: "Pytorch model layers"
nb_path: "nbs/06_layers.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/06_layers.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LinearBlock" class="doc_header"><code>class</code> <code>LinearBlock</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/layers.py#L14" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LinearBlock</code>(<strong><code>d_in</code></strong>, <strong><code>d_out</code></strong>, <strong><code>act</code></strong>=<em><code>True</code></em>, <strong><code>bn</code></strong>=<em><code>False</code></em>, <strong><code>dropout</code></strong>=<em><code>0.0</code></em>, <strong>**<code>lin_kwargs</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>LinearBlock - Combined linear, batchnorm, ReLU and dropout.
Layers are executed in the order linear, batchnorm, ReLU, dropout.
Batchnorm, activation and dropout layers are optional</p>
<p>Inputs:</p>
<ul>
<li><p><code>d_in int</code>: number of input dimensions</p>
</li>
<li><p><code>d_out int</code>: number of output dimensions</p>
</li>
<li><p><code>act bool</code>: if True, applies a ReLU activation</p>
</li>
<li><p><code>bn bool</code>: if True, applies 1d batchnorm</p>
</li>
<li><p><code>dropout float</code>: dropout percentage</p>
</li>
<li><p><code>**lin_kwargs dict</code>: keyword args passed to nn.Linear</p>
</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">layer</span> <span class="o">=</span> <span class="n">LinearBlock</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">bn</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">128</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ValueHead" class="doc_header"><code>class</code> <code>ValueHead</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/layers.py#L56" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ValueHead</code>(<strong><code>d_in</code></strong>, <strong><code>dropout</code></strong>=<em><code>0.0</code></em>) :: <code>Module</code></p>
</blockquote>
<p>ValueHead - used in RL algorithms to predict state values</p>
<p>Inputs:</p>
<ul>
<li><code>d_in int</code>: number of input dimensions</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Conv" class="doc_header"><code>class</code> <code>Conv</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/layers.py#L76" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Conv</code>(<strong><code>d_in</code></strong>, <strong><code>d_out</code></strong>, <strong><code>ks</code></strong>=<em><code>3</code></em>, <strong><code>stride</code></strong>=<em><code>1</code></em>, <strong><code>padding</code></strong>=<em><code>None</code></em>, <strong><code>ndim</code></strong>=<em><code>2</code></em>, <strong><code>act</code></strong>=<em><code>True</code></em>, <strong><code>bn</code></strong>=<em><code>False</code></em>, <strong><code>dropout</code></strong>=<em><code>0.0</code></em>, <strong>**<code>conv_kwargs</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Conv - base module for convolutions</p>
<p>Inputs:</p>
<ul>
<li><p><code>d_in int</code>: number of input dimensions</p>
</li>
<li><p><code>d_out int</code>: number of output dimensions</p>
</li>
<li><p><code>ks int</code>: kernel size</p>
</li>
<li><p><code>stride int</code>: stride</p>
</li>
<li><p><code>padding [int, None]</code>: padding. If None, derived from kernel size</p>
</li>
<li><p><code>ndim int</code>: conv dimension (1D conv, 2D conv, 3D conv)</p>
</li>
<li><p><code>act bool</code>: if True, applies a ReLU activation</p>
</li>
<li><p><code>bn bool</code>: if True, applies batchnorm consistent with <code>ndim</code></p>
</li>
<li><p><code>dropout float</code>: dropout percentage</p>
</li>
<li><p><code>**conv_kwargs dict</code>: keyword args passed to nn.Conv</p>
</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Conv1d" class="doc_header"><code>class</code> <code>Conv1d</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/layers.py#L135" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Conv1d</code>(<strong><code>d_in</code></strong>, <strong><code>d_out</code></strong>, <strong><code>ks</code></strong>=<em><code>3</code></em>, <strong><code>stride</code></strong>=<em><code>1</code></em>, <strong><code>padding</code></strong>=<em><code>None</code></em>, <strong><code>act</code></strong>=<em><code>True</code></em>, <strong><code>bn</code></strong>=<em><code>False</code></em>, <strong><code>dropout</code></strong>=<em><code>0.0</code></em>, <strong>**<code>conv_kwargs</code></strong>) :: <a href="/mrl/layers#Conv"><code>Conv</code></a></p>
</blockquote>
<p>Conv1d - 1D convolution</p>
<p>Inputs:</p>
<ul>
<li><p><code>d_in int</code>: number of input dimensions</p>
</li>
<li><p><code>d_out int</code>: number of output dimensions</p>
</li>
<li><p><code>ks int</code>: kernel size</p>
</li>
<li><p><code>stride int</code>: stride</p>
</li>
<li><p><code>padding [int, None]</code>: padding. If None, derived from kernel size</p>
</li>
<li><p><code>act bool</code>: if True, applies a ReLU activation</p>
</li>
<li><p><code>bn bool</code>: if True, applies batchnorm consistent with <code>ndim</code></p>
</li>
<li><p><code>dropout float</code>: dropout percentage</p>
</li>
<li><p><code>**conv_kwargs dict</code>: keyword args passed to nn.Conv1D</p>
</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Conv2d" class="doc_header"><code>class</code> <code>Conv2d</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/layers.py#L164" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Conv2d</code>(<strong><code>d_in</code></strong>, <strong><code>d_out</code></strong>, <strong><code>ks</code></strong>=<em><code>3</code></em>, <strong><code>stride</code></strong>=<em><code>1</code></em>, <strong><code>padding</code></strong>=<em><code>None</code></em>, <strong><code>act</code></strong>=<em><code>True</code></em>, <strong><code>bn</code></strong>=<em><code>False</code></em>, <strong><code>dropout</code></strong>=<em><code>0.0</code></em>, <strong>**<code>conv_kwargs</code></strong>) :: <a href="/mrl/layers#Conv"><code>Conv</code></a></p>
</blockquote>
<p>Conv2d - 2D convolution</p>
<p>Inputs:</p>
<ul>
<li><p><code>d_in int</code>: number of input dimensions</p>
</li>
<li><p><code>d_out int</code>: number of output dimensions</p>
</li>
<li><p><code>ks int</code>: kernel size</p>
</li>
<li><p><code>stride int</code>: stride</p>
</li>
<li><p><code>padding [int, None]</code>: padding. If None, derived from kernel size</p>
</li>
<li><p><code>act bool</code>: if True, applies a ReLU activation</p>
</li>
<li><p><code>bn bool</code>: if True, applies batchnorm consistent with <code>ndim</code></p>
</li>
<li><p><code>dropout float</code>: dropout percentage</p>
</li>
<li><p><code>**conv_kwargs dict</code>: keyword args passed to nn.Conv2D</p>
</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Conv3d" class="doc_header"><code>class</code> <code>Conv3d</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/layers.py#L193" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Conv3d</code>(<strong><code>d_in</code></strong>, <strong><code>d_out</code></strong>, <strong><code>ks</code></strong>=<em><code>3</code></em>, <strong><code>stride</code></strong>=<em><code>1</code></em>, <strong><code>padding</code></strong>=<em><code>None</code></em>, <strong><code>act</code></strong>=<em><code>True</code></em>, <strong><code>bn</code></strong>=<em><code>False</code></em>, <strong><code>dropout</code></strong>=<em><code>0.0</code></em>, <strong>**<code>conv_kwargs</code></strong>) :: <a href="/mrl/layers#Conv"><code>Conv</code></a></p>
</blockquote>
<p>Conv3d - 3D convolution</p>
<p>Inputs:</p>
<ul>
<li><p><code>d_in int</code>: number of input dimensions</p>
</li>
<li><p><code>d_out int</code>: number of output dimensions</p>
</li>
<li><p><code>ks int</code>: kernel size</p>
</li>
<li><p><code>stride int</code>: stride</p>
</li>
<li><p><code>padding [int, None]</code>: padding. If None, derived from kernel size</p>
</li>
<li><p><code>act bool</code>: if True, applies a ReLU activation</p>
</li>
<li><p><code>bn bool</code>: if True, applies batchnorm consistent with <code>ndim</code></p>
</li>
<li><p><code>dropout float</code>: dropout percentage</p>
</li>
<li><p><code>**conv_kwargs dict</code>: keyword args passed to nn.Conv3D</p>
</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SphericalDistribution" class="doc_header"><code>class</code> <code>SphericalDistribution</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/layers.py#L224" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SphericalDistribution</code>(<strong><code>loc</code></strong>, <strong><code>scale</code></strong>, <strong><code>validate_args</code></strong>=<em><code>False</code></em>) :: <code>Distribution</code></p>
</blockquote>
<p>SphericalDistribution - samples from points on the surface of a sphere</p>
<p>Inputs:</p>
<ul>
<li><p><code>loc torch.Tensor</code>: vector of means</p>
</li>
<li><p><code>scale torch.Tensor</code>: vector of variances</p>
</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Prior" class="doc_header"><code>class</code> <code>Prior</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/layers.py#L256" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Prior</code>() :: <code>Module</code></p>
</blockquote>
<p>Prior - base class for trainable priors</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="NormalPrior" class="doc_header"><code>class</code> <code>NormalPrior</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/layers.py#L281" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>NormalPrior</code>(<strong><code>loc</code></strong>, <strong><code>log_scale</code></strong>, <strong><code>trainable</code></strong>=<em><code>True</code></em>) :: <a href="/mrl/layers#Prior"><code>Prior</code></a></p>
</blockquote>
<p>NormalPrior - normal distribution prior</p>
<p>Inputs:</p>
<ul>
<li><p><code>loc torch.Tensor</code>: vector of means</p>
</li>
<li><p><code>log_scale torch.Tensor</code>: vector of log-variances</p>
</li>
<li><p><code>trainable bool</code>: if True, <code>loc</code> and <code>scale</code> are trainable</p>
</li>
</ul>
<p>Note that log-variances are used for stability. Optimizing
the variance directly can cause issues with gradient descent
sending variance values negative</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SphericalPrior" class="doc_header"><code>class</code> <code>SphericalPrior</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/layers.py#L317" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SphericalPrior</code>(<strong><code>loc</code></strong>, <strong><code>log_scale</code></strong>, <strong><code>trainable</code></strong>=<em><code>True</code></em>) :: <a href="/mrl/layers#NormalPrior"><code>NormalPrior</code></a></p>
</blockquote>
<p>SphericalPrior - spherical distribution prior</p>
<p>Inputs:</p>
<ul>
<li><p><code>loc torch.Tensor</code>: vector of means</p>
</li>
<li><p><code>log_scale torch.Tensor</code>: vector of log-variances</p>
</li>
<li><p><code>trainable bool</code>: if True, <code>loc</code> and <code>scale</code> are trainable</p>
</li>
</ul>
<p>Note that log-variances are used for stability. Optimizing
the variance directly can cause issues with gradient descent
sending variance values negative</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="n">NormalPrior</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">64</span><span class="p">,)),</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">64</span><span class="p">,)),</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">p</span><span class="o">.</span><span class="n">rsample</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="k">assert</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad</span>

<span class="n">p</span> <span class="o">=</span> <span class="n">SphericalPrior</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,)),</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,)),</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">assert</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">rsample</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="k">assert</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SequenceDropout" class="doc_header"><code>class</code> <code>SequenceDropout</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/layers.py#L341" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SequenceDropout</code>(<strong><code>p</code></strong>, <strong><code>batch_first</code></strong>=<em><code>True</code></em>) :: <code>Module</code></p>
</blockquote>
<p>SequenceDropout - dropout along the sequence dimension</p>
<p>Inputs:</p>
<ul>
<li><p><code>p float</code>: dropout probability</p>
</li>
<li><p><code>batch_first bool</code>: if batch dimension is first in input tensors</p>
</li>
</ul>
<p>Samples a dropout mask that is constant in the sequence dimension</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Conditional_LSTM" class="doc_header"><code>class</code> <code>Conditional_LSTM</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/layers.py#L375" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Conditional_LSTM</code>(<strong><code>d_embedding</code></strong>, <strong><code>d_hidden</code></strong>, <strong><code>d_output</code></strong>, <strong><code>d_latent</code></strong>, <strong><code>n_layers</code></strong>, <strong><code>condition_hidden</code></strong>=<em><code>True</code></em>, <strong><code>condition_output</code></strong>=<em><code>True</code></em>, <strong><code>bidir</code></strong>=<em><code>False</code></em>, <strong><code>input_dropout</code></strong>=<em><code>0.0</code></em>, <strong><code>lstm_dropout</code></strong>=<em><code>0.0</code></em>, <strong><code>batch_first</code></strong>=<em><code>True</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Conditional_LSTM - Conditional LSTM module</p>
<p>Inputs:</p>
<ul>
<li><p><code>d_embedding int</code>: embedding dimension</p>
</li>
<li><p><code>d_hidden int</code>: hidden dimension</p>
</li>
<li><p><code>d_output int</code>: output dimension</p>
</li>
<li><p><code>d_latent int</code>: latent vector dimension</p>
</li>
<li><p><code>n_layers int</code>: number of layers</p>
</li>
<li><p><code>condition_hidden bool</code>: if True, latent vector is used to initialize the
hidden state</p>
</li>
<li><p><code>condition_output bool</code>: if True, latent vector is concatenated to inputs</p>
</li>
<li><p><code>bidir bool</code>: if the LSTM should be bidirectional</p>
</li>
<li><p><code>input_dropout float</code>: dropout percentage on inputs</p>
</li>
<li><p><code>lstm_dropout float</code>: dropout on LSTM layers</p>
</li>
<li><p><code>batch_first bool</code>: if batch dimension is first on input tensors</p>
</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">d_embedding</span><span class="o">=</span><span class="mi">64</span>
<span class="n">d_hidden</span><span class="o">=</span><span class="mi">128</span>
<span class="n">d_latent</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">n_layers</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">l1</span> <span class="o">=</span> <span class="n">Conditional_LSTM</span><span class="p">(</span><span class="n">d_embedding</span><span class="p">,</span> <span class="n">d_hidden</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">,</span> <span class="n">d_latent</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span>
                     <span class="n">condition_hidden</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">condition_output</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                     <span class="n">bidir</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">l2</span> <span class="o">=</span> <span class="n">Conditional_LSTM</span><span class="p">(</span><span class="n">d_embedding</span><span class="p">,</span> <span class="n">d_hidden</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">,</span> <span class="n">d_latent</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span>
                     <span class="n">condition_hidden</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">condition_output</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                     <span class="n">bidir</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">l3</span> <span class="o">=</span> <span class="n">Conditional_LSTM</span><span class="p">(</span><span class="n">d_embedding</span><span class="p">,</span> <span class="n">d_hidden</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">,</span> <span class="n">d_latent</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span>
                     <span class="n">condition_hidden</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">condition_output</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                     <span class="n">bidir</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">l4</span> <span class="o">=</span> <span class="n">Conditional_LSTM</span><span class="p">(</span><span class="n">d_embedding</span><span class="p">,</span> <span class="n">d_hidden</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">,</span> <span class="n">d_latent</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span>
                     <span class="n">condition_hidden</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">condition_output</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                     <span class="n">bidir</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">l5</span> <span class="o">=</span> <span class="n">Conditional_LSTM</span><span class="p">(</span><span class="n">d_embedding</span><span class="p">,</span> <span class="n">d_hidden</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">,</span> <span class="n">d_latent</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span>
                     <span class="n">condition_hidden</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">condition_output</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                     <span class="n">bidir</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">input_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">lstm_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">bs</span> <span class="o">=</span> <span class="mi">12</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">))</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span> <span class="n">d_latent</span><span class="p">))</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">z</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">z</span><span class="p">,</span> <span class="n">l1</span><span class="o">.</span><span class="n">latent_to_hidden</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">l2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">z</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">l2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">z</span><span class="p">,</span> <span class="n">l2</span><span class="o">.</span><span class="n">latent_to_hidden</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">l3</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">z</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">l3</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">z</span><span class="p">,</span> <span class="n">l3</span><span class="o">.</span><span class="n">get_new_hidden</span><span class="p">(</span><span class="n">bs</span><span class="p">))</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">l4</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">z</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">l4</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">z</span><span class="p">,</span> <span class="n">l4</span><span class="o">.</span><span class="n">get_new_hidden</span><span class="p">(</span><span class="n">bs</span><span class="p">))</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">l5</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">z</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">l5</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="kc">None</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">l5</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">z</span><span class="p">,</span> <span class="n">l5</span><span class="o">.</span><span class="n">get_new_hidden</span><span class="p">(</span><span class="n">bs</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">l5</span><span class="o">.</span><span class="n">get_new_hidden</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>2</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">[(</span><span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">i</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">l5</span><span class="o">.</span><span class="n">get_new_hidden</span><span class="p">(</span><span class="mi">5</span><span class="p">)]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[(torch.Size([2, 5, 64]), torch.Size([2, 5, 64])),
 (torch.Size([2, 5, 32]), torch.Size([2, 5, 32]))]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LSTM" class="doc_header"><code>class</code> <code>LSTM</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/layers.py#L551" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LSTM</code>(<strong><code>d_embedding</code></strong>, <strong><code>d_hidden</code></strong>, <strong><code>d_output</code></strong>, <strong><code>n_layers</code></strong>, <strong><code>bidir</code></strong>=<em><code>False</code></em>, <strong><code>input_dropout</code></strong>=<em><code>0.0</code></em>, <strong><code>lstm_dropout</code></strong>=<em><code>0.0</code></em>, <strong><code>batch_first</code></strong>=<em><code>True</code></em>) :: <a href="/mrl/layers#Conditional_LSTM"><code>Conditional_LSTM</code></a></p>
</blockquote>
<p>LSTM - LSTM module</p>
<p>Inputs:</p>
<ul>
<li><p><code>d_embedding int</code>: embedding dimension</p>
</li>
<li><p><code>d_hidden int</code>: hidden dimension</p>
</li>
<li><p><code>d_output int</code>: output dimension</p>
</li>
<li><p><code>n_layers int</code>: number of layers</p>
</li>
<li><p><code>bidir bool</code>: if the LSTM should be bidirectional</p>
</li>
<li><p><code>input_dropout float</code>: dropout percentage on inputs</p>
</li>
<li><p><code>lstm_dropout float</code>: dropout on LSTM layers</p>
</li>
<li><p><code>batch_first bool</code>: if batch dimension is first on input tensors</p>
</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">l1</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="n">d_embedding</span><span class="p">,</span> <span class="n">d_hidden</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">bidir</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">l2</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="n">d_embedding</span><span class="p">,</span> <span class="n">d_hidden</span><span class="p">,</span> <span class="n">d_embedding</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">bidir</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
         <span class="n">input_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">lstm_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">l1</span><span class="o">.</span><span class="n">get_new_hidden</span><span class="p">(</span><span class="n">bs</span><span class="p">))</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">l2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">l2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">l2</span><span class="o">.</span><span class="n">get_new_hidden</span><span class="p">(</span><span class="n">bs</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Conditional_LSTM_Block" class="doc_header"><code>class</code> <code>Conditional_LSTM_Block</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/layers.py#L599" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Conditional_LSTM_Block</code>(<strong><code>d_vocab</code></strong>, <strong><code>d_embedding</code></strong>, <strong><code>d_hidden</code></strong>, <strong><code>d_output</code></strong>, <strong><code>d_latent</code></strong>, <strong><code>n_layers</code></strong>, <strong><code>input_dropout</code></strong>=<em><code>0.0</code></em>, <strong><code>lstm_dropout</code></strong>=<em><code>0.0</code></em>, <strong><code>bidir</code></strong>=<em><code>False</code></em>, <strong><code>condition_hidden</code></strong>=<em><code>True</code></em>, <strong><code>condition_output</code></strong>=<em><code>False</code></em>, <strong><code>forward_rollout</code></strong>=<em><code>False</code></em>, <strong><code>p_force</code></strong>=<em><code>1.0</code></em>, <strong><code>p_force_decay</code></strong>=<em><code>1.0</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Conditional_LSTM_Block - combines Embedding, Conditional LSTM,
and output layer</p>
<p>Inputs:</p>
<ul>
<li><p><code>d_vocab int</code>: vocab size</p>
</li>
<li><p><code>d_embedding int</code>: embedding dimension</p>
</li>
<li><p><code>d_hidden int</code>: hidden dimension</p>
</li>
<li><p><code>d_output int</code>: output dimension</p>
</li>
<li><p><code>d_latent int</code>: latent vector dimension</p>
</li>
<li><p><code>n_layers int</code>: number of layers</p>
</li>
<li><p><code>input_dropout float</code>: dropout percentage on inputs</p>
</li>
<li><p><code>lstm_dropout float</code>: dropout on LSTM layers</p>
</li>
<li><p><code>bidir bool</code>: if the LSTM should be bidirectional</p>
</li>
<li><p><code>condition_hidden bool</code>: if True, latent vector is used to initialize the
hidden state</p>
</li>
<li><p><code>condition_output bool</code>: if True, latent vector is concatenated to inputs</p>
</li>
<li><p><code>forward_rollout bool</code>: if model should generate outputs through rollout
with teacher forcing</p>
</li>
<li><p><code>p_force float</code>: teacher forcing frequency</p>
</li>
<li><p><code>p_force_decay float</code>: teacher forcing decay rate</p>
</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LSTM_Block" class="doc_header"><code>class</code> <code>LSTM_Block</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/layers.py#L713" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LSTM_Block</code>(<strong><code>d_vocab</code></strong>, <strong><code>d_embedding</code></strong>, <strong><code>d_hidden</code></strong>, <strong><code>d_output</code></strong>, <strong><code>n_layers</code></strong>, <strong><code>input_dropout</code></strong>=<em><code>0.0</code></em>, <strong><code>lstm_dropout</code></strong>=<em><code>0.0</code></em>, <strong><code>bidir</code></strong>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>LSTM_Block - combines Embedding, LSTM, and output layer</p>
<p>Inputs:</p>
<ul>
<li><p><code>d_vocab int</code>: vocab size</p>
</li>
<li><p><code>d_embedding int</code>: embedding dimension</p>
</li>
<li><p><code>d_hidden int</code>: hidden dimension</p>
</li>
<li><p><code>d_output int</code>: output dimension</p>
</li>
<li><p><code>n_layers int</code>: number of LSTM layers</p>
</li>
<li><p><code>input_dropout float</code>: dropout percentage on inputs</p>
</li>
<li><p><code>lstm_dropout float</code>: dropout on LSTM layers</p>
</li>
<li><p><code>bidir bool</code>: if the LSTM should be bidirectional</p>
</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Encoder" class="doc_header"><code>class</code> <code>Encoder</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/layers.py#L762" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Encoder</code>(<strong><code>d_latent</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>Base encoder module. All encoders have a
d_latent attribute which is referenced by other modules</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LSTM_Encoder" class="doc_header"><code>class</code> <code>LSTM_Encoder</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/layers.py#L771" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LSTM_Encoder</code>(<strong><code>d_vocab</code></strong>, <strong><code>d_embedding</code></strong>, <strong><code>d_hidden</code></strong>, <strong><code>n_layers</code></strong>, <strong><code>d_latent</code></strong>, <strong><code>input_dropout</code></strong>=<em><code>0.0</code></em>, <strong><code>lstm_dropout</code></strong>=<em><code>0.0</code></em>) :: <a href="/mrl/layers#Encoder"><code>Encoder</code></a></p>
</blockquote>
<p>LSTM_Encoder - LSTM-based encoder</p>
<p>Inputs:</p>
<ul>
<li><p><code>d_vocab int</code>: vocab size</p>
</li>
<li><p><code>d_embedding int</code>: embedding dimension</p>
</li>
<li><p><code>d_hidden int</code>: hidden dimension</p>
</li>
<li><p><code>n_layers int</code>: number of LSTM layers</p>
</li>
<li><p><code>d_latent int</code>: latent space dimension</p>
</li>
<li><p><code>input_dropout float</code>: dropout percentage on inputs</p>
</li>
<li><p><code>lstm_dropout float</code>: dropout on LSTM layers</p>
</li>
</ul>
<p>Generates latent vector from hidden states from the last LSTM layer</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">d_latent</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">l</span> <span class="o">=</span> <span class="n">LSTM_Encoder</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">input_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">lstm_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">l</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">31</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">15</span><span class="p">)))</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">d_latent</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="MLP_Encoder" class="doc_header"><code>class</code> <code>MLP_Encoder</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/layers.py#L812" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>MLP_Encoder</code>(<strong><code>d_in</code></strong>, <strong><code>dims</code></strong>, <strong><code>d_latent</code></strong>, <strong><code>dropouts</code></strong>) :: <a href="/mrl/layers#Encoder"><code>Encoder</code></a></p>
</blockquote>
<p>MLP_Encoder - MLP-based encoder</p>
<p>Inputs:</p>
<ul>
<li><p><code>d_in int</code>: number of input dimensions</p>
</li>
<li><p><code>dims list[int]</code>: list of layer sizes ie <code>[1024, 512, 256]</code></p>
</li>
<li><p><code>dropouts list[float]</code>: list of dropout pobabilities ie <code>[0.2, 0.2, 0.3]</code></p>
</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">MLP_Encoder</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span> <span class="n">d_latent</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">])</span>
<span class="k">assert</span> <span class="n">m</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">128</span><span class="p">))</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">d_latent</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Conv_Encoder" class="doc_header"><code>class</code> <code>Conv_Encoder</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/layers.py#L843" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Conv_Encoder</code>(<strong><code>d_vocab</code></strong>, <strong><code>d_embedding</code></strong>, <strong><code>d_latent</code></strong>, <strong><code>filters</code></strong>, <strong><code>kernel_sizes</code></strong>, <strong><code>strides</code></strong>, <strong><code>dropouts</code></strong>) :: <a href="/mrl/layers#Encoder"><code>Encoder</code></a></p>
</blockquote>
<p>Conv_Encoder - 1D conv encoder</p>
<p>Inputs:</p>
<ul>
<li><p><code>d_vocab int</code>: vocab size</p>
</li>
<li><p><code>d_embedding int</code>: embedding dimension</p>
</li>
<li><p><code>d_latent int</code>: latent vector dimension</p>
</li>
<li><p><code>filters list[int]</code>: filter sizes for conv layers ie <code>[64, 128, 256]</code></p>
</li>
<li><p><code>kernel_sizes list[int]</code>: kernel sizes for conv layers ie <code>[5, 5, 5]</code></p>
</li>
<li><p><code>strides list[int]</code>: strides for conv layers ie <code>[2, 2, 2]</code></p>
</li>
<li><p><code>dropouts list[float]</code>: list of dropout pobabilities ie <code>[0.2, 0.2, 0.3]</code></p>
</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">c</span> <span class="o">=</span> <span class="n">Conv_Encoder</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">d_latent</span><span class="p">,</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">])</span>
<span class="k">assert</span> <span class="n">c</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">31</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">15</span><span class="p">)))</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">d_latent</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="MLP" class="doc_header"><code>class</code> <code>MLP</code><a href="https://github.com/DarkMatterAI/mrl/tree/main/mrl/layers.py#L888" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>MLP</code>(<strong><code>d_in</code></strong>, <strong><code>dims</code></strong>, <strong><code>d_out</code></strong>, <strong><code>drops</code></strong>, <strong><code>outrange</code></strong>=<em><code>None</code></em>, <strong><code>bn</code></strong>=<em><code>True</code></em>) :: <code>Module</code></p>
</blockquote>
<p>MLP - multi-layer perceptron</p>
<p>Inputs:</p>
<ul>
<li><p><code>d_in int</code>: number of input dimensions</p>
</li>
<li><p><code>dims list[int]</code>: list of layer sizes ie <code>[1024, 512, 256]</code></p>
</li>
<li><p><code>d_out int</code>: number of output dimensions</p>
</li>
<li><p><code>drops list[float]</code>: list of dropout pobabilities ie <code>[0.2, 0.2, 0.3]</code></p>
</li>
<li><p><code>outrange Optional[list[float]]</code>: squashes the output to be between <code>outrange[0]</code>
and <code>outrange[1]</code></p>
</li>
</ul>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

